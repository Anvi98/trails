[
    {
        "level": "#",
        "title": "Multi-Agent Reinforcement Traffic Signal Control Based On Interpretable Influence Mechanism And Biased Relu Approximation",
        "content": "\nZhiyue Luo , Jun Xu, Fanglin Chen\n\n  Abstract\u2014Traffic signal control is important in intelligent\ntransportation system, of which cooperative control is difficult\nto realize but yet vital. Many methods model multi-intersection\ntraffic networks as grids and address the problem using multi-\nagent reinforcement learning (RL). Despite these existing studies,\nthere is an opportunity to further enhance our understanding\nof the connectivity and globality of the traffic networks by\ncapturing the spatiotemporal traffic information with efficient\nneural networks in deep RL.\n  In this paper, we propose a novel multi-agent actor-critic\nframework based on an interpretable influence mechanism with a\ncentralized learning and decentralized execution method. Specif-\nically, we first construct an actor-critic framework, for which\nthe piecewise linear neural network (PWLNN), named biased\nReLU (BReLU), is used as the function approximator to obtain a\nmore accurate and theoretically grounded approximation. Then,\nto model the relationships among agents in multi-intersection\nscenarios, we introduce an interpretable influence mechanism\nbased on efficient hinging hyperplanes neural network (EHHNN),\nwhich derives weights by ANOVA decomposition among agents\nand extracts spatiotemporal dependencies of the traffic features.\nFinally, our proposed framework is validated on two synthetic\ntraffic networks to coordinate signal control between intersec-\ntions, achieving lower traffic delays across the entire traffic\nnetwork compared to state-of-the-art (SOTA) performance.\n\n  Index\n         Terms\u2014Multi-agent\n                            reinforcement\n                                           learning,\n                                                    biased\nReLU neural network, efficient hinging hyperplanes neural\nnetwork, traffic signal control.\n\nI. INTRODUCTION\nT\nRANSPORTATION is the key driving force for economic and social growth and is one of the manifestations of urban competitiveness. With the rapid development of urbanization, traffic congestion has become a major challenge for cities around the world[1]. The increasing number of vehicles on the roads has led to longer travel times, increased fuel consumption, and higher levels of air pollution [2].\n\nTraffic signal control is usually regarded as the most significant and effective method for quick and safe transportation, which reduces traffic congestion in the urban network by adjusting the signal phase at intersections[1]. Generally, traffic signal control methods can be mainly divided into This work was supported in part by the National Natural Science Foundation of China under Grant 62173113, and in part by the Science and Technology Innovation Committee of Shenzhen Municipality under Grant GXWD20231129101652001, and in part by Natural Science Foundation of Guangdong Province of China under Grant 2022A1515011584.(*Corresponding author: Jun Xu.*)\nZhiyue Luo, Jun Xu, and Fanglin Chen are with the School of Mechanical Engineering and Automation, Harbin Institute of Technology (Shenzhen), Shenzhen 518055, China (email: xujunqgy@hit.edu.cn).\n\nthree main types, including fixed time control[3], actuated control[4] and adaptive control[5],[6],[7]. However, neither fixed time control nor actuated control methods consider longterm traffic conditions, thus cannot optimize the traffic signal phases adaptively based on real-time traffic flow. In contrast, adaptive control can effectively mitigate traffic congestion and enhance transportation efficiency, which is currently a research hotspot. With the development of artificial intelligence, datadriven control methods play an increasingly important role in intelligent transportation systems. Traffic signal control is a sequential decision problem, which can be modeled as Markov Decision Process (MDP) and solved by reinforcement learning (RL). RL is a powerful dynamic control paradigm, making no additional assumptions on the underlying traffic transition model. Rather than computing explicit traffic flow dynamic equations, RL learns the optimal strategy based on its experience interacting with the traffic environment. The objective of traffic signal control is to minimize the total waiting time within the traffic network by controlling the phase of traffic signals at intersections.\n\nSeveral RL methods have been applied to isolated traffic signal control [8], [9], [10], significantly impacting the field of traffic signal control problem. However, in real world, traffic networks are interrelated, and controlling a specific intersection signal will inevitably affect the traffic condition of the upstream and downstream intersections, leading to the chain reaction of the surrounding intersections. Multi-intersection traffic network is a complex and nonlinear system that can be quite challenging to model. The complexity arises from the need to process intricate spatiotemporal traffic flow data and address cooperative problems among agents, which are difficult to solve using a centralized approach. Consequently, many studies have explored the application of multi-agent RL (MARL) in cooperative traffic signal control problems. These studies aim to find a balance between centralized and decentralized training models, thus reaching an optimum strategy for all agents and minimizing the waiting time of vehicles within the traffic network. To explore further the spatial structural dependencies among different intersections, many researchers applied graph RL in multi-intersection traffic signal control, which uses graph neural networks to learn and exploits representations of each agent and its neighborhood in the form of node embedding.\n\nAlthough the works mentioned above deal with the multiintersection traffic signal control problem through cooperative RL, the effect of coupled agents on the global performance of the traffic signal has yet to be considered explicitly. Hence, in this paper, we introduce a novel interpretable influence mechanism using efficient hinging hyperplanes neural networks (EHHNN) [11], which aims to capture the spatiotemporal dependencies of traffic information to better build the relationships among neighboring intersections. Then, we propose a multi-agent actor-critic framework with a centralized critic and decentralized actors. The main idea is to learn the critic using our interpretable influence mechanism to coordinate the interaction between different agents. Besides, we improve the function approximator used in the deep RL, which is a piecewise linear neural network (PWLNN), named biased ReLU (BReLU) neural network. This BReLU neural network can obtain superior performance than rectified linear units (ReLU) neural network in function approximation when reasonably dividing the piecewise linear (PWL) region [12].\n\nWe approximate both the value function and policy function with BReLU neural network, and thus construct the PWL- actor-critic framework. This also coincides with the conclusion that minimizing PWL functions over polyhedrons yields PWL solutions.\n\nThe technical contribution of this paper can be summarized as follows:\n\n- A novel MARL framework is proposed, in which the\nBReLU neural network is used as the approximator for both the value function and policy function to construct the PWL-actor-critic framework.\n- We propose a novel influence mechanism using the\nEHHNN and combine it with MARL. Compared to the graph-based approaches, our proposed mechanism does not require the pre-defined adjacency matrix of the traffic network and exhibits excellent capability in capturing the spatiotemporal dependencies of traffic flow data. Instead, it models the relationships by analyzing the impact of input variables on the output variables. Compared to the attention mechanism, the EHHNN-based influence mechanism has fewer parameters due to the sparsity of the EHHHNN and does not include a nonlinear activation function, which better explains the contribution of input features to a specific output variable. To the best of our knowledge, we are the first to use EHHNN to model multi-agent relationships and introduce a novel multiagent framework using EHHNN-based influence mechanism.\n- Experiments are conducted on both the traffic grid and a\nnon-Euclidean traffic network. We compare the effectiveness of our solution with several state-of-the-art (SOTA) methods and further analyze the relation reasoning given by our influence mechanism.\nThe rest of this paper is organized as follows: Section II briefly introduces related work. Section III gives problem formulation of multi-intersection traffic signal control problem and models it as Partially-Observable MDP (POMDP). Section IV outlines the detailed implementations of our proposed multi-agent actor-critic framework, which is based on BReLU neural network approximation and employs a novel interpretable influence mechanism to learn the spatiotemporal dependency among different agents. Section V demonstrate the effectiveness of the EHHNN through traffic forecasting experiments, while also comparing our proposed multi-agent actor-critic framework with traditional fixed time control and SOTA MARL algorithm in two simulated environments of multi-intersection traffic signals control task. Finally, Section VI concludes the paper and outlines the main results.\n"
    },
    {
        "level": "##",
        "title": "Ii. Related Work",
        "content": "\nIn this section, we briefly introduce the related work of MARL and graph neural networks in traffic signal control. Additionally, we introduce the development of the EHHNN and its application.\n"
    },
    {
        "level": "##",
        "title": "A. Deep Graph Convolutional Rl",
        "content": "\nIn order to tackle the challenges of large-scale traffic signal control and address the issue of the curse of dimensionality in MARL, many studies have explored the application of MARL methods to traffic signal control problems, which includes approaches employing independent deep Q-network[13], [14], multi-agent deep deterministic policy gradient [15], multiagent advantage actor-critic [16], and large-scale decomposition method [17]. However, not all data in the real world can be represented as a sequence or a grid. To explore further the spatial structural dependencies among different intersections, many researchers applied graph RL in multi-intersection traffic signal control, which uses graph neural networks [18], [19], [20] to learn and exploits representations of each agent and its neighborhood in the form of node embedding. Then, researchers introduced structured perception and relational reasoning based on graph networks in MARL [21], [22], which aims to model the relationship in multi-agent scenarios. Following this line, graph-based RL methods have been used for traffic signal control to attain a deeper understanding of interactions among different intersections, e.g., graph convolutional network and graph attention network were applied in RL to process graph-structured traffic flow data [23], [24]. Several studies combined deep Q network method with graph neural networks [25], [26]. In another study, researchers introduced multi-agent advantage actor-critic algorithm to graph-based RL and proposed a decentralized graph-based method [27]. Furthermore, a spatiotemporal MARL framework was proposed, which considers the spatial dependency and temporal dependency of the traffic information [28].\n"
    },
    {
        "level": "##",
        "title": "B. The Ehhnn",
        "content": "\nIn deep RL, dealing with high-dimensional and intricate state spaces is a major challenge, so many studies use an effective function approximator to approximate the value function and policy function. Neural networks enable the integration of perception, decision-making, and execution phases, facilitating end-to-end learning. PWLNNs are now a successful mainstream method in deep learning, and ReLU is a commonly used activation function in PWLNNs. In typical neural networks, it is impossible to determine the contribution of different input variables to the output through neuron connections. Therefore, in the multi-agent cooperative control tasks, researchers have introduced methods incorporating attention mechanism and relational inductive biases to determine the weights of various input features through similarity measurement. However, we aspire to capture the influence of the inputs on outputs directly through an interpretable neural network.\n\nIn 2020, a novel neural network called EHHNN was proposed [11], which strikes a good balance between model flexibility and interpretability. The EHHNN is a kind of PWLNN derived from the hinging hyperplanes (HH) model [29], in which the ReLU neural network is a kind of the HH model [12]. When the HH model involves only one linear hyperplane passing through the origin, it becomes a ReLU neural network. The EHHNN exhibits a high flexibility in dynamic system identification due to its PWL characteristics. Furthermore, the network is interpretable, allowing for determining the impact of the input layer and hidden layer neurons on the output through the analysis of variance (ANOVA) [30], thereby facilitating the analysis of feature variables. In 2021, a new activation function called BReLU is proposed [12], which is similar to the ReLU. The BReLU neural network is also an HH model, i.e. a PWL function. Compared with ReLU, it employs multiple bias parameters, which can partition the input spaces into numbers of linear regions, thus resulting in high flexibility and excellent approximation capabilities, especially in regression problems.\n"
    },
    {
        "level": "##",
        "title": "Iii. Marl For Traffic Control Problem",
        "content": "\nIn multi-intersection traffic signal control problems, each signal controller reduces the traffic flow at the intersection by adjusting the phase to minimize the waiting time of the traffic flow in the whole traffic network. In this section, we first model an unstructured multi-intersection traffic network as a directed graph and give the formulation and assumptions used in the optimization problem. Then, we model the multiintersection traffic signal control as a MDP and give a detailed formulation of the state, action, and reward function.\n"
    },
    {
        "level": "##",
        "title": "A. Traffic Signal Control Problem Formulation",
        "content": "\nWe consider a more general traffic network, as shown in Fig. 1. For the urban traffic network may not be necessarily connected due to the limitation of the area for urban development [31]. Similar to related work [13],[25],[32], the typical intersection shapes are included in the traffic network, as shown in Fig. 1, v3 and v5 are three-way intersections, while others are crossroads. Besides, the length of lanes in the traffic network varies. We define intersections in the traffic network as nodes and the road between every two intersections as edges, then the multi-intersection traffic network can be modeled as a directed graph G(*V, E,* \u03a8), where V = {vi}|V |\ni=1\nis the nodes set, |V | = N, refers N nodes (intersections) in the graph. E = {ej}|E|\nj=1 is the edges set, |E| is the number of edges and there are lj lanes on the j-th edge. For each edge ej, there is a corresponding upstream node uj \u2208 V and downstream node dj \u2208 V . \u03a8 represents the global attribute of the traffic graph.\n\nBefore describing the traffic dynamic system, we make three reasonable assumptions.\n\nAssumption 1: To simplify the large-scale traffic problem, we assume that the sampling time intervals \u2206t of all intersections are the same, then the cycle time can be expressed as\n\n$$T=k\\cdot\\Delta t\\tag{1}$$\nAssumption 2: We assume that the vehicles from upstream node uj entering edge ej will travel freely until they reach the tail of the waiting vehicle queues. And they will be divided to join the separated queues of the downstream node dj that they intend to go to.\n\nAssumption 3: We assume that each intersection block is equipped with an individual roadside unit, which can observe and collect the traffic information and transmit it to the central controller.\n\nNow, the dynamic traffic model can be derived as follows.\n\nAccording to the vehicle conservation theorem, the number of vehicles on edge ej at step k is updated by\n\n$$n_{j}(k+1)=n_{j}(k)+\\left(I_{u_{j},d_{j}}-O_{d_{j},v}\\right),v\\in\\mathcal{N}_{d_{j}}-\\left\\{u_{j}\\right\\}\\tag{2}$$\n\nwhere $I_{u_{j},d_{j}}$ denotes the traffic flow entering edge $e_{j}$, while $O_{v,u_{j}}$ denotes the traffic flow leaving edge $e_{j}$ reaching its next adjacent target node $v$, $\\mathcal{N}_{d_{j}}$ represents the adjacent nodes of the downstream node $d_{j}$.\n\nAnd consequently, we can update the density of the $j$-th edge from node $u_{j}$ to node $d_{j}$ at time $k$ as\n\n$$\\Phi_{u_{j},d_{j}}=\\min\\left(1,\\frac{n_{j}(k)\\cdot\\tau}{L(e_{j})}\\right)\\tag{3}$$\nwhere \u03c4 is a constant representing the vehicular gap and L(ej)\ndenotes the length of edge j.\n\nThe queue length quj,dj(k) is the number of vehicles waiting on edge ej at step k, which is the number of vehicles on edge ej with a speed of 0. And the total queue length at intersection i can be expressed as\n\n$$Q_{i}(k)=\\sum q_{u_{j},v_{i}}(k),u_{j}\\in\\mathcal{N}_{v_{i}}\\tag{4}$$\n\nThe total vehicle waiting time at the intersection $i$ at step $k$ is denoted as $W_{i}(k)$.\n\nOur objective is to minimize the total waiting queue of the global traffic network by controlling the phase of the intersection signals\n\n$$Z^{*}=\\min\\sum_{k=0}^{T/\\Delta t}\\sum_{i=1}^{N}Q_{i}(k)\\tag{5}$$\n"
    },
    {
        "level": "##",
        "title": "B. Multi-Intersection Traffic Signal Control As Mdp",
        "content": "\nDue to the uncertainty and dynamics of the traffic system, the multi-intersection traffic signal control problem can be abstracted as a discrete stochastic control problem, and be modeled as POMDP, defined as a tuple \u27e8O, A, R*, N, \u03b3*\u27e9, where O\n= {o1, o2*, . . . , o*N} is the set of observations, A =\n{a1, a2*, . . . , a*N} is the set of actions, R = {r1, r2, . . . , rN}\nis the set of reward, and N is the number of agents, also the number of intersections and the number of nodes in the graph G.\n\nThe observation of each agent i is defined as the vector of neighborhood queue length quj,vi of intersection vi, phase of current intersection signal \u03c1, and road density \u03a6uj,vi, which can well represent the incoming traffic flow on the road and the queue numbers at the intersection. For agent i with uj \u2208 Nvj neighboring intersection, the local observation at step k is\n\n$$o_{i}(k)=[\\rho_{i}(k),q_{u_{j},v_{i}}(k),\\Phi_{u_{j},v_{i}}(k)],u_{j}\\in\\mathcal{N}_{v_{i}}\\tag{6}$$\nThe joint state over the traffic network is expressed as S =\no1 \u00d7 o2 \u00d7 *. . . o*N.\n\nEach intersection has a separate controller giving its signal\n(or action). At each time step k, the controller of intersection i selects a discrete action, denoted as ai(k) *\u2208 A*i. The joint action grows exponentially with the number of agents. We consider only feasible sign configurations in the action set and use a four-stage green phase control.\n\nIn traffic signal control, researchers often use characteristic variables such as the total waiting time and the queue length of vehicles to define the reward function. Many studies use expressions such as Eq. (7) to define reward function, which uses changes in traffic characteristic variables Xtcv between adjacent time step.\n\n$r_{tcv}(k)=X_{tcv}(k-1)-X_{tcv}(k)$ (7)\nUsing the expression above, agents tend to accumulate a certain amount of vehicles at the intersection and then release them to obtain larger rewards. This control strategy obviously does not conform to the real-world traffic application scenario. Therefore, in this paper, we propose an improved reward function based on Eq. (7). Considering the characteristics of largescale traffic network and the optimization objective of traffic control, the new reward function is shown in Eq. (8), where \u03ba1,\n\u03ba2 and \u03ba3 are hyperparameters of the reward function under different traffic conditions, \u2206Qi(k) = Qi(k) \u2212 Qi(k \u2212 1) is the changes in queue number between adjacent time step.\n\n$$r_{i}(k)=\\left\\{\\begin{array}{ll}\\kappa_{1},&Q_{i}(k)=0\\\\ -W_{i}(k)/\\kappa_{2},&\\Delta Q_{i}(k)>0\\\\ -\\kappa_{3}\\cdot\\Delta Q_{i}(k),&\\Delta Q_{i}(k)\\leq0\\end{array}\\right.\\tag{8}$$\nThe global reward on the whole traffic network is a linear weighted sum of reward ri for each agent\n\n$$r(k)=\\sum_{i=1}^{N}r_{i}(k)\\tag{9}$$\n"
    },
    {
        "level": "##",
        "title": "Iv. Multi-Agent Brelu Actor-Critic With Interpretable Influence Mechanism",
        "content": "\nIn this section, we propose a multi-agent BReLU actorcritic framework with an interpretable influence mechanism. We introduce a novel neural network called BReLU neural network, which offers improved function approximation for RL and constructs a PWL-actor-critic framework. Then, we extend the PWL-actor-critic framework to the MARL algorithm and employ the interpretable influence mechanism based on EHHNN to capture the spatiotemporal dependencies among different agents. We use a centralized training and decentralized execution method, where a joint value function is learned from the aggregated information and each actor learns its policy function based on the local observations.\n"
    },
    {
        "level": "##",
        "title": "A. Overview",
        "content": "\nThe overview framework of our proposed method is shown in Fig. 2. We first build a graph that comprises traffic signal agents and subsequently propose a novel influence mechanism to extract the spatial dependencies from the input graph. In detail, the observations of each agent\n\ufffd\noi k\n\ufffdN\ni=1 go through a node embedding layer and the EHH-based mechanism to obtain the node embedding V in k for the actor layer and the aggregation embedding for the critic layer, respectively. The module inside the influence mechanism is shown on the righthand side of Fig. 2, the observations\n\ufffd\noi k\n\ufffdN\ni=1 is fed into a linear transformation layer followed by an EHHNN to obtain the hidden variable Hk, then an ANOVA decomposition layer is designed for feature extraction to obtain the important coefficient \u03c3m. Finally, the aggregation embedding V out k is obtained through weighted aggregation. We approximate the policy function and value function in the actor-critic layers with a novel neural network named BReLU and thus construct a PWL-actor-critic framework. Next, we will provide a detailed description of each module within the proposed framework.\n"
    },
    {
        "level": "##",
        "title": "B. Node Embedding",
        "content": "\nFirstly, we obtain the node embedding V in\n                                       k\n                                          =\n                                             \ufffd\n                                               vin\n                                                i,k\n                                                  \ufffdN\n\n                                                                                    i=1\nat the current time step k according to the neighboring edge\ninformation of intersection vi, i.e. agent i. As mentioned in\nSection III, each traffic intersection represents a node, and the\nnode and edge features in the graph network can be repre-\nsented by the variables in the MDP. The traffic information\ncollected by edge ej at time k is defined the same as the\nvalue of the local observation\n                                            \ufffd\n                                              oi\n                                                k\n                                                 \ufffdN\n                                                   i=1 in the MDP\n\nej(k) = \ufffd \u03c1dj(k), quj,dj(k), \u03a6uj,dj(k) \ufffd , (10)\nwhere uj and dj is the upstream node and downstream node of edge ej, respectly. The traffic information of edge ej can represent the interplay among agents, which offers a more comprehensive understanding of how upstream and downstream traffic flow effects propagate between adjacent intersections. Then, the node embedding can be expressed as the aggregation of its adjacent edges traffic information\n\nvin\n i,k = f e\u2192v (ej(k)) = f e\u2192v \ufffd\n                         euj,vi(k)\n                                 \ufffd\n                                  , uj \u2208 Nvi\n                                             (11)\n\nwhere f e\u2192v is a one-layer MultiLayer Perception (MLP) with the ReLU activation function.\n"
    },
    {
        "level": "##",
        "title": "C. Interpretable Influence Mechanism Based On Ehhnn",
        "content": "\nThe traffic model is a nonlinear system, which is challenging to model for its complex spatiotemporal traffic flow data. Therefore, establishing an interpretable influence mechanism and extracting the impact of neighboring traffic information can enhance collaborative control among different intersections. The EHHNN was first applied in short-term traffic flow prediction in 2022, which figured out the spatiotemporal factors influencing the traffic flow using ANOVA decomposition [33]. Compared with this work, we further explore the application of EHHNN in large-scale multi-intersection control problem. We propose an influence mechanism based on EHHNN, which can not only capture the spatiotemporal dependencies of traffic flow data, but also illustrate the relation representation among different agents, enabling multiagent cooperative control. The structure of our proposed interpretable influence mechanism module is shown in the blue box in Fig. 2.\n\n\ufffd Firstly, we reduce the dimension of the local observation oi k\n\ufffdN\ni=1 through a linear transformation layer W to obtain the input variable Xin k for the EHH layer. Unlike other neural networks, the EHHNN possesses interpretability, allowing us to extract the interactions among different input variables through ANOVA decomposition and an interaction matrix. The hidden layer in EHHNN can be seen as a directed acyclic graph, as shown in the yellow box in Fig. 2. All nodes in the\n\ndirected acyclic graph contribute to the output, including two types of neurons, source nodes D and intermediate nodes C. In the EHHNN, the output of source nodes can be described as: z1,s = max {0, xm \u2212 \u03b2m,qm} (12)\nwhere m represents the dimension of the input variable, and\n\u03b2m,qm represents the qm-th bias parameters on the input variable xm.\n\nIn the hidden layer of the EHHNN, the intermediate nodes are obtained by minimizing existing neurons of the previous layers, which comes from different input dimension\n\nzp,s = min nns1,...,nnsp\u2208Jp,s \ufffd max \ufffd 0, xnns1 \u2212 \u03b2s1 \ufffd , . . . max \ufffd 0, xnnsp \u2212 \u03b2sp \ufffd \ufffd (13)\nwhere we define Jp,s =\n\ufffd\nnns1, . . . , nnsp\n\ufffd\ncontains the indices of neurons generated by previous layers, and |Jp,s| = p, which represents the number of interacting neurons of the p-th layer.\n\nFinally, the output of the EHHNN Hk = {hi,k}N\ni=1 is the weighted sum of all neurons in the hidden layer:\n\nHk = \u03b10 + s=1 \u03b11,sz1,s(\u02dcx) + p=2 s=1 \u03b1p,szp,s(\u02dcx) (14) n1 \ufffd P \ufffd np \ufffd\nwhere \u03b11,s, \u03b1p,s are the weight of the EHHNN and \u03b10 is the constant bias, \u02dcx = Xin k , n1 and np denotes the number of neurons in the 1-th and p-th layer, respectively.\n\nIn this paper, we employ a two-factor analysis of variance\n(ANOVA) to determine the main effect of different traffic flow information as individual factors, as well as the interaction effect of bivariate factors on intersection congestion, i.e. P = 2\nin Eq. (14). The first sum represents the influence of individual variables, the second sum represents the joint influence of two variables when P = 2. This characteristic of the EHHNN\nprovides insights into how different variables contribute to the overall prediction and facilitates a deeper understanding of the underlying relationships within the data. Similar to the related work in [30], [11] and [33], we can identify the hidden nodes that influence each output component in the ANOVA decomposition, which is calculated by\n\n$$\\sigma_{m}=\\sqrt{\\text{VAR}(f_{m}(\\tilde{x}))}$$ $$f_{m}(\\tilde{x})=\\sum_{J_{p,s}=\\{m\\}}\\alpha_{p,s}z_{p,s}(\\tilde{x})\\tag{15}$$\nwhere VAR(\u00b7) denotes the corresponding variance of the prediction output related to the m-th component of input variable \u02dcx. The larger the value of \u03c3m, the greater the impact of its corresponding input variable on the degree of congestion of the traffic network.\n\nRemark 1. Due to the physical connection between traffic data and road networks, many researchers employ relational reasoning through graph-based methods or attention mechanism. However, the graph-based methods require a predefined and fixed adjacency matrix to reflect the spatial dependencies of different nodes, which may not effectively capture the spatiotemporal dependencies in the dynamic traffic flow data. The attention mechanism computes the attention coefficient by performing the dot products for all input vectors and utilizes a nonlinear activation function, making it challenging to interpret the relationships between learned weights. Furthermore, most traditional neural networks lack interpretability, making it challenging to select and analyze the input variables while accurately predicting the complex spatiotemporal traffic flow. The EHHNN can meet the requirements above, which employs a unique network structure to capture spatiotemporal dependencies from the input data. It has fewer parameters due to the sparsity of the network structure and can extract the influence coefficients derived from ANOVA decomposition without knowing the node connectivity of the data, as illustrated in the framework shown in Fig. 3.\n\nWhile achieving precise prediction, it processes interpretability through ANOVA decomposition, which obtains the important coefficients of the hidden variables \u03c3m as mentioned in Eq. (15). Additionally, since there is no nonlinear activation function after the linear transformation layer, the importance coefficients of input variables can be derived through inverse transformation:\n\n$$\\sigma_{i n}=W^{-1}\\sigma_{m}\\tag{16}$$\n\n  Finally, we derive the aggregation embedding V out\n                                                k\n                                                     =\n\ufffd\n vout\n  i,k\n     \ufffdN\n\nsectionsi=1, which aggregates information from other inter-\n                     vout\n                       i,k =\n                             \ufffd\n                                 \u03c3mvin\n                                      i,k\n                                                          (17)\n\nwhere vin\n      i,k is the node embedding.\n  This output graph V out\n                   k\n                      , which extracts the spatiotemporal\nfeatures through ANOVA decomposition of EHHNN, is used\nas the input of the centralized critic to learn a joint value\nfunction and faciliate collaboration among different agents.\nWhile in decentralized execution, each actor learns its policy\nfunction based on the locally observed embedding vector V in\n                                                k .\n\nD. Actor-Critic Framework based on BReLU neural network\napproximation\n\nThe RL optimization problem is to obtain the optimal strategy $\\mu^{*}=\\{u_{0},u_{1},\\dots\\}$ that satisfies the constraints, while maximizing the total reward, which can be written as\n\n$$\\tilde{V}_{k+1}(x)=\\max_{u\\in\\pi(x)}E\\left\\{g(x,u,w)+\\gamma\\tilde{V}_{k}(f(x,u,w))\\right\\}\\tag{18}$$\n\nwhere $x$ denotes the state, $w$ is a random disturbance with a probability distribution $P(\\cdot|x,u)$, $\\tilde{V}$ is the value function, $\\pi$ denotes the policy function, $g(x,u,w)$ is the cost per step, $\\gamma$ is the discount factor.\n\nLemma 1. When the value function \u02dcV is a PWL function, the policy function \u03c0 is also a PWL function.\n\nProof.: According to the Bellman's equation, we have\n\n$$\\tilde{V}^{*}(x)=\\max_{u\\in\\pi^{*}(x)}E\\left\\{g(x,u,w)+\\gamma\\tilde{V}^{*}(f(x,u,w))\\right\\}\\tag{19}$$\n\nthis equation can be view as the limit as $k\\rightarrow\\infty$ of Eq. (18). Then the optimal strategy can be derived as\n\n$$\\mu^{*}=\\arg\\max E\\left\\{g(x,u,w)+\\gamma\\tilde{V}^{*}(f(x,u,w))\\right\\}\\tag{20}$$ $$u\\in\\pi(x)$$\nThe cost function in Eq. (20) is g(*x, u, w*) + \u03b3 \u02dcV (f(*x, u, w*)), while g(*x, u, w*) is aslo known as the reward function in RL. In this paper, the reward function as shown in Eq. (8) is a PWL function with respect to state x. If the value function \u02dcV\nis a PWL function, the cost function is the sum of two PWL functions, which is also a PWL function. It has been proved by bemporad in 2002 [34] that minimizing or maximizing a PWL cost function \u02dcV over a polyhedron yields a PWL solution\n\u03c0. Therefore, if we approximate the value function \u02dcV using a PWLNN, the policy function \u03c0 should also be a PWL solution, as stated in the conclusion above.\n\nTo align with the conclusion obtained from Lemma 1, we employ PWL neural networks to approximate the value function and policy function in actor-critic. The activation function plays an important role in neural network approximation, in which the ReLU activation function is prevalent in neural networks due to its simplicity and computation efficiency, and it can be defined as:\n\n$\\mathbb{Z}(x)=\\max\\left\\{0,x\\right\\}$ (21)\nwhere x = [x1, x2, . . . , xn]T\n\u2208 R. The prevalent ReLU\nnetwork is a kind of HH neural network [12].\n\nBased on ReLU neural network, in 2021, Liang and Xu [12]\nproposed BReLU neural network, which can be expressed as:\n\n$$z(x)=\\max\\left\\{0,x_{i}-\\beta_{i_{1}}\\right\\},\\ldots,\\max\\left\\{0,x_{i}-\\beta_{i_{q_{i}}}\\right\\}\\tag{22}$$\n\nwhere $q$ represents the number of linear regions in each dimension of the input data.\n\nDifferent from ReLU neuron, BReLU uses different bias parameters $\\beta_{i_{q_{i}}}$ for variables in various dimensions, thereby partitioning the input spaces into a number of linear regions and take advantage of the characteristics of PWL functions. The input variables after normalization follow an approximately normal distribution, and the multiple bias parameters $\\beta_{i_{q_{i}}}$ are determined by the distribution of the input data:\n\n$$\\beta_{i_{q_{i}}}=[-3\\nu,-0.824\\nu,-0.248\\nu,0.248\\nu,0.834\\nu]+\\eta\\tag{23}$$\nwhere the parameter *\u03bd, \u03b7* represent the variance and expectation of input variable after normalization, respectly. qi is the number of bias parameters of the i-th layer and also represents the number of linear sub-regions. The selection of the value qi is a trade-off between network accuracy and the number of parameters. And the weights in BReLU neural network are obtained using the backpropagation method.\n\nThe BReLU neural network demonstrates higher flexibility and better approximation capabilities by effectively dividing the input spaces into numbers of linear regions, even when the output increases exponentially with the input. Therefore, we approximate the value function \u02dcV and policy function \u03c0\nin the actor-critic framework using BReLU neural network, where the functions approximated by BReLU neural network are PWL. The reason for doing this is two-fold. First, it coincides with the conclusion that minimizing (maximizing) PWL functions over polyhedron yields PWL solutions. Second, the approximation of the value function and policy functions using BReLU neural network provides a more precise approximation than that of ReLU. The red box in Fig. 2 shows the structure of the BReLU neural network used in the proposed PWL-actorcritic framework.\n\nDifferent from independent proximal policy optimization\n(IPPO)[35], which lacks collaboration among agents, in our proposed multi-agent actor-critic algorithm, all critics are updated together to minimize a joint regression loss function as shown in Eq. (24), where the joint value function remains a PWL function. And \u02c6Ai represents the advantage function, which is estimated by the truncated version of the generalized advantage estimation [36], Nb denotes the training batch size, b\u2032 is an arbitrary sampling sequence after sample b, \u03b3 is the discount factor, \u03bb is the regularization parameters and W is the weight of the neural network that approximates the value function \u02dcV .\n\n$$L(\\phi)=-\\sum_{i=1}^{N}\\sum_{b=1}^{N_{b}}(\\hat{A}_{i}(b))^{2}+\\lambda\\left\\|W\\right\\|_{1}\\tag{24}$$ $$\\hat{A}_{i}(b)=\\sum_{b^{\\prime}>b}\\gamma^{b^{\\prime}-b}r_{i}(b^{\\prime})-\\tilde{V}(v_{i,b}^{out})$$\nEach decentralized actor updates its policy function only based on its local observations, which is updated using the proximal policy optimization method. And a clip function is used to limit the range of changes in the probability ratio rt(*\u03b8, b*) of old and new strategies \u03c0i,\u03b8\u2212, \u03c0i,\u03b8 to avoid large variance changes and unstable training.\n\n$$L_{i}(\\theta)=\\sum_{b=1}^{N_{b}}\\min\\left(r_{t}(\\theta,b),\\text{clip}\\left(r_{t}(\\theta,b),1-\\epsilon,1+\\epsilon)\\right)\\cdot\\hat{A}_{i}(b)\\right.$$ $$\\left.r_{t}(\\theta,b)=\\frac{\\pi_{i,\\theta}(a_{b}|v_{i,b}^{in})}{\\pi_{i,\\theta^{-}}(a_{b}|v_{i,b}^{in})}\\right.\\tag{25}$$\n\nwhere\n\nTo give an overview of the proposed multi-agent algorithm, we now summarize it in Algorithm 1 and briefly introduce the training process. During a total of $N_{ep}$ episodes, each lasting for a duration of $T$ in the training process, we store the transition $\\left\\{(o_{i}(k),a_{i}(k),o_{i}(k+1),r_{i}(k))\\right\\}_{i=1}^{N}$ into the memory buffer $\\mathcal{M}$. When the recorded data exceeds the batch size $N_{b}$, we use the pre-trained EHHNN to compute the importance coefficients of the input features and update both the actor and critic network with constant learning rate.\n"
    },
    {
        "level": "##",
        "title": "Algorithm 1 Multi-Agent Brelu Actor-Critic Framework",
        "content": "\nInput: A pre-trained EHHNN\nOutput: actor \u03b8i for i \u2208 N, critic \u03d5\n1: Initialization: actor \u03b8i \u2190 0 and critic \u03d5i \u2190 0 for i \u2208 N;\n2: **for** ep = 1*, . . . , N*ep do\n3:\nfor k = 1*, . . . , T/*\u2206t do\n4:\nReset the environment, oi(k), M = \u2205\n5:\nSample ai(k) from \u03c0i(k)\n6:\nreceive ri(k) (8) and oi(k + 1)\n7:\nend for\n8:\nStore transitions M \u2190 M \u222a [oi(k), ai(k), ri(k)]N\ni=1\n9:\nif N(M) *> N*b then\n10:\nCompute \u03c3 using the pre-trained EHHNN (15)\n11:\nCompute the aggregated result vout\ni,k using (17)\n12:\nfor i = 1*, . . . , N* do\n13:\nCalculate the advantage function \u02c6Ai(b) (25)\n14:\nUpdate the actor \u03b8i by minimizing Li(\u03b8) (25)\n15:\nend for\n16:\nUpdate the global critic \u03d5 by minimizing L(\u03d5) (24)\n17:\nend if\n18: end for\n"
    },
    {
        "level": "##",
        "title": "V. Simulation Results",
        "content": "\nIn this section, we evaluate our proposed method using the SUMO traffic simulator [37]. Firstly, we employ traffic data of Los Angeles country (METR-LA) dataset [38] for traffic forecasting, comparing existing attention mechanism and analyzing the effectiveness of the proposed interpretable influence mechanism based on EHHNN. Compared with the work in [33], we conduct the result on a different dataset and validate the performance of EHHNN on a much larger traffic network. Subsequently, we conduct both quantitative and qualitative experiments for multi-intersection traffic signal control on two synthetic traffic networks, and compare it to the traditional fixed time control method and the SOTA MARL controllers.\n"
    },
    {
        "level": "##",
        "title": "A. Traffic Forecasting",
        "content": "\n1) Dataset Description: To validate the effectiveness and interpretability of the proposed influence mechanism, we conduct experiments on the METR-LA dataset . The METR-LA dataset consists of traffic data collected from circular detectors on highways in Los Angeles Country, comprising a total of 207 sensors. For our experiments, we selecte 15 adjacent nodes, as illustrated in Fig. 4. We evaluate the forecasting performance of the EHHNN compared with four baseline neural networks. Furthermore, we conducte the interpretability analysis of the EHH network to facilitate a deeper understanding of the underlying relationships between different nodes.\n\nFor the traffic data in METR-LA dataset, information is collected every 5 minutes, with an observation window of 60 minutes and a maximum prediction horizon of 45 minutes. We split the dataset into three distinct sets, including a training set, a validation set and a test set, with the training set accounting for 60% of the total samples, and the remaining 40% each allocated to the validation and test sets.\n\n2) Measures of effectiveness: To evaluate the accuracy of different forecasting models, we employ three measures of effectiveness, including mean absolute error (MAE), R2\nvalue[39] and root mean square error (RMSE):\n\n$$\\text{MAE}=\\frac{1}{N\\cdot T}\\sum_{i=1}^{N}\\sum_{t=1}^{T}|(x_{i,t}-\\hat{x}_{i,t})|$$ $$\\text{R}^{2}=1-\\sum_{i=1}^{N}\\sum_{t=1}^{T}(x_{i,t}-\\hat{x}_{i,t})^{2}/\\sum_{i=1}^{N}\\sum_{t=1}^{T}(x_{i,t}-\\bar{x})^{2}$$ $$\\text{RMSE}=\\sqrt{\\frac{1}{N\\cdot T}\\sum_{i=1}^{N}\\sum_{t=1}^{T}(x_{i,t}-\\hat{x}_{i,t})^{2}}\\tag{26}$$\n\nwhere $N$ denotes the number of predicted nodes of METR-LA dataset, $T$ denotes the historical time window for prediction, $x_{i,t}$ is the input traffic flow data of the $i$-th node at time $t$, and $\\bar{x}$ represents the mean value of the input traffic flow data $x_{i,t}$.\n\n_3) Experimental Settings:_ All experiments are compiled and tested on Linux cluster (CPU: Intel(R) Xeon(R) Silver 4216 CPU @ 2.10GHz GPU:NVIDIA GeForce RTX 3090). The EHHNN is a single hidden layer neural network. In the traffic forecasting experiments of this section, our model initially reduces the input dimension through a linear transformation layer. Then, the hidden variables after dimension \nreduction are fed into the EHHNN, which outputs the prediction results. Our proposed model is effectively a two-layer shallow neural network. Therefore, we compare it with three other shallow networks, including a two-layer fully connected (FC) neural network, a FC long short-term memory (FC- LSTM) neural network, and graph attention neural network\n(GAT). Besides, we compared the EHHNN with a SOTA method in traffic forecasting, which is a deep network called Spatial-Temporal Graph Convolutional Network (STGCN). All models utilized the same training parameters, undergoing 300 epochs of training on the training set, employing a variable learning rate optimizer. Subsequently, the optimal parameters of the model were determined through performance evaluation on the validation set. Finally, the models were evaluated on the test set. All tests were conducted with a historical time window of 60 minutes, i.e. 12 sampling points, for predicting the traffic condition in the subsequent 15, 30, and 45 minutes.\n\n4) Experiment Results: Table I demonstrates the results of EHHNN and baselines on the dataset METR-LA. Compared with the three shallow networks, the EHHNN achieved the best performance. In comparison with the STGCN deep network, our model outperformed in both RMSE and R2 metrics, indicating better fitting to large errors and overall predictions that better align with the actual trends in values.\n\nBesides, due to the sparse connectivity and the neuronsconnected structure of the EHHNN, it can achieve superior prediction accuracy with fewer training parameters. The number of hidden layer neurons significantly impacts the performance and capability of a neural network. Increasing the number of hidden layer neurons enhances the expressive ability and complexity of the neural network. However, in a FC network, adding neurons implies increasing training parameters, leading to longer training times and heightened demands on computational resources. Compared with the FC network, FC-LSTM network, the EHHNN requires fewer training parameters when having the same number of hidden neurons.\n"
    },
    {
        "level": "##",
        "title": "B. Multi-Intersection Traffic Signal Control",
        "content": "\nAfter evaluating the effectiveness and interpretability of the EHHNN in traffic forecasting, in this section, we apply the proposed influence mechanism based on EHHNN to the multiintersection traffic signal control problem, and evaluate the algorithm on two different synthetic traffic networks.\n\n1) Traffic Signal Control using Synthetic Traffic Networks:\nWe evaluate our proposed method on two synthetic traffic networks, including a 5\u00d75 traffic grid and a non-Euclidean traffic network, as shown in Fig. 5. The details are introduced as follows:\n\n- Network5\u00d75: A 5 \u00d7 5 traffic grid with three bidirectional\nlanes in four directions at each intersection. The road length is 100m, and the lane speed limit is 13.89m/s. There are approximately 930 vehicles generated and added to the network per episode.\n- Network*NonE*: An non-Euclidean traffic network with 8\nintersections. The intersections are not connected in a grid-like pattern, and the length of roads varies between\n\nModel\n(15/30/60 min)\nMAE \u2193\nR2 \u2193\nRMSE \u2191\nParams\nNeurons\n2-layers FC\n8.26949/8.42299/8.70810\n0.08866/0.07988/0.06103\n19.86146/19.96055/20.17158\n26029/28954/34804\n64\nFC-LSTM\n4.01271/4.63315/5.66560\n0.77434/0.72570/0.64237\n9.78062/10.85584/12.69706\n59181/93786/162996\n64\nGAT\n5.30224/6.01932/6.77592\n0.79210/0.71892/0.63816\n9.48623/11.03227/12.52191\n14339/15878/18956\n64\nSTGCN\n2.83072/ 3.61224/4.60460\n0.83381/0.74521/0.64770\n8.48147/10.50363/12.35573\n96767/97154/97928\n960\nEHHNN\n3.35360/4.21736/5.24809\n0.84352/0.77446/0.66847\n8.22992/9.88232/11.98600\n42763/62293/101353\n370\n\n75m to 150m. There are 10 external road inputs, and approximately 250 vehicles are generated and added to the network per episode. We designed this network for the various buliding areas of the cities, as described in Subsection III-A.\n\n2) Evaluation Metric: To quantitatively measure the condition of the traffic network, we define two metrics, as shown in Eq. (27) and Eq. (28). The average waiting time (AVE) is used to measure the overall congestion degree of the traffic network, while the traffic flow stability (STA) indicates the frequency of local vehicle accumulation that appears during the simulation time.\n\n$$\\text{AVE}=\\frac{1}{T_{s}}\\sum_{t=0}^{T_{s}}\\sum_{i=1}^{N}W_{i}(t)\\tag{27}$$\n\n$$\\text{STA}=\\frac{1}{T_{s}}\\sum_{t=0}^{T_{s}}\\left[\\sum_{i=1}^{N}W_{i}(t)-E\\right]^{2}\\tag{28}$$\n\nwhere $T_{s}$ denotes the simulation time, $N$ denotes the number of intersections, i.e., agents.\n\n_3) Methods Compared with:_ To evaluate the effectiveness of our proposed algorithm, we compare it with serval SOTA traffic signal control methods, including both traditional approach and RL methods.\n\n- Fixed time control: a pre-defined rule-based traffic control\nmethod, which uses a four-stage phase sequence control.\n- Independent Deep Q Network (IDQN): a decentralized\nmethod where each agent learns a Q network to maximize its reward independently without interacting with each other.\n- Multi-Agent Deep deterministic Policy Gradient (MAD-\nDPG): each agent learns a deterministic policy, and this method takes into account the interaction between different agents by sharing the experience pool and employing collaborative training methods.\n- Independent Proximal Policy Optimization (IPPO): a\ndecentralized actor-critic-based method where each agent learns a truncated advantage function and a policy function to improve performance.\n- Graph Convolution RL (DGN): a standard graph-based\nRL method that uses graph convolution neural network and self-attention mechanism to extract the traffic feature, thereby learning the Q-values of the agents.\n4) Simulation Settings: The simulation runs T = 2500s\nper episode, and the traffic signal controllers update every \u2206t = 5s. To ensure the safety at the intersection, we set the yellow phase to 2s, the minimum and maximum green time\ngmin, gmax to 5s and 50s, respectively.\nIn the fixed time control, we set the green phase for each\ntraffic signal controller as 25s. And for all RL methods, we use the same training parameters. All methods are trained for 100 episodes, Table II shows the hyperparameters settings of the reward function in MDP and Algotithm 1.\n\n| Variable                        |\n|---------------------------------|\n| extra reward                    |\n| \u03ba                               |\n| 1                               |\n| 25                              |\n| reduce congestion parameter     |\n| \u03ba                               |\n| 2                               |\n| 5                               |\n| increase congestion parameter   |\n| \u03ba                               |\n| 3                               |\n| 5                               |\n| Q network learning rata         |\n| \u03b1                               |\n| Q                               |\n| 0.01                            |\n| critic learning rate            |\n| \u03b9                               |\n| c                               |\n| 0.01                            |\n| actor learning rate             |\n| \u03b9                               |\n| A                               |\n| 0.001                           |\n| hyperparameter of clip function |\n| \u03f5                               |\n| 0.2                             |\n| minibatch size                  |\n| N                               |\n| b                               |\n| 32                              |\n\n5) Experimental Results:\nWe compare our proposed method with the baseline methods on both the 5\u00d75 traffic grid and non-Euclidean traffic network. Fig. 6 shows the comparison results of our proposed method and all the baseline algorithms on a 5\u00d75 traffic grid. It is evident that, under the current parameter settings, the IDQN and MADDPG methods fail to alleviate traffic congestion, as shown in Fig. 6(b) that the reward of IDQN and MADDPG have been negative per episode. After excluding these two algorithms, as shown in Fig. 6, our proposed algorithm achieves similar performance to the two SOTA algorithm IPPO, DGN. Besides, compared to fixed time control, IDQN, MADDPG, our proposed mehtod achieves reduce the congestion on the traffic network.\n\nSubsequently, as illustrated in Fig. 7, we conduct the results on an non-Euclidean traffic network. Our proposed algorithm exhibits better performance than IPPO, DGN and fixed time control method, and achieves reduced delay and smoother traffic flow conditions compared to other methods. Meanwhile, we conduct an ablation experiment, revealing that the performance obtained using reward function Eq. (7) is inferior to that achieved using the reward function proposed in this paper.\n"
    },
    {
        "level": "##",
        "title": "Vi. Conclusion",
        "content": "\nIn this paper, we have introduced a novel multi-agent actorcritic framework with an interpretable influence mechanism based on the EHHNN. Specifically, we used the BReLU neural network as a function approximator for both the value and policy functions and thus construct the PWL-actor-critic framework. Besides, the proposed influence mechanism based on the EHHNN can capture the spatiotemporal dependencies in traffic information without knowing the pre-defined adjacency matrix. The importance of the input variables using ANOVA decomposition of EHHNN, providing a deeper understanding of the underlying relationships within the data. Moreover, the approximation of the global value function and local policy functions using BReLU neural network not only provides a more precise approximation but also coincides with the conclusion that minimizing PWL functions over polyhedron yields PWL solutions. Simulation experiments on both the synthetic traffic grid and non-Euclidean traffic network demonstrate the effectiveness of the proposed multiagent actor-critic framework, which can effectively extract important information and coordinate signal control across different intersections, resulting in lower delays in the whole traffic network.\n"
    },
    {
        "level": "##",
        "title": "References",
        "content": "\n[1]\nD. Zhao, Y. Dai, and Z. Zhang. \"Computational intelligence in urban traffic signal control: A survey\". In: IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews) 42.4 (2011),\npp. 485\u2013494.\n[2]\nS. Gupta, A. Hamzin, and A. Degbelo. \"A low-cost open\nhardware system for collecting traffic data using Wi-Fi signal strength\". In: *Sensors* 18.11 (2018), p. 3623.\n[3]\nA. J. Miller. \"Settings for fixed-cycle traffic signals\".\nIn: *Journal of the Operational Research Society* 14.4 (1963), pp. 373\u2013386.\n[4]\nS.-B. Cools, C. Gershenson, and B. D'Hooghe. \"Selforganizing traffic lights: A realistic simulation\". In: Advances in applied self-organizing systems (2013),\npp. 45\u201355.\n[5]\nP. Mannion, J. Duggan, and E. Howley. \"An experimental review of reinforcement learning algorithms for adaptive traffic signal control\". In: Autonomic road Transport Support Systems (2016), pp. 47\u201366.\n[6]\nA. Haydari and Y. Y\u0131lmaz. \"Deep reinforcement learning for intelligent transportation systems: A survey\".\nIn: IEEE Transactions on Intelligent Transportation Systems 23.1 (2020), pp. 11\u201332.\n[7]\nB. Abdulhai, R. Pringle, and G. J. Karakoulas. \"Reinforcement learning for true adaptive traffic signal control\". In: *Journal of Transportation Engineering* 129.3 (2003), pp. 278\u2013285.\n[8]\nW. Genders and S. Razavi. \"Using a deep reinforcement\nlearning agent for traffic signal control\". In: arXiv\npreprint arXiv:1611.01142 (2016).\n[9]\nE. Van Der Pol. \"Deep reinforcement learning for\ncoordination in traffic light control\". In: Master's thesis,\nUniversity of Amsterdam (2016).\n[10]\nK. Behrendt, L. Novak, and R. Botros. \"A deep learning approach to traffic lights: Detection, tracking, and classification\". In: 2017 IEEE International Conference on Robotics and Automation (ICRA). IEEE. 2017, pp. 1370\u20131377.\nnational Conference on Computer and Communication\nSystems (ICCCS). IEEE. 2021, pp. 645\u2013650.\n[28]\nY. Wang, T. Xu, X. Niu, et al. \"STMARL: A spatiotemporal multi-agent reinforcement learning approach for cooperative traffic light control\". In: IEEE Transactions on Mobile Computing 21.6 (2020), pp. 2228\u20132242.\n[29]\nL. Breiman. \"Hinging hyperplanes for regression, classification, and function approximation\". In: IEEE Transactions on Information Theory 39.3 (1993), pp. 999\u2013 1013.\n[30]\nJ.\nH.\nFriedman.\n\"Multivariate\nadaptive\nregression\nsplines\". In: *The Annals of Statistics* 19.1 (1991), pp. 1\u2013 67.\n[31]\nB. Jiang and C. Claramunt. \"A structural approach to\nthe model generalization of an urban street network\". In: *GeoInformatica* 8 (2004), pp. 157\u2013171.\n[32]\nS. S. Mousavi, M. Schukat, and E. Howley. \"Traffic\nlight control using deep policy-gradient and valuefunction-based reinforcement learning\". In: IET Intelligent Transport Systems 11.7 (2017), pp. 417\u2013423.\n[33]\nQ. Tao, Z. Li, J. Xu, et al. \"Short-term traffic flow\nprediction based on the efficient hinging hyperplanes neural network\". In: IEEE Transactions on Intelligent Transportation Systems 23.9 (2022), pp. 15616\u201315628.\n[34]\nA. Bemporad, M. Morari, V. Dua, and E. N. Pistikopoulos. \"The explicit linear quadratic regulator for constrained systems\". In: *Automatica* 38.1 (2002), pp. 3\u201320.\n[35]\nD. Guo, L. Tang, X. Zhang, and Y.-C. Liang. \"Joint\noptimization of handover control and power allocation based on multi-agent deep reinforcement learning\". In: *IEEE Transactions on Vehicular Technology* 69.11 (2020), pp. 13124\u201313138.\n[36]\nJ. Schulman, P. Moritz, S. Levine, et al. \"Highdimensional\ncontinuous\ncontrol\nusing\ngeneralized\nadvantage\nestimation\".\nIn:\narXiv\npreprint\narXiv:1506.02438 (2015).\n[37]\nM. Behrisch, L. Bieker, J. Erdmann, and D. Krajzewicz.\n\"SUMO\u2013simulation of urban mobility: an overview\". In: Proceedings of SIMUL 2011, The Third International Conference on Advances in System Simulation.\nThinkMind. 2011.\n[38]\nY. Li, R. Yu, C. Shahabi, and Y. Liu. \"Diffusion convolutional recurrent neural network: Data-driven traffic forecasting\". In: arXiv preprint arXiv:1707.01926 (2017).\n[39]\nM. F. Niri, K. Liu, G. Apachitei, et al. \"Machine learning for optimised and clean Li-ion battery manufacturing: Revealing the dependency between electrode and cell characteristics\". In: Journal of Cleaner Production 324 (2021), p. 129272.\n[11]\nJ. Xu, Q. Tao, Z. Li, et al. \"Efficient hinging hyperplanes neural network and its application in nonlinear system identification\". In: *Automatica* 116 (2020), p. 108906.\n[12]\nX. Liang and J. Xu. \"Biased ReLU neural networks\".\nIn: *Neurocomputing* 423 (2021), pp. 71\u201379.\n[13]\nE. Van der Pol and F. A. Oliehoek. \"Coordinated\ndeep reinforcement learners for traffic light control\". In: Proceedings of Learning, Inference and Control of Multi-agent Systems (NIPS) 8 (2016), pp. 21\u201338.\n[14]\nJ. A. Calvo and I. Dusparic. \"Heterogeneous multi-agent\ndeep reinforcement learning for traffic lights control\". In: *AICS*. 2018, pp. 2\u201313.\n[15]\nN. Casas. \"Deep deterministic policy gradient for\nurban\ntraffic\nlight\ncontrol\".\nIn:\narXiv\npreprint\narXiv:1703.09035 (2017).\n[16]\nT. Chu, J. Wang, L. Codec`a, and Z. Li. \"Multi-agent\ndeep reinforcement learning for large-scale traffic signal control\". In: IEEE Transactions on Intelligent Transportation Systems 21.3 (2019), pp. 1086\u20131095.\n[17]\nT. Tan, F. Bao, Y. Deng, et al. \"Cooperative deep\nreinforcement learning for large-scale traffic grid signal control\". In: *IEEE Transactions on Cybernetics* 50.6\n(2019), pp. 2687\u20132700.\n[18]\nT. N. Kipf and M. Welling. \"Semi-supervised classification with graph convolutional networks\". In: arXiv preprint arXiv:1609.02907 (2016).\n[19]\nW. Hamilton, Z. Ying, and J. Leskovec. \"Inductive\nrepresentation learning on large graphs\". In: Advances in Neural Information Processing Systems 30 (2017).\n[20]\nP.\nVeli\u02c7ckovi\u00b4c,\nG.\nCucurull,\nA.\nCasanova,\net\nal.\n\"Graph\nattention\nnetworks\".\nIn:\narXiv\npreprint\narXiv:1710.10903 (2017).\n[21]\nV. Zambaldi, D. Raposo, A. Santoro, et al. \"Relational deep reinforcement learning\". In: arXiv preprint\narXiv:1806.01830 (2018).\n[22]\nP. W. Battaglia, J. B. Hamrick, V. Bapst, et al. \"Relational inductive biases, deep learning, and graph networks\". In: *arXiv preprint arXiv:1806.01261* (2018).\n[23]\nJ. Jiang, C. Dun, T. Huang, and Z. Lu. \"Graph convolutional reinforcement learning\". In: arXiv preprint arXiv:1810.09202 (2018).\n[24]\nH. Chen, Y. Liu, Z. Zhou, et al. \"Gama: Graph attention multi-agent reinforcement learning algorithm for cooperation\". In: *Applied Intelligence* 50 (2020),\npp. 4195\u20134205.\n[25]\nF.-X. Devailly, D. Larocque, and L. Charlin. \"IG-RL:\nInductive graph reinforcement learning for massivescale traffic signal control\". In: IEEE Transactions on Intelligent Transportation Systems 23.7 (2021), pp. 7496\u20137507.\n[26]\nL. Yan, L. Zhu, K. Song, et al. \"Graph cooperation\ndeep reinforcement learning for ecological urban traffic signal control\". In: *Applied Intelligence* 53.6 (2023), pp. 6248\u20136265.\n[27]\nZ. Zeng. \"GraphLight: graph-based reinforcement learning for traffic signal control\". In: 2021 IEEE 6th Inter-"
    }
]