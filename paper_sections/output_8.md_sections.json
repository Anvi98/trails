[
    {
        "level": "#",
        "title": "Model Order Reduction Of Deep Structured State-Space Models: A System-Theoretic Approach",
        "content": "\nMarco Forgione, Manas Mejari, and Dario Piga IDSIA Dalle Molle Institute for Artificial Intelligence USI-SUPSI, Via la Santa 1, CH-6962 Lugano-Viganello, Switzerland.\n\nMarch 25, 2024\n"
    },
    {
        "level": "##",
        "title": "Abstract",
        "content": "\nWith a specific emphasis on control design objectives, achieving accurate system modeling with limited complexity is crucial in parametric system identification. The recently introduced deep structured state-space models (SSM), which feature linear dynamical blocks as key constituent components, offer high predictive performance. However, the learned representations often suffer from excessively large model orders, which render them unsuitable for control design purposes. The current paper addresses this challenge by means of system-theoretic model order reduction techniques that target the linear dynamical blocks of SSMs. We introduce two regularization terms which can be incorporated into the training loss for improved model order reduction. In particular, we consider modal \u21131 and Hankel nuclear norm regularization to promote sparsity, allowing one to retain only the relevant states without sacrificing accuracy. The presented regularizers lead to advantages in terms of parsimonious representations and faster inference resulting from the reduced order models. The effectiveness of the proposed methodology is demonstrated using real-world ground vibration data from an aircraft.\n"
    },
    {
        "level": "##",
        "title": "1 Introduction",
        "content": "\nIn recent years, deep structured state-space models (SSM) have emerged as powerful and flexible architectures to tackle machine-learning tasks over sequential data such as time series classification, regression, and forecasting [11, 12, 17, 24]. Notably, they exhibit state-of-the-art performance in problems defined over very long sequences, where Transformers struggle due to their computational complexity that grows quadratically with the sequence length [25].\n\nEssentially, SSMs consist in the sequential connection of linear dynamical blocks interleaved with static non-linearities and normalization units, organized in identical repeated layers with skip connections (see Fig. 1). In this sense, they are closely related to the block-oriented modeling framework [23] traditionally employed by the system identification community, and made compatible for training in a deep-learning setting thanks to the *dynoNet* architecture proposed by some of the authors in [8].\n\nSeveral mathematical and software implementation solutions have been devised to make learning of SSM architectures\u2013in particular of their key constituent linear dynamical block\u2013fast, well-posed, and efficient. For instance, S4 [12] adopts a continuous-time parameterization, an initialization strategy based on continuous-time memorization theory, and a convolution-based implementation in frequency domain based on fast Fourier transform. Conversely, the deep Linear Recurrent Unit architecture [17] adopts a discrete-time parameterization with a diagonal state-transition matrix, an initialization strategy that constrains the eigenvalues of the system in a region of stability, and an efficient implementation in time domain exploiting the parallel scan algorithm [4].\n\nFrom the Systems and Control perspective, *parsimonious* representations are often sought, *i.e.*, it is desired to obtain a model that describes system's behaviour with as few parameters and states as possible, to simplify downstream tasks such as controller synthesis, state estimation, etc.\n\nThe inadequacy of high-dimensional models has driven growing interest in Model Order Reduction (MOR) techniques.\n\nIn particular, several contributions focus on reducing the number of states of linear dynamical systems, employing methods that can be broadly categorized into SVD-based and Krylov approximation-based techniques [2].\n\nThe former rely on the concept of the Hankel singular values, which characterize the complexity of the reduced-order model and provide an error bound of the approximation [9]. These methods include balanced truncation [1, 15], singular perturbation approximation [14] and Hankel norm approximation [18]. On the other hand, Krylov-based approximation methods are iterative in nature. They are based on *moment matching* of the impulse response rather than computation of singular values, see the recent survey paper [21] for an overview of these approaches.\n\nIn this paper, we demonstrate the effective adaptation of these MOR techniques, initially designed for linear dynamical systems, to the task of learning simplified deep SSMs architectures while maintaining their predictive capabilities. In principle, an SSM could first be learned using standard machine-learning algorithms, and then each of the constituent linear dynamical blocks could be reduced employing one of the MOR techniques mentioned above. However, we show that the effectiveness of MOR is significantly increased when the low-order modeling objective is already integrated in the training procedure, by means of a *regularization term* in the loss function which promotes parsimonious representations. In particular, we adopt modal \u21131 and Hankel nuclear norm regularization approaches that penalize the magnitude of the linear units' eigenvalues and Hankel singular values, respectively. We illustrate our methodology on a well-known system identification benchmark [16] where the aim is to model the oscillations of an aircraft subject to a ground vibration test.\n\nWe show that specific combinations of regularizers applied during training, along with MOR techniques applied after training, yield the best results.\n\nAll our codes are available in the GitHub repository https://github.com/\nforgi86/lru-reduction, allowing full reproducibility of the reported results.\n"
    },
    {
        "level": "##",
        "title": "2 Problem Setting",
        "content": "\nWe consider a training dataset consisting of a sequence of input-output samples D = {uk, yk}N\nk=1, generated from an unknown dynamical system S, where uk \u2208\nRnu is the input and yk \u2208 Rny is the measured output at time step k. The problem considered in this work is to learn a parametric simulation model M(\u03b8)\nwith parameters \u03b8 \u2208 \u0398, mapping an input sequence u1:k to the (estimated)\noutput sequence \u02c6y1:k, which fits the training dataset D. In particular, the focus is to identify a *parsimonious* model with as few states (in turn, parameters) as possible via regularization and model order reduction techniques.\n\nThe parameters \u03b8 characterising the model M(\u03b8) are estimated according to the criterion:\n\n$$\\hat{\\theta}=\\arg\\min_{\\theta\\in\\Theta}\\frac{1}{N}\\sum_{k=1}^{N}\\mathcal{L}\\left(y_{k},\\hat{y}_{k}(\\theta)\\right)+\\gamma\\mathcal{R}(\\theta),\\tag{1}$$\nwhere \u02c6yk(\u03b8) represents the model's output at time k, and L(\u00b7) denotes the chosen fitting loss function. The term R(\u03b8) is a regularization cost which aims at enforcing sparsity and reducing the complexity of the model, ultimately facilitating the subsequent MOR step. Different choices for the regularization loss R(\u03b8) will be introduced in the Section 4.2.\n\nIn this work, M(\u03b8) is a SSM architecture recently introduced in [17] and known as deep Linear Recurrent Unit.\n\nIn the next section, we describe in details the building blocks and parameterizations of this architecture.\n"
    },
    {
        "level": "##",
        "title": "3 Deep Structured State-Space Model",
        "content": "\nThe deep Linear Recurrent Unit architecture is visualized in Fig. 1. Its core\n(shown in gray) is a stack of nlayers Linear Recurrent Units (LRU), which are linear dynamical systems, interleaved with static non-linearities, (*e.g.*, Multi Layer Perceptrons (MLP) or Gated Linear Units (GLU) [5]) and normalization units (typically Layer or Batch Normalization [27]), with *skip connections* included in each repeated layer. In particular, the l-th layer of the network with inputs ul k and output sl k is defined by:\n\n$$\\mathcal{M}^{i}:\\begin{cases}\\tilde{u}_{k}^{i}=\\text{Norm}(u_{k}^{i}),\\\\ y_{k}^{i}=\\text{LRU}(\\tilde{u}_{1:k}^{i},\\theta^{i}),\\\\ s_{k}^{i}=u_{k}^{i}+f(y_{k}^{i}),\\end{cases}\\tag{2}$$\n\nwhere $\\text{Norm}(\\cdot)$ is the normalization unit; the LRU is a linear time-invariant (LTI) multi-input multi-output (MIMO) dynamical block whose exact structure \nis described in the next sub-section; and f(\u00b7) : Rdmodel \u2192 Rdmodel is the static non-linearity, applied in an element-wise fashion to all samples of the LRU output sequence. The first and last transformations in the architecture (pink blocks) are static linear projections mapping the input samples uk \u2208 Rnu to Uk \u2208 Rdmodel, and Yk \u2208 Rdmodel to predicted outputs \u02c6yk \u2208 Rny, respectively.\n\nWe remark that deep LRU shares a close proximity to the *dynoNet* architecture proposed in [8]. The main difference is that the LRU is a state-space representation of an LTI system, while *dynoNet* employs input-output transfer function descriptions. The architecture is also related to (decoder-only) Transformers [19], with information shared across time steps with an LTI system instead of a causal attention layer.\n\nIn the following subsection, we summarize the details of the LRU block. We omit the layer index l when referencing parameters and signals to simplify the notation. Furthermore, we redefine uk/yk as the input/output samples of the LRU to match the standard notation of linear system theory.\n"
    },
    {
        "level": "##",
        "title": "Linear Recurrent Unit",
        "content": "\nThe LRU is a linear, discrete-time, MIMO dynamical system described in statespace form as:\n\n$x_{k}=A_{D}\\ x_{k-1}+Bu_{k}$, (3a) $y_{k}=\\Re[Cx_{k}]+Du_{k}$, (3b)\nwhere \u211c[\u00b7] denotes the real part of its argument, AD \u2208 Cnx\u00d7nx, B \u2208 Cnx\u00d7nu and C \u2208 Cny\u00d7nx are complex-valued matrices, and D \u2208 Rny\u00d7nu is a real-valued matrix. The matrix AD has a diagonal structure:\n\n$A_{D}=\\mbox{diag}(\\lambda_{1},\\ldots,\\lambda_{n_{x}})$, (4)\nwhere \u03bbj, j = 1*, . . . , n*x are the complex eigenvalues, or modes, of the system, which is thus represented in a *modal* form. In order to guarantee asymptotic stability of the system, *i.e*, to enforce |\u03bbj| < 1, j = 1*, . . . , n*x, each eigenvalue\n\u03bbj \u2208 C is, in turn, parameterized as \u03bbj = exp(\u2212 exp(\u03bdj)+i exp(\u03d5j)), where \u03bdj >\n0. Note that since exp(\u03bdj) > 0 for \u03bdj > 0, this ensures |\u03bbj| = exp(\u2212 exp(\u03bdj)) <\n1.\n\nThe input matrix B is parameterized as B = diag(\u03b31*, . . . , \u03b3*nx) \u02dcB, where\n\u03b3j =\n\ufffd\n1 \u2212 |\u03bbj|2, j = 1*, . . . , n*x, is a normalization factor introduced to obtain state signals with the same power as that of the input signal.\n\nOverall, the learnable parameters of a single LRU block are \u03b8 := {{\u03bdj, \u03d5j}nx j=1, \u02dc*B, C, D*}.\n\nRemark 1 *System* (3) has an equivalent complex-conjugate representation:\n\n$$\\tilde{x}_{k}=\\begin{bmatrix}A_{D}&0\\\\ 0&A_{D}^{*}\\end{bmatrix}\\ \\tilde{x}_{k-1}+\\begin{bmatrix}B\\\\ B^{*}\\end{bmatrix}u_{k},\\tag{5a}$$ $$y_{k}=\\frac{1}{2}\\begin{bmatrix}C&C^{*}\\end{bmatrix}\\tilde{x}_{k}+Du_{k},\\tag{5b}$$\n\n_with $\\tilde{x}_{k}\\in\\mathbb{C}^{2n_{x}}$, which in turn may be transformed in a real Jordan form, with a block-diagonal state-transition matrix containing $n_{x}$ 2x2 blocks, see e.g. Appendix E.3 of [17]. The complex-valued, diagonal representation (3) is preferred for its implementation simplicity and halved state dimension._\n"
    },
    {
        "level": "##",
        "title": "4 Model Order Reduction And Regularization",
        "content": "\nIn this section, we provide a brief review of the MOR techniques used in this paper. Next, we introduce regularization techniques aimed at promoting the learning of parsimonious LRU representations in terms of state complexity.\n"
    },
    {
        "level": "##",
        "title": "4.1 Reduction By Truncation And Singular Perturbation",
        "content": "\nOrder reduction techniques based on truncation decrease the dimensionality of a dynamical system by eliminating states that, for various reasons, are considered less important [10]. Consider a state-space model $G$ with realization:\n\n$$\\begin{bmatrix}x_{1,k}\\\\ x_{2,k}\\end{bmatrix}=\\begin{bmatrix}A_{11}&A_{12}\\\\ A_{21}&A_{22}\\end{bmatrix}\\begin{bmatrix}x_{1,k-1}\\\\ x_{2,k-1}\\end{bmatrix}+\\begin{bmatrix}B_{1}\\\\ B_{2}\\end{bmatrix}u_{k},\\tag{6a}$$ $$y_{k}=\\begin{bmatrix}C_{1}&C_{2}\\end{bmatrix}\\begin{bmatrix}x_{1,k}\\\\ x_{2,k}\\end{bmatrix}+Du_{k},\\tag{6b}$$\nwhere the partitioning corresponds to important states to be kept x1 \u2208 Rr and unimportant states to be removed x2 \u2208 Rnx\u2212r, respectively. The state-space truncation method approximates (6) with a reduced-order system Gr having state-space matrices:\n\n$A_{r}=A_{11},B_{r}=B_{1},C_{r}=C_{1},D_{r}=D$.\n\nNote that state-space truncation does not preserve the steady-state behavior of G and, in general, it may alter its low-frequency response significantly. Alternatively, the removed states x2 may be set to equilibrium by solving for x2,k = x2,k\u22121 in (6a). This results in the so-called *singular perturbation* approximation, where the reduced-order system Gr is defined by the following state-space matrices:\n\n$$A_{r}=A_{11}+A_{12}(I-A_{22})^{-1}A_{21}$$ (8a) $$B_{r}=B_{1}+A_{12}(I-A_{22})^{-1}B_{2}$$ (8b) $$C_{r}=C_{1}+C_{2}(I-A_{22})^{-1}A_{21}$$ (8c) $$D_{r}=C_{2}(I-A_{22})^{-1}B_{2}+D.$$ (8d)\n\nSingular perturbation preserves the steady-state behavior of (6), and generally provides a better match in the lower frequency range with respect to plain state-space truncation.\n\nIn the control literature, state-space truncation and singular perturbation approximations are typically applied to systems described either in modal or imbalanced realization form [10]. The resulting MOR techniques will be denoted here as modal truncation (MT), nodal singular perturbation (MSP), balanced truncation (BT), and bold and angular perturbation (MSP), in the local methods, the states are located according to non-increasing magnitude of the model methods, the eigenstates, so that the fastest dynamics can be guided. This choice is often motivated by physical arguments, when the fast dynamics is are associated to uninteresting second-order effects (e.g., electrical dynamics in mechanical systems). In balanced methods, the states are sorted for non-increasing value of the corresponding Hankel singular values. This choice is supported by the following approximation bound, which holds both for BT and BSP:\n\n$$\\|G-G_{r}\\|_{\\infty}\\leq2\\sum_{j=r+1}^{n_{r}}\\sigma_{j},\\tag{9}$$\nwhere *\u2225\u00b7\u2225*\u221e denotes the H\u221e norm and \u03c3j, j = r + 1*, . . . , n*x are the Hankel singular values corresponding to the eliminated states, see [13, Lemma 3.7].\n\nFor the LRU, MT/MSP are directly applicable, being the block already represented in a modal form. Conversely, BT/BSP require a three-step procedure where (i) system (3) is first transformed to a (non-diagonal) balanced form, then (ii) reduced according to either (7) or (8), and finally (iii) re-diagonalized with a state transformation obtained from the eigenvalue decomposition of the state matrix of the system obtained at step (ii) to fit the LRU famework.\n"
    },
    {
        "level": "##",
        "title": "4.2 Regularized Linear Recurrent Units",
        "content": "\nIn this section, we introduce two regularization approaches that promote learning of LRUs with a reduced state complexity. These methods leverage systemtheoretic MOR methods described in the previous sub-section and exploit the diagonal structure of the LRU's state-transition matrix AD.\n"
    },
    {
        "level": "##",
        "title": "4.2.1 Modal \u21131-Regularization",
        "content": "\nAs mentioned in Section 4.1, high-frequency modes often correspond to secondary phenomena that may be eliminated without compromising the salient aspects of the modeled dynamics. For discrete-time LTI systems, fast dynamics are associated to eigenvalues whose modulus |\u03bbj| is small. An \u21131-regularization is therefore introduced to push some of the modes towards zero during training:\n\n$${\\cal R}(\\theta)=\\sum_{l=1}^{n_{\\rm layers}}\\sum_{j=1}^{n_{x}}|\\lambda_{j}^{l}|.\\tag{10}$$\nIndeed, \u21131-regularization is known to promote sparsity in the solution [26]. The states corresponding to eigenvalues that are closer to zero will then be eliminated with a MOR method at a second stage after training. Note that modal\n\u21131-regularization of the LRU is computationally cheap, as the eigenvalues are directly available on the diagonal of the state-transition matrix AD.\n"
    },
    {
        "level": "##",
        "title": "4.2.2 Hankel Nuclear Norm Regularization",
        "content": "\nIt is known that the McMillan degree (minimum realization order) of a discretetime LTI system coincides with the rank of its associated (infinte-dimensional)\nHankel operator H [13]. The (*i, j*)-th block of H is defined as Hij = gi+j\u22121, where gk = CAk\u22121\nD\nB is the impulse response coefficient at time step k. Minimizing the rank of the Hankel operator thus aligns with the objective of obtaining a low-order representation. However, the rank minimization problem is hard to solve and the *nuclear norm* of the Hankel operator \u2225H(g)\u2225\u2217 := \ufffd\nj \u03c3j, defined as the sum of its singular values \u03c3j, is often used as a *convex surrogate* of the rank [6, 18].\n\nFollowing this rationale, we employ the Hankel nuclear norm of the LRUs as a regularization term in training:\n\n$${\\cal R}(\\theta)=\\sum_{l=1}^{n_{\\rm layers}}\\sum_{j=1}^{n_{x}}\\sigma_{j}^{l},\\tag{11}$$\neigj(PQ), where P and Q are the controllability and observability Grammians of the LTI model [13]. In appendix A.1, we show how the where \u03c3l j denotes the j-th singular value of the Hankel operator of the LRU in the l-th layer. Note that, as \u03c3l j \u2265 0, j = 1*, . . . , n*x, the term \ufffdnx j=1 \u03c3l j in (11) is the \u21131-norm of the Hankel singular values, thus, promoting sparsity.\n\nIt can be proved that the j-th singular value of the Hankel operator is given by \u03c3j(H) =\n\ufffd\nGrammians P and Q, and in turn the Hankel singular values can be computed efficiently for systems in modal form.\n\nRemark 2 (\u21132-regularization) If the \u21132-norm of the Hankel singular values\nis considered in (11) instead of the \u21131-norm, the computational burden during\ntraining can be further reduced exploiting the identity \ufffdnx\n                                         j=1 \u03c32\n                                            j = \ufffdnx\n                                                 j=1 eigj(PQ) =\ntrace(PQ).\n          Thus, differentiation of the eigenvalues of PQ is not required.\nNonetheless, it is known that \u21132-norm regularization does not enforce sparsity\nin the solution, contrary to the \u21131 case.\n\nRemark 3 (H\u221e-error bound) The use of the regularizer (11) is further mo-\ntivated by the H\u221e error bound (9).\n                             This suggests to combine Hankel-norm\nregularization during training with MOR based on either BT or BSP.\n"
    },
    {
        "level": "##",
        "title": "5 Case Study",
        "content": "\nWe test the methodologies of the paper on the ground vibration dataset of an F-16 aircraft introduced in [16].\n\nThe input u \u2208 R (N) is the force generated by a shaker mounted on the aircraft's right wing, while the outputs y \u2208 R3 (m/s2) are the accelerations measured at three test locations on the aircraft. Input and output samples are collected at a constant frequency of 400 Hz. We train deep LRU models with structure as shown in Fig. 1 characterized by: Layer Normalization; MLP nonlinearity; dmodel = 50; nx = 100; and nlayers = 6. The MLP has one hidden layer with 400 hidden units and Gaussian Error Linear Unit (GELU) non-linearities.\n\nTraining is repeated three times with identical settings except for the regularization strategy, which is set to: (i) no regularization, (ii) modal \u21131-regularization, and (iii) Hankel nuclear norm regularization. For both (ii) and (iii), the regularization strength is set to \u03b3 = 10\u22122. We train the models on all the input/output sequences suggested for training in [16] except the one denoted as \"Special Odds\". To take advantage of parallel computations on more sequences, we split the datasets in (partially overlapping) sub-sequences of length N = 5000 samples each and compute the training loss (1) over batches of 64 sub-sequences simultaneously. In the definition of the loss, the first 200 samples of each subsequence are discarded to cope with the state initialization issue, according to the ZERO initialization scheme described in [7]. We minimize the mean squared error training loss over 10 epochs of AdamW with constant learning rate 10\u22124, where at each epoch all the 688820 sub-sequences of length N in the training data are processed.\n\nWe report the *fit* index [22], the Root Mean Squared Error (RMSE), and the Normalized Root Mean Squared Error (NRMSE) on the three output channels in Table 1. For brevity, we exclusively report the results obtained on the test dataset denoted as \"FullMSine Level6 Validation\". The three trained models achieve similar performance, which is also in line with existing state-of-the-art. For instance, the average NRMSE over the three channels is about 0.15, which\n\nRegularization\nChannel 1\nChannel 2\nChannel 3\nfit\nRMSE\nNRMSE\nfit\nRMSE\nNRMSE\nfit\nRMSE\nNRMSE\nNo reg.\n86.5\n0.180\n0.134\n90.0\n0.167\n0.099\n76.2\n0.368\n0.237\nModal \u21131\n85.4\n0.195\n0.145\n89.8\n0.171\n0.102\n74.5\n0.395\n0.254\nHankel norm\n85.8\n0.190\n0.142\n89.0\n0.185\n0.110\n75.5\n0.379\n0.245\n\nis close to the result reported in [20]. However, we observe that regularization has a strong effect on the properties of the estimated LTI blocks.\n\nThe plots in the six columns of Fig. 2, 3 and 4 illustrate this effect, where each column corresponds to LRU in one of the 6 layers. For the model without regularization (Fig. 2), most of the eigenvalues have non-zero magnitude (top panel).\n\nIn this sense, the modal reduction methods MT/MSP are not expected to be effective. The Hankel singular values decrease slightly faster towards zero, suggesting that the effectiveness of the balanced reduction methods BT/BSP might be marginally superior. As for the model obtained with modal\n\u21131-regularization (Fig. 3), several eigenvalues have been pushed towards zero\n(top panel), suggesting the potential effectiveness of modal reduction methods. Finally, for the model trained with Hankel nucelar norm regularization (Fig. 4), the Hankel singular values decrease very sharply towards zero (bottom panel), while none of the eigenvalues' magnitude is pushed towards zero.\n\nThus, we expect balanced reduction methods to be effective in this case.\n\nIn Table 2, we report the maximum number of eliminated states with the different MOR techniques applied to the three trained models, such that the performance degradation in test (in terms of *fit* index) averaged over the three output channels is less than 1%. The best results are obtained for the combinations of modal \u21131-regularization followed by MSP and Hankel nuclear norm regularization followed by BSP, which both lead to 91 eliminated states. We also observe that, when regularization is not applied in training, the subsequent MOR is decisively less effective. Fig. 5 further highlights this key experimental results: when training with no regularization, the best reduction approach (BSP) is significantly less effective than the optimal regularizer+MOR combinations: modal \u21131+MSP and Hankel nuclear norm+BSP.\n\nTruncation Method\nRegularization Method\nBT\nBSP\nMT\nMSP\nNo Regularization\n28\n43\n3\n35\nModal \u21131\n56\n73\n0\n91\nHankel nuclear norm\n89\n91\n18\n76\n"
    },
    {
        "level": "##",
        "title": "6 Conclusions",
        "content": "\nWe have presented regularization methods and model order reduction approaches that enable substantial simplification of deep structured state-space models. Our experimental results suggest that regularization is a fundamental ingredient of our procedure. Indeed, model order reduction executed as a mere post-hoc step, after a standard training conducted without regularization appears to be significantly less effective. In future works, we will analyze in more depth the effect of the regularization strength \u03b3 through more extensive numerical experiments and possibly with analytical tools. Moreover, we aim at decreasing the number of internal input/output channels dmodel and of layers nlayers of the architecture. A possible approach is based on group LASSO regularization, following an approach similar to the one recently introduced in [3] for more classical neural state-space models. Finally, we will extend our techniques to other architectures that feature linear dynamical blocks at their core, such as dynoNet and deep Koopman representations.\n"
    },
    {
        "level": "##",
        "title": "A Appendix A.1 Computation Of Hankel Singular Values",
        "content": "\nThe Hankel singular values of a discrete-time LTI system with complex-valued matrices $(A,B,C,D)$ are given by:\n\n$$\\sigma_{j}=\\sqrt{\\mathfrak{a}\\mathfrak{i}\\mathfrak{g}_{j}(PQ)},\\ \\ \\forall j\\in1,\\ldots,n_{x},\\tag{12}$$\n\nwhere $P\\in\\mathbb{C}^{n_{x}\\times n_{x}}$ and $Q\\in\\mathbb{C}^{n_{x}\\times n_{x}}$ are the controllability and observability Gramians, respectively, which are the solutions of the discrete Lyapunov equations [13]:\n\n$$APA^{*}-P+BB^{*}=0\\tag{13}$$ $$A^{*}QA-Q+C^{*}C=0,\\tag{14}$$\n"
    },
    {
        "level": "##",
        "title": "A.2 Solution To A Diagonal Discrete Lyapunov Equation",
        "content": "\nWe show that discrete Lyapunov equations can be solved efficiently for systems in modal representation where matrix A is diagonal. The direct method to solve the Lyapunov equation with variable X:\n\n$$AXA^{*}-X+Y=0\\tag{15}$$\nis obtained by exploiting the product property:\n\n$${\\rm vec}(AXB)=(B^{\\top}\\otimes A){\\rm vec}(X),\\tag{16}$$\nwhere \u2297 is the Kronecker product operator and vec(\u00b7) represents the columnwise vectorization operation. Applying this formula to (15), one obtains:\n\n$(I-A^{*}\\otimes A){\\rm vec}(X)={\\rm vec}(Y)$, (17)\nwhich is a linear system in the unknowns vec(X). If A is diagonal, the left-hand side matrix of (17) is also diagonal, and thus its solution is simply obtained through n2\nx scalar divisions.\n"
    },
    {
        "level": "##",
        "title": "References",
        "content": "\n[1] U.M. Al-Saggaf and G. F. Franklin. Model reduction via balanced realizations: an extension and frequency weighting techniques. IEEE Transactions\non Automatic Control, 33(7):687\u2013692, 1988.\n[2] A. C. Antoulas. *Approximation of Large-Scale Dynamical Systems*. Society\nfor Industrial and Applied Mathematics, 2005.\n[3] A.\nBemporad.\nLinear\nand\nnonlinear\nsystem\nidentification\nunder\n\u21131- and group-Lasso regularization via L-BFGS-B.\narXiv preprint\narXiv:2403.03827, 2024.\n[4] G. E. Blelloch. Prefix sums and their applications. 1990. [5] Y. N. Dauphin, A. Fan, M. Auli, and D. Grangier. Language modeling with\ngated convolutional networks.\nIn International Conference on Machine\nLearning, pages 933\u2013941. PMLR, 2017.\n[6] M. Fazel, H. Hindi, and S.P. Boyd. A rank minimization heuristic with application to minimum order system approximation. In Proc. of the American Control Conf., volume 6, pages 4734\u20134739, 2001.\n[7] M. Forgione, M. Mejari, and D. Piga. Learning neural state-space models:\ndo we need a state estimator? *arXiv preprint arXiv:2206.12928*, 2022.\n[8] M. Forgione and D. Piga.\ndynoNet: A neural network architecture for\nlearning dynamical systems.\nInternational Journal of Adaptive Control\nand Signal Processing, 35(4):612\u2013626, 2021.\n[9] K. Glover. All optimal hankel-norm approximations of linear multivariable systems and their L\u221e-error bounds. *International Journal of Control*,\n39(6):1115\u20131193, 1984.\n[10] M. Green and D. Limebeer. *Linear Robust Control*. Dover publications,\n2012.\n[11] A. Gu, K. Goel, A. Gupta, and C. R\u00b4e. On the parameterization and initialization of diagonal state space models. Advances in Neural Information\nProcessing Systems, 35:35971\u201335983, 2022.\n[12] A. Gu, K. Goel, and C. R\u00b4e.\nEfficiently modeling long sequences with\nstructured state spaces. The International Conference on Learning Representations (ICLR), 2022.\n[13] T. Katayama. *Subspace Methods for System Identification*. Springer London, 2005.\n[14] Y. Liu and B. O. D. Anderson. Singular perturbation approximation of\nbalanced systems. *International Journal of Control*, 50(4):1379\u20131405, 1989.\n[15] B. Moore. Principal component analysis in linear systems: Controllability, observability, and model reduction. IEEE Transactions on Automatic\nControl, 26(1):17\u201332, 1981.\n[16] J. P. No\u00a8el and M. Schoukens. F-16 aircraft benchmark based on ground\nvibration test data. In 2017 Workshop on Nonlinear System Identification\nBenchmarks, pages 19\u201323, 2017.\n[17] A. Orvieto, S. L. Smith, A. Gu, A. Fernando, C. Gulcehre, R. Pascanu, and\nS. De. Resurrecting recurrent neural networks for long sequences. arXiv preprint arXiv:2303.06349, 2023.\n[18] G. Pillonetto, T. Chen, A. Chiuso, G. De Nicolao, and L. Ljung. Regularized linear system identification using atomic, nuclear and kernel-based norms: The role of the stability constraint. *Automatica*, 69:137\u2013149, 2016.\n[19] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al.\nLanguage models are unsupervised multitask learners. *OpenAI blog*, 1(8):9,\n2019.\n[20] M. Revay, R. Wang, and I. R. Manchester. Recurrent equilibrium networks:\nFlexible dynamic models with guaranteed stability and robustness. IEEE Transactions on Automatic Control, 2023.\n[21] G. Scarciotti and A. Astolfi. Interconnection-based model order reduction\n- a survey. *European Journal of Control*, 75:100929, 2024.\n[22] J. Schoukens and L. Ljung. Nonlinear system identification: A user-oriented\nroad map. *IEEE Control Systems Magazine*, 39(6):28\u201399, 2019.\n[23] M. Schoukens and K. Tiels. Identification of block-oriented nonlinear systems starting from linear approximations: A survey. *Automatica*, 85:272\u2013\n292, 2017.\n[24] J. TH. Smith, A. Warrington, and S. W. Linderman. Simplified state space\nlayers for sequence modeling. *arXiv preprint arXiv:2208.04933*, 2022.\n[25] Y. Tay, M. Dehghani, S. Abnar, D. Shen, Y.and Bahri, P. Pham, J. Rao,\nL. Yang, S. Ruder, and D. Metzler. Long range arena: A benchmark for efficient transformers. *arXiv preprint arXiv:2011.04006*, 2020.\n[26] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of\nthe Royal Statistical Society. Series B, 58(1):267\u2013288, 1996.\n[27] Y. Wu and K. He. Group normalization. In Proceedings of the European\nconference on computer vision (ECCV), pages 3\u201319, 2018."
    }
]