[
    {
        "level": "#",
        "title": "On Moment Relaxations For Linear State Feedback Controller Synthesis With Non-Convex Quadratic Costs And Constraints",
        "content": "\nDennis Gramlich, Sheng Gao, Hao Zhang, Carsten W. Scherer and Christian Ebenbauer\n\n  Abstract\u2014 We present a simple and effective way to\naccount for non-convex costs and constraints in state\nfeedback synthesis, and an interpretation for the vari-\nables in which state feedback synthesis is typically\nconvex. We achieve this by deriving the controller\ndesign using moment matrices of state and input. It\nturns out that this approach allows the consideration\nof non-convex constraints by relaxing them as expec-\ntation constraints, and that the variables in which\nstate feedback synthesis is typically convexified can\nbe identified with blocks of these moment matrices.\n\n          I. INTRODUCTION\n To derive convex controller synthesis conditions sub-\nject to potentially non-convex quadratic constraints we\ntake a similar approach as [7]. This approach uses mo-\nment matrices and relaxes (non-convex) quadratic con-\nstraints as expectation constraints (power constraints).\nDue to the similarities to [7], we highlight the novelties\nof this paper/differences to [7] right at the beginning.\n\n- We show that, as in non-convex quadratic programming [16], the relaxation of non-convex quadratic constraints as expectation constraints yields a deterministic solution, for some problems, and a stochastic solution, for other problems. Deterministic means that the optimal policy of the relaxed problem is deterministic and coincides with an optimal policy of the original problem. Stochastic means that the optimal policy is stochastic and not optimal for the original problem.\n- We show how to realize the stochastic optimal policies. - We identify blocks of our moment matrices with the\nvariables in which state feedback synthesis is usually convexified using the dualization lemma [21].\n- We study moment matrices of affine linear systems. - We explain the benefits of linear controller synthesis\nwith non-convex constraints in compelling examples. Convexification of state feedback synthesis is considered to be a relatively well understood problem in\n\n  Carsten W. Scherer is funded by Deutsche Forschungsgemein-\nschaft (DFG, German Research Foundation) under Germany's\nExcellence Strategy - EXC 2075 - 390740016. He acknowledges the\nsupport by the Stuttgart Center for Simulation Science (SimTech).\n  Dennis Gramlich and Christian Ebenbauer are with the Chair\nof Intelligent Control Systems, RWTH Aachen University, 52074\nAachen, Germany {dennis.gramlich,christian.ebenbauer}\n@ic.rwth-aachen.de\n  Sheng Gao, and Hao Zhang are with the College of Electronic\nand Information Engineering, the Department of Control Science\nand Engineering, Tongji University, Shanghai, 200092, P. R. China.\n2110134@tongji.edu.cn,zhang_hao@tongji.edu.cn.\n  Carsten W. Scherer is with the Chair of Mathematical Sys-\ntems Theory, University of Stuttgart, 70569 Stuttgart, Germany\ncarsten.scherer@imng.uni-stuttgart.de\n\ncontrol theory. Lossless convexifications, i.e., transforma-\ntions that convert a generally non-convex optimization\nproblem into a convex optimization problem, exist for the\nquadratic stabilization of linear systems [2], H2 and H\u221e\nstate feedback synthesis [6], robust state feedback syn-\nthesis for systems in linear fractional representation form\n[20], and data-based controller synthesis [24].The equiv-\nalent convex problems are usually obtained by working\nwith the dual system [11], or by invoking a dualization\nlemma [21]. Throughout, a variable transformation from\nthe parameters of a quadratic storage function to new\nparameters is used, since the controller synthesis is non-\nconvex in the original parameters for the listed problems.\n  All the cited studies consider the new parameters\nmerely as a useful tool for finding the actual parameters\nof interest. In the present work, however, we give the\ntransformed parameters an interpretation by deriving the\nconvexification based on moment matrices of state and\ninput. In the moment matrices, state feedback synthesis\nis convex and the blocks of these moment matrices can be\nidentified with transformed parameters of other works.\n  Studying the moment matrices of state and input of\na dynamical system is certainly an established approach\nin the research literature. The latter comes from the fact\nthat occupation measures [17] and Lyapunov measures\n[18], [23] offer a dual perspective to value functions and\nLyapunov functions with advantages for the convexifica-\ntion of state feedback synthesis. It is therefore proposed,\ne.g., in [14], [9], to optimize over the moment matrices of\noccupation measures of state and input. The restriction\nto linear systems and moments of order two, which we\nconsider in this paper, is a special case of what is consid-\nered in these studies. On the other hand, linear systems\nare generally recognized as an important case and allow\nfor stronger results than polynomial systems. Controller\nsynthesis for linear systems via deterministically inter-\npreted second-order moment matrices is discussed in\ndetail in [1]. As shown in [8], it enables the convex com-\nputation of the H\u221e-norm of linear systems. In addition,\nstate feedback LQG synthesis via second-order moment\nmatrices is discussed in [12]. In contrast to [1], [12], the\npresent paper and [7] address control problems with non-\nconvex quadratic costs and constraints. These are convex\nin the moment matrices and always admit the extraction\nof a stochastic optimal solution. However, in the presence\nof non-convex costs or constraints, the optimal moment\nmatrices may be realizable only by a stochastic policy\nsatisfying constraints only in expectation.\n\n  Moment matrices can have various benefits for the\nconvexification structural controller constraints [19], [5],\nor multi-objective control problems [15]. We expand this\nlist with the design affine linear controllers that avoid\nnon-convex regions in the state space (Section VI) or\nswing up an inverted pendulum (Section VII).\n"
    },
    {
        "level": "##",
        "title": "Ii. Problem Statement We Study A Discrete-Time Linear Time Varying System",
        "content": "\n$$x_{t+1}=f_{t}+A_{t}x_{t}+B_{t}u_{t}+w_{t}.\\tag{1}$$\n\nHere, $x_{t}\\in\\mathbb{R}^{n}$ is the state, $u_{t}\\in\\mathbb{R}^{m}$ is the control input and $w_{t}\\in\\mathbb{R}^{n}$ is a disturbance. Accordingly, $A_{t}\\in\\mathbb{R}^{n\\times n}$ and $B_{t}\\in\\mathbb{R}^{n\\times m}$ are the system matrices at time $t$ and $f_{t}\\in\\mathbb{R}^{n}$ is a additive element $t$, $w_{t}$ is the observation of the \nis an additional constant term in the dynamics.\n\nFor (1) we study the stochastic control problem\n\n$$\\begin{array}{ll}\\mbox{minimize}&\\mathbb{E}\\sum_{t=0}^{N}\\left(\\begin{array}{c}1\\\\ x_{t}\\\\ u_{t}\\end{array}\\right)^{\\top}R_{t}\\left(\\begin{array}{c}1\\\\ x_{t}\\\\ u_{t}\\end{array}\\right)\\\\ \\mbox{s.t.}&x_{t+1}=f_{t}+A_{t}x_{t}+B_{t}u_{t}+w_{t},\\\\ &\\mathbb{E}\\left(\\begin{array}{c}1\\\\ x_{t}\\\\ u_{t}\\end{array}\\right)^{\\top}H_{ti}\\left(\\begin{array}{c}1\\\\ x_{t}\\\\ u_{t}\\end{array}\\right)\\leq0,\\quad i=1,\\ldots,s,\\\\ &u_{t}\\sim\\mathbb{P}_{u_{t}},\\quad w_{t}\\sim\\mathbb{P}_{w_{t}},\\quad x_{0}\\sim\\mathbb{P}_{x_{0}}.\\end{array}\\tag{2a}$$\nIn (2), Px0 denotes the probability distribution of the initial state, Pwt denotes the distribution of the disturbance at time t and Put denotes the distribution of the input at time t. We assume that the first and second moments of x0 are fixed and known and denote them with\n\n$$\\overline{\\Sigma}_{0}=\\begin{pmatrix}\\sigma_{0}^{11}&\\sigma_{0}^{12}\\\\ \\sigma_{0}^{21}&\\Sigma_{0}^{22}\\end{pmatrix}=\\mathbb{E}\\begin{pmatrix}1\\\\ x_{0}\\end{pmatrix}\\begin{pmatrix}1\\\\ x_{0}\\end{pmatrix}^{\\top}.\\tag{3}$$\nThe initial state x0 and the disturbances wt for t =\n0*,... ,N*\u22121 are assumed to be stochastically independent.\n\nIn addition, the control input ut is restricted to be conditionally independent from x0 and wt for t = 0*,... ,N* \u22121\ngiven the current state xt. The latter restriction makes sure that the closed loop is a Markov process [10] and is satisfied, e.g., if ut = \u03c0t(xt,vt), where \u03c0t is some measurable function and vt is a random variable independent from x0 and wt for t = 0*,... ,N* \u22121. The matrices Rt,Hti \u2208 R(1+n+m)\u00d7(1+n+m) are assumed to be symmetric, but may be indefinite. In (2) we regard the distributions Put of the control inputs ut as the control policy and we minimize over this control policy. Problem (2) could be a relaxation of a deterministic control problem with non-convex quadratic constraints, which is np-hard.\n"
    },
    {
        "level": "##",
        "title": "Iii. Convexification Over Moment Matrices To Approach (2), We Study The Moment Matrices",
        "content": "\n$$\\Sigma_{t}=\\begin{pmatrix}\\sigma_{t}^{11}&\\sigma_{t}^{12}&\\sigma_{t}^{13}\\\\ \\sigma_{t}^{21}&\\Sigma_{t}^{22}&\\Sigma_{t}^{23}\\\\ \\sigma_{t}^{31}&\\Sigma_{t}^{32}&\\Sigma_{t}^{33}\\end{pmatrix}:=\\mathbb{E}\\begin{pmatrix}1\\\\ x_{t}\\\\ u_{t}\\end{pmatrix}\\begin{pmatrix}1\\\\ x_{t}\\\\ u_{t}\\end{pmatrix}^{\\top}\\tag{4}$$\nof a random trajectory (xt,ut) of (1), where x0 \u223c Px0, wt \u223c Pwt and ut \u223c Put for t = 0*,... ,N*. Obviously, each control policy (Put) gives rise to a sequence of moments (\u03a3t). The advantage of studying the moments\n(\u03a3t) lies in the fact that the sequence of moments that can result from a control policy (Put) is characterized by simple linear and semi-definite constraints.\n\nThese constraints involve the matrix valued functions\n(\u03a3,\u03a3+,\u03a3w) \u21a6 \u0303Ft(\u03a3,\u03a3+,\u03a3w) defined by \u0303Ft(\u03a3,\u03a3+,\u03a3w)\nequal to\n\n$$\\begin{array}{r l r l}{-\\sigma^{11}}&{{}-\\sigma^{12}}&{{}-\\sigma^{13}}\\\\ {-\\sigma^{21}}&{{}-\\Sigma^{22}}&{{}-\\Sigma^{23}}\\\\ {-\\sigma^{31}}&{{}-\\Sigma^{32}}&{{}-\\Sigma^{33}}\\end{array}$$ ($\\bullet$)${}^{\\top}$\n\n$-\\Sigma^{w}$ $$\\left[\\begin{array}{c c c}{{}}&{{}}&{{}}\\\\ {{}}&{{}}&{{}}\\\\ {{}}&{{}}&{{}}\\\\ {{}}&{{}}&{{}}\\end{array}\\right]\\sigma_{+}^{11}\\;\\;\\;\\;\\sigma_{+}^{12}$$\nTheorem 3.1: Let \u03a3t = \u03a3\u22ba\nt \u2208 R(1+n+m)\u00d7(1+n+m) be a sequence of matrices satisfying the initial condition (3).\n\nThen there exists a policy (Put) generating the sequence of moments (\u03a3t) satisfying (4) if and only if\n\n$$\\Sigma_{t}\\geq0,\\qquad\\qquad\\widetilde{F}_{t}(\\Sigma_{t},\\Sigma_{t+1},\\Sigma_{t}^{w})=0\\qquad\\qquad(5)$$\nhold for t = 0*,... ,N*. Moreover, if (5) holds, then there exist controller parameters Kt = (k1\nt K2\nt ) and \u03a3v t with\n\n$$\\left(\\sigma_{t}^{31}\\quad\\Sigma_{t}^{32}\\right)=\\left(k_{t}^{1}\\quad K_{t}^{2}\\right)\\left(\\sigma_{t}^{11}\\quad\\sigma_{t}^{12}\\right)\\tag{6}$$\n\n$$\\Sigma_{t}^{v}=\\Sigma_{t}^{33}-\\left(k_{t}^{1}\\quad K_{t}^{2}\\right)\\left(\\sigma_{t}^{31}\\quad\\Sigma_{t}^{32}\\right)^{\\top}\\geq0\\tag{7}$$\nand the moments (\u03a3t) result from the particular policy\n\n$u_{t}=k_{t}^{1}+K_{t}^{2}x_{t}+v_{t}$, (8)\nwhere vt \u223c Pvt are some random variables independent from x0,wt for t = 0*,... ,N* \u2212 1 with zero mean and variance \u03a3v t .\n\nProof: Let a policy (Put) be given and let (\u03a3t) be the sequence of moment matrices. We start by showing that (5) must hold. To this end, consider the equation\n\n$$\\begin{pmatrix}1&0&0\\\\ f_{t}&A_{t}&B_{t}\\end{pmatrix}\\begin{pmatrix}\\sigma_{t}^{11}&\\sigma_{t}^{12}&\\sigma_{t}^{13}\\\\ \\sigma_{t}^{21}&\\Sigma_{t}^{22}&\\Sigma_{t}^{23}\\\\ \\sigma_{t}^{31}&\\Sigma_{t}^{32}&\\Sigma_{t}^{33}\\end{pmatrix}(\\bullet)^{\\top}+\\begin{pmatrix}0&0\\\\ 0&\\Sigma^{w}\\end{pmatrix}$$\n\n$$\\begin{pmatrix}\\frac{(4)}{=}\\mathbb{E}\\begin{pmatrix}1&0&0&0\\\\ f_{t}&A_{t}&B_{t}&I\\end{pmatrix}\\begin{pmatrix}1\\\\ x_{t}\\\\ u_{t}\\\\ w_{t}\\end{pmatrix}\\begin{pmatrix}1\\\\ x_{t}\\\\ u_{t}\\\\ w_{t}\\end{pmatrix}^{\\top}(\\bullet)^{\\top}\\\\ \\end{pmatrix}$$\n\n$$\\begin{pmatrix}1\\\\ \\mathbb{E}\\begin{pmatrix}1\\\\ x_{t+1}\\end{pmatrix}\\begin{pmatrix}1\\\\ x_{t+1}\\end{pmatrix}^{\\top}\\stackrel{{(4)}}{{=}}\\begin{pmatrix}\\sigma_{t+1}^{11}&\\sigma_{t+1}^{12}\\\\ \\sigma_{t+1}^{21}&\\Sigma_{t+1}^{22}\\end{pmatrix},$$\nwhich follows from (4), (1) and the independence of wt from (xt,ut). This equation implies \u0303Ft(\u03a3t,\u03a3t+1,\u03a3w t ) = 0.\n\nThe constraints \u03a3t \u2ab0 0 must obviously hold. Hence, (5)\nis satisfied for any control policy (Put).\n\nNext, let a sequence of matrices (\u03a3t) satisfying (5) be given. We show that these matrices equal the moment matrices under the policy (8). To this end, assume that\n\n$$\\mathbb{E}\\left(\\begin{matrix}1\\\\ x_{t}\\end{matrix}\\right)\\left(\\begin{matrix}1\\\\ x_{t}\\end{matrix}\\right)^{\\top}=\\left(\\begin{matrix}\\sigma_{t}^{11}&\\sigma_{t}^{12}\\\\ \\sigma_{t}^{21}&\\Sigma_{t}^{22}\\end{matrix}\\right)\\tag{9}$$\n\nand (8) hold at time t. Then, due to \u03a3t \u2ab0 0, there exist\ncontroller parameters Kt = (k1\n                             t\n                                 K2\n                                   t ) and \u03a3v\n                                            t = \u03a333\n                                                  t\n                                                    \u2212\nk1\n t \u03c313\n   t\n      \u2212 K2\n         t \u03a323\n            t\n               \u2ab0 0 satisfying (6) and (7) such that a\nrandom variable vt \u223c Pvt with zero mean and variance\n\u03a3v\n t can be defined. Next, due to Evt = 0, Evtx\u22ba\n                                             t = 0 and\n(8), we can show\n\n$$\\mathbb{E}\\left(\\begin{matrix}1\\\\ x_{t}\\\\ u_{t}\\end{matrix}\\right)u_{t}^{\\top}=\\left(\\begin{matrix}1&0&0\\\\ 0&I&0\\\\ k_{t}^{1}&K_{t}^{2}&I\\end{matrix}\\right)\\mathbb{E}\\left(\\begin{matrix}1\\\\ x_{t}\\\\ v_{t}\\end{matrix}\\right)\\left(\\begin{matrix}1\\\\ x_{t}\\\\ v_{t}\\end{matrix}\\right)^{\\top}\\left(\\begin{matrix}(k_{t}^{1})^{\\top}\\\\ (K_{t}^{2})^{\\top}\\\\ I\\end{matrix}\\right)$$ $$=\\left(\\begin{matrix}1&0&0\\\\ 0&I&0\\\\ k_{t}^{1}&K_{t}^{2}&I\\end{matrix}\\right)\\left(\\begin{matrix}\\sigma_{t}^{11}&\\sigma_{t}^{12}&0\\\\ \\sigma_{t}^{21}&\\Sigma_{t}^{22}&0\\\\ 0&0&\\Sigma_{t}^{v}\\end{matrix}\\right)\\left(\\begin{matrix}(k_{t}^{1})^{\\top}\\\\ (K_{t}^{2})^{\\top}\\\\ I\\end{matrix}\\right)$$ $$=\\left(\\begin{matrix}1&0&0\\\\ 0&I&0\\\\ k_{t}^{1}&K_{t}^{2}&I\\end{matrix}\\right)\\left(\\begin{matrix}\\sigma_{t}^{13}\\\\ \\Sigma_{t}^{23}\\\\ \\Sigma_{t}^{23}\\\\ \\Sigma_{t}^{33}\\end{matrix}\\right)=\\left(\\begin{matrix}\\sigma_{t}^{23}\\\\ \\Sigma_{t}^{23}\\\\ \\Sigma_{t}^{23}\\\\ \\Sigma_{t}^{33}\\end{matrix}\\right).$$\nCombining the above with (9) shows that (4) holds at time t. Next, (4) and \u0303Ft(\u03a3t,\u03a3t+1,\u03a3w t ) = 0 together imply\n(9) at time t + 1. In this way, we can prove inductively that (\u03a3t) satisfies (4) for all times t.\n\nDue to Theorem 3.1, we can directly optimize over the matrix sequence (\u03a3t) instead of the policy (Put). To this end, notice that the cost (2a) and constraints (2c) can easily be expressed in terms of \u03a3t as\n\n$$\\sum_{t=0}^{N}\\operatorname{trace}\\Sigma_{t}R_{t}\\qquad\\quad{\\mathrm{and}}\\qquad\\quad\\operatorname{trace}\\Sigma_{t}H_{t i}\\leq0.$$\nHence, we can reformulate (2) as the convex program\n\n$$\\begin{array}{ll}\\underset{(\\Sigma_{t})}{\\text{minimize}}&\\sum_{t=0}^{N}\\text{trace}\\,\\Sigma_{t}R_{t}\\\\ \\text{s.t.}&\\widetilde{F}\\big{(}\\Sigma_{t},\\Sigma_{t+1},\\Sigma_{t}^{w}\\big{)}=0,&t=0,\\ldots,N-1,\\\\ &\\text{trace}\\,\\Sigma_{t}H_{ti}\\leq0,&t=0,\\ldots,N,i=1,\\ldots,s,\\\\ &\\Sigma_{t}\\geq0,&t=0,\\ldots,N.\\end{array}$$\nRemark 3.2: We mention that in the case \u03a3v t = 0 for t = 0*,... ,N*, the optimal policy is deterministic and otherwise it is stochastic. As it is shown in [12], if Rt and Hti are positive semi-definite for all t and i, then the optimal policy will be deterministic.\n\nRemark 3.3: Consider [7] to see how to combine the state feedback synthesis presented in this section with a Kalman filter to obtain optimal output feedback policies.\n"
    },
    {
        "level": "##",
        "title": "Iv. The Time-Invariant Case",
        "content": "\nWe can also study the time-invariant infinite timehorizon version of (2) defined as\n\n$$\\begin{array}{ll}\\underset{\\mathbb{P}_{u}}{\\text{minimize}}&\\mathbb{E}\\lim_{N\\to\\infty}\\frac{1}{N}\\sum_{t=0}^{N}\\left(\\begin{array}{c}1\\\\ x_{t}\\\\ u_{t}\\end{array}\\right)^{\\top}R\\left(\\begin{array}{c}1\\\\ x_{t}\\\\ u_{t}\\end{array}\\right)\\\\ \\text{s.t.}&x_{t+1}=f+Ax_{t}+Bu_{t}+u_{t},\\\\ &\\mathbb{E}\\lim_{N\\to\\infty}\\frac{1}{N}\\sum_{t=0}^{N}\\left(\\begin{array}{c}1\\\\ x_{t}\\\\ u_{t}\\end{array}\\right)^{\\top}H_{i}\\left(\\begin{array}{c}1\\\\ x_{t}\\\\ u_{t}\\end{array}\\right)\\leq0,\\quad i=1,\\ldots,s,\\\\ &\\\\ u_{t}\\sim\\mathbb{P}_{u},\\quad w_{t}\\sim\\mathbb{P}_{w},\\quad x_{0}\\sim\\mathbb{P}_{x_{0}}.\\end{array}\\tag{11c}$$\nIn this problem, we minimize the average cost (11a), since the summed cost might often be unbounded, and we impose the average constraints (11c). In addition, we assume that the system data (*f,A,B*), our control policy Pu, and the noise distribution Pw with variance \u03a3w are time-invariant. As in the time-varying case, we study the matrix function \u0303F. This time, however, we seach for a time-invariant matrix\n\n$$\\Sigma=\\begin{pmatrix}\\sigma^{11}&\\sigma^{12}&\\sigma^{13}\\\\ \\sigma^{21}&\\Sigma^{22}&\\Sigma^{23}\\\\ \\sigma^{31}&\\Sigma^{32}&\\Sigma^{33}\\end{pmatrix}\\tag{12}$$\nand define a time-invariant control policy according to\n\n$u_{t}=k^{1}+K^{2}x_{t}+v_{t}$ (13)\nwith controller parameters K = (k1\nK2), \u03a3v satisfying\n\n$$\\left(\\sigma^{31}\\quad\\Sigma^{32}\\right)=\\left(k^{1}\\quad K^{2}\\right)\\left(\\sigma^{11}\\quad\\sigma^{12}\\right)\\tag{14}$$ $$\\Sigma^{v}=\\Sigma^{33}-\\left(k^{1}\\quad K^{2}\\right)\\left(\\sigma^{13}\\quad\\Sigma^{23}\\right)^{\\top}\\tag{15}$$\nand vt for t \u2208 N0 being identically and independently distributed random variables, independent from x0 and wt for t \u2208 N0 with zero mean and variance \u03a3v.\n\nLemma 4.1: Let \u03a3 \u2208 R(1+n+m)\u00d7(1+n+m) satisfy the conditions \u03a3 \u2ab0 0, \u0303F(\u03a3,\u03a3,\u03a3w) = 0 and \u03c311 = 1. Then, if\n\u03a3w \u227b 0 holds and ut is chosen according to the policy\n(13), the sequence of moment matrices (\u03a3t) generated by the policy (13) converges to \u03a3.\n\nProof: Denote the equation \u0303F(\u03a3,\u03a3,\u03a3w) = 0 as\n\n$$\\begin{pmatrix}\\sigma^{11}&\\sigma^{12}\\\\ \\sigma^{21}&\\Sigma^{22}\\end{pmatrix}\\geq\\begin{pmatrix}1&0\\\\ f^{K}&A^{K}\\end{pmatrix}\\begin{pmatrix}\\sigma^{11}&\\sigma^{12}\\\\ \\sigma^{21}&\\Sigma^{22}\\end{pmatrix}\\begin{pmatrix}1&0\\\\ f^{K}&A^{K}\\end{pmatrix}^{\\top}$$ $$+\\begin{pmatrix}0&0\\\\ 0&\\Sigma^{w}+B\\Sigma^{v}B^{\\top}\\end{pmatrix},$$\nwhere f K \u2236= f +Bk1 and AK = A+BK2. This Lyapunov inequality implies that AK is stable. Next consider \u0303\u03a3t \u2236=\n\u03a3 \u2212 \u03a3t. For this matrix \u02dc\u03c311\nt\n= 0 holds for all t, since by assumption \u03c311 = 1 = \u03c311\nt . In addition, this matrix satisfies the Lyapunov equation\n\n$$\\begin{pmatrix}\\tilde{\\sigma}_{t+1}^{11}&\\tilde{\\sigma}_{t+1}^{12}\\\\ \\tilde{\\sigma}_{t+1}^{21}&\\tilde{\\Sigma}_{t+1}^{22}\\end{pmatrix}=\\begin{pmatrix}1&0\\\\ f^{K}&A^{K}\\end{pmatrix}\\begin{pmatrix}\\tilde{\\sigma}_{t}^{11}&\\tilde{\\sigma}_{t}^{12}\\\\ \\tilde{\\sigma}_{t}^{21}&\\tilde{\\Sigma}_{t}^{22}\\end{pmatrix}\\begin{pmatrix}1&0\\\\ f^{K}&A^{K}\\end{pmatrix}^{\\intercal}.$$\n\nThe left lower block of this Lyapunov equation reads\n\u02dc\u03c321\n t+1 = AK \u02dc\u03c321\n          t\n             + f K\u02dc\u03c311\n                   t , which, by \u02dc\u03c311\n                                 t\n                                    = 0 for all t and\nstability of AK implies \u02dc\u03c321\n                        t\n                           \u2192 0 for t \u2192 \u221e. Finally,\ninspecting the right lower block of this Lyapunov equa-\ntion \u0303\u03a322\n     t+1 = f K\u03c311\n              t (f K)\u22ba + AK\u03c321\n                            t (f K)\u22ba + f K\u03c312\n                                          t (AK)\u22ba +\nAK \u0303\u03a322\n    t (AK)\u22ba and recalling AK being stable, \u02dc\u03c311\n                                            t\n                                              being\nzero for all t and \u02dc\u03c321\n                t\n                  converging to zero, we conclude that\n\u0303\u03a322\n t\n    converges to zero. In total, \u0303\u03a3t converges to zero.\n\n Due to Lemma 4.1, the average cost and average\nconstraints simplify under the control policy (13) to\n\n$$\\mathbb{E}\\lim_{N\\to\\infty}{\\frac{1}{N}}\\sum_{t=0}^{N}\\begin{pmatrix}1\\\\ x_{t}\\\\ u_{t}\\end{pmatrix}^{\\top}R\\begin{pmatrix}1\\\\ x_{t}\\\\ u_{t}\\end{pmatrix}=\\operatorname{trace}\\Sigma R,$$\n$$\\mathbb{E}\\lim_{N\\to\\infty}{\\frac{1}{N}}\\sum_{t=0}^{N}\\begin{pmatrix}1\\\\ x_{t}\\\\ u_{t}\\end{pmatrix}^{\\top}H_{i}\\begin{pmatrix}1\\\\ x_{t}\\\\ u_{t}\\end{pmatrix}=\\operatorname{trace}\\Sigma H_{i}.$$\nConsequently, the time-invariant control problem (11) can be solved as the convex program\n\nminimize $\\mbox{trace}\\,\\Sigma R$ (16) s.t. $\\sigma^{11}=1,\\quad\\Sigma\\geq0,$ $\\widetilde{F}(\\Sigma,\\Sigma,\\Sigma^{w})=0,$ $\\mbox{trace}\\,\\Sigma H_{i}\\leq0,$ $i=1,\\ldots,s.$\nRemark 4.2: Since, by Lemma 4.1, \u03a3t converges to the stationary solution \u03a3, the expectation constraint (2c) is satisfied asymptotically and not just on average.\n\nMoreover, if \u03a30 happens to equal \u03a3, then this constraint is satisfied pointwise in time.\n\nRemark 4.3: Also a mixture of the time-varying and time-invariant control problem can be studied. To this end, we may assume that the covariance matrix sequence becomes stationary, that is, (\u03a3t)\n=\n(\u03a30,\u03a31,... ,\u03a3N\u22121,\u03a3N,\u03a3N,\u03a3N*,...*). Then, if the problem parameters (ft,At,Rt,Hti) also become stationary for t\n\u2265\nN, we could consider the constraints\n\u0303F(\u03a3t,\u03a3t+1,\u03a3w t )\n=\n0\nfor t\n=\n0*,... ,N* \u2212 1, and\n\u0303F(\u03a3N,\u03a3N,\u03a3w N) = 0 for the sequence (\u03a3t).\n\nRegarding a suitable cost, we mention that a stationary sequence (\u03a3t) permits, e.g., for a discounted cost\n\n$$\\mathbb{E}\\sum_{t=0}^{N}\\gamma^{t}\\begin{pmatrix}1\\\\ x_{t}\\\\ u_{t}\\end{pmatrix}^{\\top}R\\begin{pmatrix}1\\\\ x_{t}\\\\ u_{t}\\end{pmatrix}=\\sum_{t=0}^{N-1}\\gamma^{t}\\operatorname{trace}R\\Sigma_{t}+\\frac{\\gamma^{N}\\operatorname{trace}R\\Sigma_{N}}{1-\\gamma}$$\n1\u2212\u03b3 R.\n\nwith discount factor \u03b3 \u2208 [0,1[ by choosing Rt = \u03b3tR for t = 0*,... ,N* \u2212 1 and RN = \u03b3N\n"
    },
    {
        "level": "##",
        "title": "V. The Moment Matrix Perspective And Duality",
        "content": "\nNext, we explore how the convexification of state feedback synthesis problems given in Section III and IV relates to standard strategies for state feedback synthesis. To this end, we study one of the most basic problems of state feedback synthesis, namely, quadratic stabilization.\n\nParticularly, an affine linear controller K stabilizes (some equilibrium of) the closed loop (f K,AK) \u2236= (f +Bk1,A+\nBK2) of an affine linear system with K in the sense of Lyapunov if there exists a symmetric matrix P such that the primal matrix inequalities\n\n$$\\begin{array}{r l}{-p^{11}}&{{}=p^{12}}\\end{array}$$ p21 P 22 1 0 f K AK \u239e \u239f\u239f\u239f\u239f\u239f \u23a0 \u239b \u239c\u239c\u239c\u239c\u239c \u239d \u239e \u239f\u239f\u239f\u239f\u239f \u23a0 \u239b \u239c\u239c\u239c\u239c\u239c \u239d\nand P \u227b 0 hold. Equivalently an affine linear controller K stabilizes an affine linear system if there exists a symmetric matrix \u0303P such that the dual matrix inequalities\n\n$$\\left(\\bullet\\right)^{\\top}\\left(\\begin{array}{ccc}-\\hat{p}^{11}&-\\hat{p}^{12}\\\\ -\\hat{p}^{21}&-\\hat{P}^{22}\\\\ \\hat{p}^{11}&\\hat{p}^{12}\\\\ \\hat{p}^{21}&\\hat{P}^{22}\\end{array}\\right)\\left(\\begin{array}{ccc}1&(f^{K})^{\\top}\\\\ 0&(A^{K})^{\\top}\\\\ 1&0\\\\ 0&I\\end{array}\\right)\\geq0\\tag{18}$$\nand \u0303P \u227b 0 hold. The matrices P and \u0303P can be linked by\n\n$$P^{-1}=\\begin{pmatrix}p^{11}&p^{12}\\\\ p^{21}&P^{22}\\end{pmatrix}^{-1}=\\begin{pmatrix}\\hat{p}^{11}&\\hat{p}^{12}\\\\ \\hat{p}^{21}&\\tilde{P}^{22}\\end{pmatrix}=\\widetilde{P}.$$\nThe relations between (17) and (18), and P and \u0303P can be established by congruence transforms and the Schur complement, or by the following dualization lemma.\n\nLemma 5.1 (derived from Lemma 10.2, [21]): Let M\nbe a real, symmetric, non-singular matrix. Then the primal matrix inequalities\n\n$$\\left(\\begin{array}{l}{{I}}\\\\ {{W}}\\end{array}\\right)^{\\top}M\\left(\\begin{array}{l}{{I}}\\\\ {{W}}\\end{array}\\right)\\leq0,\\qquad\\quad\\left(\\begin{array}{l}{{0}}\\\\ {{I}}\\end{array}\\right)^{\\top}M\\left(\\begin{array}{l}{{0}}\\\\ {{I}}\\end{array}\\right)>0$$\nare equivalent to the dual matrix inequalities\n\n$$\\begin{pmatrix}W^{\\top}\\\\ -I\\end{pmatrix}^{\\top}M^{-1}\\begin{pmatrix}W^{\\top}\\\\ -I\\end{pmatrix}\\geq0,\\qquad\\begin{pmatrix}I\\\\ 0\\end{pmatrix}^{\\top}M^{-1}\\begin{pmatrix}I\\\\ 0\\end{pmatrix}\\prec0.$$\n\nThe dual matrix inequality (18) can be convexified by the change of variables from $(P,K)$ to $(\\widetilde{P},\\widetilde{K})$, where $\\widetilde{K}:=KP^{-1}$; we refer to [3] for this convexification.\n\nIn $(\\widetilde{P},\\widetilde{K})$, the dual matrix inequality (18) reads\n\n$$\\begin{pmatrix}-\\tilde{p}^{11}&-\\tilde{p}^{12}&-(\\tilde{k}^{1})^{\\top}\\\\ -\\tilde{p}^{21}&-\\tilde{P}^{22}&-(\\widetilde{K}^{2})^{\\top}\\\\ -\\tilde{k}^{1}&-\\widetilde{K}^{2}&-\\widetilde{K}(\\widetilde{P})^{-1}\\widetilde{K}^{\\top}\\\\ &&\\tilde{p}^{11}&\\tilde{p}^{12}\\\\ \\tilde{p}^{21}&\\widetilde{P}^{22}\\end{pmatrix}\\begin{pmatrix}1&f^{\\top}\\\\ 0&A^{\\top}\\\\ 0&B^{\\top}\\\\ 1&0\\\\ 0&I\\end{pmatrix}\\\\ \\geq0.\\tag{19}$$\nNotice that the non-convexity in \u0303\nK( \u0303P)\u22121 \u0303\nK\u22ba can easily be resolved making use of the Schur complement lemma.\n\nIn addition, the representation (19) of the dual matrix inequality (18) reveals how established convexifying variable transforms relate to the one proposed in this article.\n\nNamely, the feasibility of (19) with \u0303P \u2ab0 0 is equivalent to the feasibility of \u0303F(\u03a3,\u03a3,0) \u2ab0 0 with \u03a3 \u2ab0 0. One direction of this statement can be shown by choosing \u03a3 as\n\n$$\\begin{pmatrix}\\sigma^{11}&\\sigma^{12}&\\sigma^{13}\\\\ \\sigma^{21}&\\Sigma^{22}&\\Sigma^{23}\\\\ \\sigma^{31}&\\Sigma^{32}&\\Sigma^{33}\\end{pmatrix}=\\begin{pmatrix}-\\tilde{p}^{11}&-\\tilde{p}^{12}&-(\\tilde{k}^{1})^{\\intercal}\\\\ -\\tilde{p}^{21}&-\\tilde{P}^{22}&-(\\tilde{K}^{2})^{\\intercal}\\\\ -\\tilde{k}^{1}&-\\tilde{K}^{2}&-\\tilde{K}(\\tilde{P})^{-1}\\tilde{K}^{\\intercal}\\end{pmatrix}.\\tag{20}$$\nFor the reverse direction, we consider any \u03a3 \u2ab0 0 with\n\u0303F(\u03a3,\u03a3,0) \u2ab0 0. Next, perturb \u03a333 to project \u03a3 onto the image of the right hand side of (20). One can check that the latter does not lead to a violation of \u03a3 \u2ab0 0 or\n\u0303F(\u03a3,\u03a3,0) \u2ab0 0. This reverses the variable transform (20).\n\nWe emphasize that the variables \u0303P and \u0303\nK in which established variable transforms convexify the quadratic stabilization problem equal, by (20), blocks of the moment matrix \u03a3. This suggests that \u0303P and \u0303\nK should be interpreted as moment matrices. The fact that (18) is related to \u0303F(\u03a3,\u03a3,0) \u2ab0 0 and not to \u0303F(\u03a3,\u03a3,0) = 0 does not alter the fact that \u0303P and \u0303\nK can be interpreted as moment variables. Indeed, Theorem 3.1 can be proven with \u0303F(\u03a3,\u03a3,0) \u2ab0 0 (or \u0303F(\u03a3,\u03a3,0) \u2aaf 0) in (5) with the difference that, in this case, the sequence \u03a3t is an upper\n(lower) bound on the true moment matrices.\n\nWe mention that these relations to quadratic stabilization extend to state feedback H2 synthesis. Indeed, it can be shown that solving (11) for R\n=\n(0\nC\n0)\n\u22ba\n(0\nC\n0) and \u03a3w = B2(B2)\u22ba is equivalent to designing a state feedback controller minimizing the H2-norm from an input with input matrix B2 to an output with output matrix C; consider also [12].\n"
    },
    {
        "level": "##",
        "title": "Vi. Example: Non-Convex Constraints",
        "content": "\nIn this section we examine the possibility of considering non-convex constraints (avoiding non-convex regions in state space) with numerical examples. To this end, we study the stochastic optimal control problem\n\n$$\\begin{array}{ll}\\mbox{minimize}&\\mathbb{E}\\sum_{t=0}^{N-1}\\left(\\left\\|x_{t}\\right\\|^{2}+\\left\\|u_{t}\\right\\|^{2}\\right)+100\\left\\|x_{N}\\right\\|^{2}\\\\ \\mbox{s.t.}&x_{t+1}=x_{t}+u_{t},\\\\ &\\mathbb{E}\\|u_{t}\\|^{2}\\leq0.1,\\\\ &\\mathbb{E}\\|x_{t}-\\hat{x}_{i}\\|^{2}\\geq(r_{i}+\\varepsilon)^{2},\\quad i=1,\\ldots,s,\\\\ &u_{t}\\sim\\mathbb{P}_{u_{t}},\\quad x_{0}\\sim\\delta_{\\bar{x}}.\\end{array}\\tag{21a}$$\nWe assume that the states xt and control inputs ut take values in R2. The system dynamics (21b) define a two-dimensional integrator. We might imagine that this integrator is a very simple model for a robot in a plane. In this case, the constraint (21c) restricts the speed of the robot. Moreover, the constraints (21d) model that the robot should maintain a distance of ri from the points\n\u02c6xi for i = 1*,... ,s*. The initial distribution is the dirac distribution Px0 = \u03b4\u00afx with \u00afx = (10\n0)\n\u22ba\nand the moments\n\n$$\\begin{pmatrix}\\bar{\\sigma}_{0}^{11}&\\bar{\\sigma}_{0}^{12}\\\\ \\bar{\\sigma}_{0}^{21}&\\overline{{{\\Sigma}}}_{0}^{22}\\end{pmatrix}=\\begin{pmatrix}1&\\bar{x}^{\\intercal}\\\\ \\bar{x}&\\bar{x}\\bar{x}^{\\intercal}\\end{pmatrix}.$$\nThe cost reflects that the robot should reach (0\n0)\n\u22ba\n.\n\nThe problem (21) is an instance of (2) for appropriate choices of Rt and Hti for t = 0*,... ,N* and i = 1*,... ,s*.\n"
    },
    {
        "level": "##",
        "title": "A. Test 1",
        "content": "\nFor a first test, we consider two constraints (21d) with\n\n$${\\begin{pmatrix}{\\hat{x}}_{11}\\\\ {\\hat{x}}_{12}\\end{pmatrix}}={\\begin{pmatrix}-7.5\\\\ 0.5\\end{pmatrix}},\\qquad\\qquad{\\begin{pmatrix}{\\hat{x}}_{21}\\\\ {\\hat{x}}_{22}\\end{pmatrix}}={\\begin{pmatrix}-2.5\\\\ -0.5\\end{pmatrix}}$$\nand r1 = r2 = 1. For these constraints, we map (21)\nto the semi-definite program (10) and solve the latter to obtain a control policy (Put). We then simulate the closed loop of this policy with the dynamics (21b). Ten trajectories of this closed loop are depicted in Figure 1. As we can see, the trajectories are almost not distinguishable. Indeed, the solution of this problem is an (almost) deterministic policy. The constraints are then also satisfied deterministically.\n\nNote that we have chosen \u03b5 = 0.1 in (21d) to create a visible margin between the areas to be avoided and the trajectory. The code for this example can be accessed via https://github.com/SphinxDG/MomentRelaxationsControllerSyn Assuming that we wish to satisfy the constraints (21d)\ndeterministically, we can consider this first example as a case where the relaxation works well. Unfortunately, there are also cases, where the controller is not deterministic and constraints are not deterministically satisfied.\n"
    },
    {
        "level": "##",
        "title": "B. Test 2",
        "content": "\nIn a second test, we consider one constraint (21d) with\n\u02c6x1 = (\u02c6x11\n\u02c6x12) = (\u22125\n0) and r1 = 1, i.e., the area to be avoided is exactly in between the starting position of our system and the desired target.\n\nIf we solve the control problem (21) for this constraint, we obtain closed-loop trajectories resembling the ten trajectories depicted in Figure 2. This scenario presents a significantly different case compared to the previous example. The variance in the trajectories highlights that the policy is stochastic, and seven out of ten trajectories pass through the area to be avoided. If (21) results from the relaxation of a problem with hard constraints, then this example yields an inexact relaxation. This behavior is feasible for the control problem (21), since we are optimizing over stochastic policies and impose only expectation constraints. Hence, we see that the solution of (2) can yield a stochastic policy if (2) includes nonconvex costs or constraints. Moreover, in the stochastic case, some or even all of the trajectories sampled from the closed loop can violate the constraints. We mention that the situation of an *obstacle* exactly in between a robot and a target is sometimes considered to be especially challenging since avoiding the obstacle by going to the left or going to the right seems equally preferable; consider, e.g., the elaboration on indecision in [4], where avoidance strategies are examined just for this scenario. Incidentally, the solution to the current example becomes deterministic if perturbing \u02c6x1 by a tiny amount in the x2\ndirection.\n\nOf course, this example shows that caution is required when using such a relaxation in practice. For non-convex problems, we cannot rely on obtaining a deterministic solution. For problems with soft constraints, the risk of a stochastic solution may be acceptable if the latter occurs rarely. For problems with hard constraints, as in obstacle avoidance, the risk may not be acceptable. Nevertheless, we believe that the methodology presented here may be useful for the iterative solution of such problems, e.g., for the initialization of solvers.\n"
    },
    {
        "level": "##",
        "title": "Vii. Example: Fixed-Point Escape",
        "content": "\nA first example of the flexibility of the moment matrix approach to (11) that resembles obstacle avoidance is given in Section VI. As a second example, we now consider the swing up of an inverted pendulum. The dynamics of an inverted pendulum can be described by the system of nonlinear differential equations [13]\n\n$$\\begin{pmatrix}\\dot{x}_{1}\\\\ \\dot{x}_{2}\\\\ \\dot{x}_{3}\\\\ \\dot{x}_{4}\\end{pmatrix}=\\begin{array}{c}x_{3}\\\\ x_{4}\\\\ \\frac{m_{2}lx_{4}^{2}\\sin x_{2}-m_{2}g\\sin x_{2}\\cos x_{2}+u}{m_{1}+(\\sin x_{2})^{2}m}\\\\ \\frac{(m_{2}lx_{4}^{2}\\cos x_{2}-(m_{1}+m_{2})g)\\sin x_{2}-(l\\cos x_{2})u}{l(m_{1}+(\\sin x_{2})^{2}m_{2})}\\end{array}.\\tag{22}$$\nIn this system of equations, g = 9.81 is the gravitational acceleration and m1 = 1 is the mass of a cart on which a pendulum with mass m2 = 10\u22123 rests. The input u is a force acting on the cart. The state x1 is the position of the cart, x2 is the angle of the pendulum, and x3 and x4\nare the respective velocity and angular velocity.\n\nTo solve the swing up task, we design a controller that swings the pendulum high enough to reach the region of attraction of a second linear controller that stabilizes the upper pendulum position. The controller that makes the pendulum *escape* from the lower equilibrium is obtained as the solution to the control problem\n\n$$\\begin{array}{ll}\\mbox{minimize}&\\lim_{N\\to\\infty}\\frac{1}{N}\\mathbb{E}\\sum_{t=0}^{N-1}\\left(x_{1t}^{2}+x_{3t}^{2}-10^{4}e(x_{2t},x_{4t})+u_{t}^{2}\\right)\\\\ \\mbox{s.t.}&x_{1t+1}=x_{1t}+hx_{3t},\\\\ &x_{2t+1}=x_{2t}+hx_{4t},\\\\ &x_{3t+1}=x_{3t}-h\\frac{mg}{m_{1}}x_{2t}+h\\frac{1}{m_{2}}u,\\\\ &x_{4t+1}=x_{4t}-h\\frac{(m_{1}+m_{2})g}{lm_{2}}x_{2t}+h\\frac{l}{m_{1}}u,\\\\ &\\mathbb{E}e(x_{2t},x_{4t})\\leq e(2,0),\\\\ &\\mathbb{E}u_{t}^{2}-\\mathbb{E}u_{t}x_{t}^{\\top}(\\mathbb{E}x_{t}x_{t}^{\\top})^{-1}\\mathbb{E}x_{t}u_{t}\\geq10^{4}h,\\end{array}\\tag{23a}$$\nwhich is (almost) an instance of the infinite-horizon design problem (11). The dynamic constraint of this control problem is obtained from a linearization of (22) in the lower equilibrium and a subsequent Euler discretization. Unlike (11), this problem incorporates the additional constraint (23b), which ensures that the designed control policy excites the pendulum sufficiently. Without this constraint, the optimal solution of the above problem would yield the zero policy. Fortunately, this constraint can be convexified in \u03a3 using the Schur complement.\n\nWe also highlight the term e(x2t,x4t) =\n1\n2*mglx*2\n2t +\n1\n2gl2x2\n4t which enters negatively into the cost. This term is a quadratic approximation of the pendulum energy mgl(1 \u2212 cos(x2t)) + 1\n2gl2x2\n4t making sure that the control policy pursues the goal of increasing the energy of the pendulum. Note that once the pendulum is supplied with enough energy, it will eventually reach the upper position. The energy constraint (23a) caps the average pendulum energy to the value of the potential energy of a pendulum at an angle of 2rad \u2248 115\u25cb preventing the pendulum energy from going to infinity.\n\nTwo exemplary closed loop trajectories of the proposed control policy with the nonlinear pendulum model (22) are depicted in Figure 3 and Figure 4. First, the policy for the escape from the lower equilibrium is active and supplies the pendulum with energy. Then, the pendulum reaches the region of attraction of a second controller and is stabilized in the upward position. The moment for the switching of control policies is determined based on a Laypunov function for the second controller.\n\nThe presented control strategy for the swing up of an inverted pendulum stands out by being composed of two linear controllers. This means that we can take advantage of the extensive possibilities for tuning linear controllers.\n\nFor example, we can add additional constraints to the optimal control problem, such as restrictions on the position and speed of the cart. We also have the possibility of including a frequency-dependent cost in the optimization problem [22]. This allows us to put a penalty on high frequency control inputs. Finally, we mention that the controller in the form of two linear controllers requires only minimal online computation.\n\nVIII. Conclusion We present a convexification strategy for linear state feedback synthesis problems with affine, time-varying system dynamics, random initial state, and additive stochastic noise. This convexification is based on moment matrices and permits for non-convex quadratic costs and constraints. Like second-order moment relaxations in non-convex quadratic programming, the case we study always permits for the extraction of a *solution*, which is actually optimal if we relax hard constraints to expectation constraints. In addition, we identify the parameters of known convexification strategies based on the dualization lemma with blocks of our moment matrices. Hence, we interpret the new decision variables after applying the dualization lemma as moment variables.\n\nReferences\n[1] B. Bamieh. Linear-quadratic problems in systems and controls via\ncovariance representations and linear-conic duality: Finite-horizon\ncase. *arXiv preprint arXiv:2401.01422*, 2024.\n[2] S. Boyd, V. Balakrishnan, E. Feron, and L. ElGhaoui.\nControl\nsystem analysis and synthesis via linear matrix inequalities.\nIn\n1993 American Control Conference, pages 2147\u20132154, 1993.\n[3] S. Boyd, L. El Ghaoui, E. Feron, and V. Balakrishnan.\nLinear\nmatrix inequalities in system and control theory. SIAM, 1994.\n[4] C. Cathcart, M. Santos, S. Park, and N. E. Leonard. Proactive\nopinion-driven robot navigation around human movers, 2023.\n[5] V. Causevic, P. U. Abara, and S. Hirche.\nOptimal powerconstrained control of distributed systems with information constraints. *Asian Journal of Control*, 24(5):2049\u20132061, 2022.\n[6] P. Gahinet and P. Apkarian. A linear matrix inequality approach to\nH\u221e control. *International journal of robust and nonlinear control*,\n4(4):421\u2013448, 1994.\n[7] A. Gattami. Generalized linear quadratic control. IEEE Transactions on Automatic Control, 55(1):131\u2013136, 2009.\n[8] A. Gattami and B. Bamieh. Simple covariance approach to H\u221e-\nanalysis.\nIEEE Transactions on Automatic Control, 61(3):789\u2013\n794, 2015.\n[9] F. Holtorf and C. Rackauckas. Stochastic optimal control via local\noccupation measures. *arXiv preprint arXiv:2211.15652*, 2022.\n[10] R. A. Howard. Dynamic Probabilistic Systems, Volume I: Markov\nModels, volume 1. Courier Corporation, 2012.\n[11] R. Kalman.\nOn the general theory of control systems.\nIFAC\nProceedings Volumes, 1(1):491\u2013502, 1960. 1st International IFAC\nCongress on Automatic and Remote Control, Moscow, USSR, 1960.\n[12] M. Kamgarpour and T. Summers. On infinite dimensional linear\nprogramming approach to stochastic control. *IFAC-PapersOnLine*,\n50(1):6148\u20136153, 2017.\n[13] H. K. Khalil. *Control of nonlinear systems*. Prentice Hall, New\nYork, NY, 2002.\n[14] J. B. Lasserre, D. Henrion, C. Prieur, and E. Tr\u00e9lat. Nonlinear optimal control via occupation measures and lmi-relaxations. SIAM journal on control and optimization, 47(4):1643\u20131666, 2008.\n[15] D. Lee.\nMulti-objective lqg design with primal-dual method.\nMathematics, 11(8):1857, 2023.\n[16] J.\nPark\nand\nS.\nBoyd.\nGeneral\nheuristics\nfor\nnonconvex\nquadratically constrained quadratic programming. arXiv preprint arXiv:1703.07870, 2017.\n[17] J. Pitman. Occupation measures for markov chains. Advances in\nApplied Probability, 9(1):69\u201386, 1977.\n[18] A. Rantzer. A dual to lyapunov's stability theorem. Systems &\nControl Letters, 42(3):161\u2013168, 2001.\n[19] A. Rantzer.\nLinear quadratic team theory revisited.\nIn 2006\nAmerican Control Conference, pages 5 pp.\u2013, 2006.\n[20] C. Scherer and S. Weiland. Linear matrix inequalities in control.\nLecture Notes, Dutch Institute for Systems and Control, Delft, The\nNetherlands, 3(2), 2000.\n[21] C. W. Scherer. Robust mixed control and linear parameter-varying\ncontrol with full block scalings.\nIn Advances in linear matrix\ninequality methods in control, pages 187\u2013207. SIAM, 2000.\n[22] G. Stein and M. Athans. The LQG/LTR procedure for multivariable feedback control design.\nIEEE Transactions on Automatic\nControl, 32(2):105\u2013114, 1987.\n[23] U. Vaidya and P. G. Mehta.\nLyapunov measure for almost\neverywhere stability. *IEEE Transactions on Automatic Control*,\n53(1):307\u2013323, 2008.\n[24] H. J. van Waarde, J. Eising, H. L. Trentelman, and M. K. Camlibel.\nData informativity: A new perspective on data-driven analysis and\ncontrol. *IEEE Transactions on Automatic Control*, 65(11):4753\u2013\n4768, 2020."
    }
]