[
    {
        "level": "##",
        "title": "Abstract",
        "content": "\nIn this paper, we study the feature learning ability\nof two-layer neural networks in the mean-field\nregime through the lens of kernel methods. To\nfocus on the dynamics of the kernel induced by\nthe first layer, we utilize a two-timescale limit,\nwhere the second layer moves much faster than\nthe first layer. In this limit, the learning prob-\nlem is reduced to the minimization problem over\nthe intrinsic kernel. Then, we show the global\nconvergence of the mean-field Langevin dynam-\nics and derive time and particle discretization er-\nror. We also demonstrate that two-layer neural\nnetworks can learn a union of multiple reproduc-\ning kernel Hilbert spaces more efficiently than\nany kernel methods, and neural networks aquire\ndata-dependent kernel which aligns with the tar-\nget function. In addition, we develop a label noise\nprocedure, which converges to the global opti-\nmum and show that the degrees of freedom ap-\npears as an implicit regularization.\n"
    },
    {
        "level": "##",
        "title": "1. Introduction",
        "content": "\nAlthough deep learning has achieved great success in var-\nious fields, the theoretical understanding is still limited.\nSeveral works studied the relation between deep learning\nand kernel medhods, which are well-studied in the machine\nlearning community. A line of work has shown that the\ntraining dynamics of infinite-width neural networks can be\napproximated by linearized dynamics and the corresponding\nkernel is called neural tangent kernel (NTK) (Jacot et al.,\n2018; Arora et al., 2019b). Furthermore, generalizability of\nneural networks is shown to be characterized by the spec-\ntral properties of the NTK (Arora et al., 2019a; Nitanda &\nSuzuki, 2020). However, the NTK regime is reffered as a\nlazy regime and cannot explain the feature learning ability\nto adapt the intrinsic structure of the data since neural net-\n\n    1Department of Mathematical Informatics, the University of\nTokyo, Tokyo, Japan 2Center for Advanced Intelligence Project,\nRIKEN, Tokyo, Japan. Correspondence to: Shokichi Takakura\n<masayoshi361@g.ecc.u-tokyo.ac.jp>.\n\nworks behave as a static kernel machine in the NTK regime.\nOn the other hand, several works have shown the superiority\nof the neural networks to the kernel methods in terms of\nthe sample complexity (Barron, 1993; Yehudai & Shamir,\n2019; Hayakawa & Suzuki, 2020). Thus, as shown in sev-\neral empirical studies (Atanasov et al., 2021; Baratin et al.,\n2021), neural networks must acquire the data-dependent\nkernel by gradient descent. However, it is challenging to\nestablish a beyond NTK results on the feature learning of\nneural networks with gradient-based algorithm due to the\nnon-convexity of the optimization landscape.\n\nOne promising approach is the mean-field analysis (Mei\net al., 2018; Hu et al., 2020), which is an infinite-width\nlimit of the neural networks in a different scaling than the\nNTK regime. In the mean-field regime, the optimization\nof 2-layer neural networks, which is non-convex in general,\nis reduced to the convex optimization problem over the\ndistribution on the parameters. Exploiting the convexity\nof the problem, several works (Nitanda & Suzuki, 2017;\nMei et al., 2018; Chizat & Bach, 2018) have shown the\nconvergence to the global optimum. Recently, quantitative\noptimiztion guarantees has been established for the mean-\nfield Langevin dynamics (MFLD) which can be regarded as\na continuous limit of a noisy gradient descent (Chizat, 2022;\nNitanda et al., 2022). Moreover, very recently, uniform-in-\ntime results on the particle discretization error have been\nobtained (Chen et al., 2023; Suzuki et al., 2022; 2023a).\nThis allows us to extend results effectively from infinite-\nwidth neural networks to finite-width neural networks.\n\nAlthough the mean-field limit allows us to analyze the fea-\nture learning in neural networks, the connection between\nmean-field neural networks and its corresponding kernel is\nstill unclear. To establish the connection to the previous\nworks (Jacot et al., 2018; Suzuki, 2018; Ma & Wu, 2022) on\nthe relationship between neural networks and kernel meth-\nods, we address the following question:\nIs it possible to learn the optimal kernel through the MFLD?\nFuthermore, can this kernel align with the target function\nby excluding the effect of noise?\n\nTo analyze the dynamics of the kernel inside the neural net-\nworks, we adopt a two-timescale limit (Marion & Berthier,\n2023), which separates the dynamics of the first layer and\nthe second layer. Then, we establish the connection between\n\nneural networks training and kernel learning (Bach et al.,\n2004), which involves selecting the optimal kernel for the\ndata. We provide the global convergence gurantee of the\nMFLD by showing the convexity of the objective functional\nand derive the time and particle discretization error. Then,\nwe prove that neural networks can aquire data-dependent\nkernel and achieve better sample complexity than any linear\nestimators including kernel methods for a union of multiple\nRKHSs. We also investigate the alignment with the target\nfunction and the degrees of freedom of the acquired kernel,\nwhich measures the complexity of the kernel, and develop\nthe label noise procedure which provably reduces the de-\ngrees of freedom by just adding the label noise. Finally, we\nverify our theoretical findings by numerical experiments.\nOur contribution can be summarized as follows:\n\n- We prove the convexity of the objective functional with\nrespect to the first layer distribution and the global convergence of the MFLD in two-timescale limit in spite of the complex dependency of the second layer on the distribution of the first layer. We also derive the time and particle discretization error of the MFLD.\n- We show that neural networks can adapt the intrinsic\nstructure of the target function and achieve a better sample complexity than kernel methods for a variant of Baron space (Ma & Wu, 2022), which is a union of multiple RKHS.\n- We study the training dynamics of the kernel induced\nby the first layer and show that the alignment is increased during the training and achieve \u03a9(1) alignment while the kernel alignment at the initialization is O(1/\n\u221a\nd) for a single-index model, where d is the\ninput dimension. We also show that the presence of the intrinsic noise induces a bias towards the large degrees of freedom. To alleviate this issue, we propose the label noise procedure to reduce the degrees of freedom and prove the linear convergence to the global optimum.\n"
    },
    {
        "level": "##",
        "title": "1.1. Related Works",
        "content": "\nRelation between Neural Networks and Kernel Methods\nSuzuki (2018) derived the generalization error bound for\ndeep learning models using the notion of the degrees of\nfreedom in the kernel literature. Ma & Wu (2022) charac-\nterized the function class, which two-layer neural networks\ncan approximate, by a union of multiple RKHSs. However,\nthey did not give any optimization gurantee. Atanasov et al.\n(2021) pointed out the connection between training of neural\nnetworks and kernel learning, but their analysis is limited to\nlinear neural networks with whitened data.\n\nMean-field Analysis\n                     Chen et al. (2020) conducted NTK-\ntype analysis using mean-field limit. However, their anal-\nysis relies on the closeness of the first-layer distribution\n\nto the initial distribution. Several works have shown that\nthe superiority of the mean-field neural networks to kernel\nmethods including NTK with global optimization guaran-\ntee. For example, Suzuki et al. (2023b) derived a linear\nsample complexity with respect to the input dimension d\nfor k-sparse parity problems while kernel methods require\n\u03a9(dk) samples. In addition, Mahankali et al. (2023) showed\nthe superiority of the mean-field neural networks to the ker-\nnel methods for even quartic polymonial. However, these\nworks fix the second layer during the training to ensure the\nboundedness of each neuron. Unlike these studies, we con-\nsider trainable second layer, and focus on the relationship\nbetween neural networks and kernel machines and its impli-\ncation to the feature learning ability of neural networks.\n\nTwo-timescale Limit\n                     Two-timescale limit is introduced\nto the analysis for training of neural networks in Marion\n& Berthier (2023). They provided the global convegence\ngurantee for simplified neural networks but their analysis\nis limited to the single input setting and the relation to the\nkernel learning was not discussed. Bietti et al. (2023) lever-\naged the two-timescale limit to analyze the training of neural\nnetworks with a multi-index model and show the saddle-\nto-saddle dynamics. However, they used non-parametric\nmodel for the second layer, which is different from practical\nsettings.\n\nFeature Learning in Two-layer Neural Networks\n                                               Aside\nfrom the mean-field analysis, there exists a line of work\nwhich studies the feature learning in (finite-width) two-layer\nneural networks (Damian et al., 2022; Mousavi-Hosseini\net al., 2022). For instance, Damian et al. (2022) show that\nthe random feature model with the first layer parameter up-\ndated by one-step gradient descent can learn p-degree poly-\nnomial with O(d) samples while kernel methods require\n\u03a9(dp) samples. However, most of these works consider\ntwo-stage optimization procedure, where the first layer is\ntrained before proceeding to train the second layer.\n\nImplicit Bias of Label Noise\n                              Implicit bias of label noise\nhas been intensively studied recently (Damian et al., 2021;\nLi et al., 2021; Vivien et al., 2022). For example, Li et al.\n(2021) developed a theoretical framework to analyze the im-\nplicit bias of noise in small noise and learning rate limit and\nprove that the label noise induces bias towards flat minima.\nOn the other hand, we elucidate the implicit regularization\nof label noise on the kernel inside the neural networks.\n"
    },
    {
        "level": "##",
        "title": "1.2. Notations",
        "content": "\nWe write the expectation with respect to X \u223c \u00b5 by EX\u223c\u00b5[\u00b7]\nor E\u00b5[\u00b7].\n\nKL denotes the Kullback-Leibler divergence negative entropy Ent(\u00b5) = E\u00b5[log \u00b5]. N(v, \u03a3) denotes the\n\nKL(\u03bd | \u00b5) =\n          \ufffd\n            log\n              \ufffd\n               \u00b5(w)\n               \u03bd(w)\n                  \ufffd\n                   d\u00b5(w) and Ent denotes the\n\nGaussian distribution with mean v and covariance \u03a3 and\n\u03c5(S) for S \u2282 Rd denotes the uniform distribution on S. P\ndenotes the set of probability measures on Rd\u2032 with finite second moment. For a matrix A, \u2225A\u2225op denotes the operator norm with respect to *\u2225\u00b7\u2225*2 and \u2225A\u2225F denotes the Frobenius norm. For an operator A : L2(\u00b5) \u2192 L2(\u00b5), \u2225A\u2225op denotes the operator norm with respect to *\u2225\u00b7\u2225*L2(\u00b5). For l : R2 \u2192 R,\n\u22021l denotes the partial derivative with respect to the first argument. For a symmetric matrix A, \u03bbmin(A) denotes the minimum eigenvalue of A. With a slight abuse of notation, we use f(X) for f : Rd \u2192 R and X = [x(1), . . . , x(n)]\u22a4 \u2208\nRn\u00d7d to denote [f(x(1))*, . . . , f*(x(n))]\u22a4.\n"
    },
    {
        "level": "##",
        "title": "2. Problem Settings 2.1. Mean-Field And Two-Timescale Limit In Two-Layer Neural Networks",
        "content": "\nGiven input x \u2208 Rd, let us consider the following two-layer neural network model:\n\n$$f(x;a,\\{w_{i}\\}_{i=1}^{N})=\\frac{1}{N}\\sum_{i=1}^{N}a_{i}h(x;w_{i}),$$\n\nwhere $a_{i}\\in\\mathbb{R}$, $w_{i}\\in\\mathbb{R}^{d^{\\prime}}$ and $h(x;w_{i})$ is the activation function with parameter $w_{i}$.\n\nMean-field limit of the above model is defined as an integral over neurons:\n\n$$f(x;P):=\\int ah(x;w)P(\\mathrm{d}a,\\mathrm{d}w),\\tag{1}$$\n\nwhere P is a probability distribution over the parameters of\nthe first and second layers. However, in this formulation,\nthe first and the second layer are entangled, and thus it is\ndifficult to characterize the feature learning, which takes\nplace in the first layer. To alleviate this issue, we consider\nthe following formulation:\n\n$$f(x;a,\\mu):=\\int a(w)h(x;w)\\mathrm{d}\\mu(w),$$\n\nwhere $a(w)=\\int aP(\\mathrm{d}a\\mid w)$ and $\\mu(w)=\\int P(a,w)\\mathrm{d}a$ is the marginal distribution of $w$. Similar formulation can be found in Fang et al. (2019). This formulation explicitly separates the first and the second layer, which allows us to focus on the feature learning dynamics in the first layer. More generally, we consider the multi-task learning settings. That is, $f(x;a,\\mu):\\mathbb{R}^{d}\\to\\mathbb{R}^{T}$ is defined by\n\n$$f_{i}(x;a,\\mu):=\\int a^{(i)}(w)h(x;w)\\mathrm{d}\\mu(w),$$\n\nwhere $a:\\mathbb{R}^{d^{\\prime}}\\to\\mathbb{R}^{T}$ is the second layer and $a^{(i)}$ is the $i$-th component of $a$. Note that the first layer $\\mu(w)$ is shared among tasks.\n\nLet \u03c1 be the true or empirical distribution of the pair of input and output (*x, y*) \u2208 Rd+T , and \u03c1X be the marginal distribution of x. Then, for *\u03bb >* 0, the (regularized) risk is defined by\n\n$$\\begin{array}{l}{{L(a,\\mu)=\\frac{1}{T}\\sum_{i=1}^{T}\\mathbb{E}_{\\rho}[l_{i}(f_{i}(x;a,\\mu),y)],}}\\\\ {{F(a,\\mu)=L(a,\\mu)+\\lambda\\mathbb{E}_{\\mu}[r(a(w),w)],}}\\end{array}$$\nwhere li is the loss function for the i-th task, and r is the regularization term. In this paper, we consider l2-regularization r(*a, w*) = \u03bba\n2T\n\ufffdT\ni=1 a(i)2 + \u03bbw\n2 \u2225w\u22252\n2, where a = (a(i))T\ni=1\nand \u03bba, \u03bbw > 0. We define \u00af\u03bba = \u03bb\u03bba, \u00af\u03bbw = \u03bb\u03bbw, and\n\u03bd = N(0*, I/\u03bb*w) for notational simplicity.\n\nTo separate the dynamics of the first and second layer, we\nintroduce the two-timescale limit (Marion & Berthier, 2023),\nwhere the second layer moves much faster than the first layer.\nIn this limit, the first layer a(i) converges instantaneously to\nthe unique optimal solution of mina E\u03c1[l(f(x; a, \u00b5), y)] +\n\u00af\u03bba\n 2 \u2225a\u22252\n      L2(\u00b5) since F(a, \u00b5) is strongly convex with respect\n\nto a, As shown in the next section, \u2225a\u22252\n                                        L2(\u00b5) corresponds\nto the RKHS norm for the kernel induced by the first layer.\nSince the optimal second layer is a functional of the first\nlayer distribution \u00b5, we write a\u00b5 for the optimal solution.\nThen, the learning problem is reduced to the minimization\nof the limiting functional G(\u00b5) = F(a\u00b5, \u00b5). We also define\nU(\u00b5) by U(\u00b5) := L(a\u00b5, \u00b5) +\n                               \u00af\u03bb\n                               2T E\u00b5[\u2225a\u00b5(w)\u22252\n                                              2]\n\nThroughout the paper, we assume that h(x; wi) satisfies\nAssumption 2.1. For example, tanh(u \u00b7 x + b)(w = (u, b))\nsatisfies the assumption if E\u03c1X[\u2225x\u22252\n                               2], E\u03c1X[\u2225x\u22254\n                                          2] are finite.\n\nAssumption 2.1. h(x; w) is twice differentiable with re-\nspect to w and there exist constants cR, cL > 0 such\nthat supw |h(x; w)| \u2264 1, E\u03c1X[supw \u2225\u2207wh(x; w)\u22252\n                                             2] \u2264\nc2\nR, E\u03c1X[supw\n            \ufffd\ufffd\u22072\n               wh(x; w)\n                       \ufffd\ufffd2\n                        op] \u2264 c2\n                               L.\n"
    },
    {
        "level": "##",
        "title": "2.2. Kernel Induced By The First Layer",
        "content": "\nLet us define the kernel induced by the first layer as follows:\n\n$$k_{\\mu}(x,x^{\\prime})=\\int h(x;w)h(x^{\\prime};w)\\mathrm{d}\\mu(w).$$\n\nObviously, this is a symmetric positive definite kernel. It is well-known that there exists a unique RKHS $\\mathcal{H}_{\\mu}$ corresponding the kernel $k_{\\mu}$. Furthermore, the RKHS norm $\\|f\\|_{\\mathcal{H}_{\\mu}}$ is equal to the minimum of $\\|a\\|_{L^{2}(\\mu)}$ over all $a$ such that $f(x;a,\\mu)=\\int a(w)h(x;w)\\mathrm{d}\\mu(w)$(Bach, 2017). Thus, the learning problem of the second layer is equivalent to the following optimization problem:\n\n$$\\min_{f_{i}\\in\\mathcal{H}_{\\mu}}\\sum_{i=1}^{T}\\mathbb{E}_{\\rho}[l_{i}(f_{i}(x),y)]+\\sum_{i=1}^{T}\\frac{\\bar{\\lambda}_{a}}{2}\\|f_{i}\\|_{\\mathcal{H}_{\\mu}}^{2}.\\tag{2}$$\nThus, learning first layer is equivalent to kernel learning (Bach et al., 2004), which choosing suitable RKHS\nH\u00b5.\n"
    },
    {
        "level": "##",
        "title": "2.3. Mean-Field Langevin Dynamics",
        "content": "\nWe optimize the limiting functional G(\u00b5) by the mean-field Langevin dynamics (MFLD):\n\n$$\\mathrm{d}w_{t}=-\\mathbf{\\nabla}_{w}{\\frac{\\delta G(\\mu)}{\\delta\\mu}}\\mathrm{d}t+{\\sqrt{2\\lambda}}\\mathrm{d}B_{t},$$\nwhere w0 \u223c \u00b50 := \u03bd, (Bt)t\u22650 is the d\u2032-dimensional Brownian motion, and \u03b4G(\u00b5)\n\u03b4\u00b5\nis the first variation of G(\u00b5). The Fokker-Planck equation of the above SDE is given by\n\n$$\\partial_{t}\\mu_{t}=\\lambda\\Delta\\mu_{t}+\\mathbf{\\nabla}\\cdot\\left[\\mu_{t}\\mathbf{\\nabla}{\\frac{\\delta G(\\mu)}{\\delta\\mu}}\\right],$$\nwhere \u00b5t is the distribution of wt. It is known that the MFLD is a Wasserstein gradient flow which minimizes the entropy-regularized functional: G(\u00b5) := G(\u00b5) + \u03bb Ent(\u00b5).\n\nTo implement the MFLD, we need time and particle dis-\ncretization (Chen et al., 2023; Suzuki et al., 2022; 2023a).\nFor a set of N particles W = {wi}N\n                              i=1, we define the empir-\nical distribution \u00b5W = 1\n\n              N\n                \ufffdN\n                  i=1 \u03b4wi. Let Wk = {w(k)\n                                i\n                                  }N\n                                  i=1\nbe a set of N particles at the k-th iteration, and \u00b5(N)\n                               k\n                                  be a\ndistribution of Wk on Rd\u2032\u00d7N. Then, at each step, we update\nthe particles as follows:\n\n$$w_{i}^{(k+1)}=w_{i}^{(k)}-\\eta\\mathbf{\\nabla}\\frac{\\delta G(\\mu w_{k})}{\\delta\\mu}(w_{i}^{(k)})+\\sqrt{2\\eta\\lambda}\\xi_{i}^{(k)},$$\nwhere *\u03b7 >* 0 is the step size and \u03be(k)\ni\n\u223c N(0, I) are i.i.d.\n\nGaussian noise. This can be regarded as a noisy gradient descent.\n"
    },
    {
        "level": "##",
        "title": "3. Convergence Analysis",
        "content": "\n\u03bb\n\u03b4G(\u00b5)\n\nAs shown in Nitanda et al. (2022); Chizat (2022), the con-\nvergence of the MFLD depends on the convexity of the\nfunctional and the properties of the proximal Gibbs distri-\nbution, which is defined as p\u00b5(w) \u221d exp\n                                    \ufffd\n                                     \u2212 1\n\n                                            \u03b4\u00b5 (w)\n                                                   \ufffd\n                                                     .\nIn the training of neural networks, the convexity is usually\nensured by the linearity of f with respect to distribution as\nin Eq. (1). On the other hand, in the two-timescale limit,\nf(x; \u00b5) := f(x; a\u00b5, \u00b5) is not linear with respect to \u00b5 be-\ncause the second layers a\u00b5 depend on \u00b5 in a non-linear way.\nHowever, we can prove that the functional G(\u00b5) is convex\nand its first variation can be written in a simple form if\n{li}T\n   i=1 are convex.\n\nTheorem 3.1. Assume that the losses {li}T\ni=1 are convex.\n\nThen, the limiting functional G(\u00b5) is convex. That is, it holds that\n\n$$G(\\mu_{1})+\\int\\frac{\\delta G(\\mu_{1})}{\\delta\\mu}(w)(\\mu_{2}(w)-\\mu_{1}(w))\\mathrm{d}w\\leq G(\\mu_{2})$$\nfor any \u00b51, \u00b52 \u2208 P*. In addition, the first variation of* G(\u00b5)\nis given by\n\n$$\\delta G\\over\\delta\\mu}(\\mu)(w)=\\lambda\\biggl{(}-{\\lambda_{a}\\over2T}||a_{\\mu}(w)||_{2}^{2}+{\\lambda_{w}\\over2}||w||_{2}^{2}\\biggr{)}.\\tag{3}$$\nSee Appendix B.1 for the proof. We remark that the convexity holds for general regularization term r which is strongly convex with respect to a.\n\nThe convergence rate of the MFLD depends on the constant in the log-Sobolev inequality for the proximal distribution p\u00b5.\n\nDefinition 3.2. We say a probability distribution \u00b5 satisfies log-Sobolev inequality with constant *\u03b1 >* 0 if for all smooth function g : Rd \u2192 R with E\u00b5[g2] < \u221e,\n\n$$\\mathbb{E}_{\\mu}[g^{2}\\log g^{2}]-\\mathbb{E}_{\\mu}[g^{2}]\\log\\mathbb{E}_{\\mu}[g^{2}]\\leq{\\frac{2}{\\alpha}}\\mathbb{E}_{\\mu}[\\left\\|\\nabla g\\right\\|^{2}].$$\nTo derive the LSI constant \u03b1, we assume either of the following conditions on the loss functions:\n\nAssumption 3.3. li is squared loss. That is, li(z, y) =\n1\n2(z \u2212 y)2. In addition, |yi| \u2264 cl a.s. for some constant\ncl > 0.\n\nAssumption 3.4. li is convex and twice differentiable\nwith respect to the first argument and |\u22021li(z, y)|\n                                               \u2264\ncl,\n  \ufffd\ufffd\u22022\n    1li(z, y)\n           \ufffd\ufffd \u2264 1 for any z, y \u2208 R.\n\nThe latter assumption is satisfied by several loss functions\nsuch as logistic loss.\n                        Then, using the formula for the\nfirst variation, we can derive the LSI constant applying\nthe Holley-Stroock argument (Holley & Stroock, 1987) for\nbounded perturbation.\n\nLemma 3.5. Assume that each li satisfies Assumption 3.3\nor 3.4. Then, the proximal distribution p\u00b5 for any \u00b5 \u2208 P\n\nsatisfies LSI with constant \u03b1 = \u03bbw exp\n                                        \ufffd\n                                         \u22122 \u03bbac2\n                                                l\n                                              \u00af\u03bb2a\n\n\ufffd\n .\n\nThe proof can be found in Appendix B.2.\nRemark 3.6. In the standard formulation (1), the first varia-\ntion of F(P) = E\u03c1[l(f(x; P), y)] + \u03bbEP [r(a, w)] is given\nby \u03b4F\n\n   \u03b4P = E\u03c1[\u22021l(f(x; P), y)ah(x; w)] + \u03bbr(a, w). Then,\nE\u03c1[\u22021l(f(x; P), y)ah(x; w)] is not bounded nor Lipschitz\ncontinuous with respect to (a, w), even if \u22021l and h is\nbounded. Therefore, without two-timescale limit, it is diffi-\ncult to obtain a LSI constant even for the single output set-\nting. Indeed, previous works fix the second layer or clip the\noutput using some bounded function (Chizat, 2022; Nitanda\net al., 2022; Suzuki et al., 2023a) to ensure the boundedness\nor Lipschitz continuity of the output of neurons.\n\nCombining above results, we can show the linear conver-\ngence of the MFLD.\nTheorem 3.7. Let \u00b5\u2217 be the minimizer of G(\u00b5) and\nGN(\u00b5(N)\n     k\n        ) = NEW \u223c\u00b5(N)\n                     k\n                        [G(\u00b5W )] + \u03bb Ent(\u00b5(N)\n                                           k\n                                               ). Then,\nfor the constant \u03b1 in Lemma 3.5, \u00b5t satisfies\n\nG(\u00b5t) \u2212 G(\u00b5\u2217) \u2264 exp(\u22122\u03b1\u03bbt)(G(\u00b50) \u2212 G(\u00b5\u2217))\n\nfor any 0\n            \u2264\n                t.\n                     Furthermore,\n                                  for any \u03b7\n                                              <\nmin {1/4, 1/(4\u03bb\u03b1)}, \u00b5(N)\n                   k\n                      satisfies\n\n1\nN GN(\u00b5(N)\n      k\n         ) \u2212 G(\u00b5\u2217)\n\n\u2264 exp(\u2212\u03b1\u03bb\u03b7k/2)(GN(\u00b5(N)\n             0\n              ) \u2212 G(\u00b5\u2217)) + \u00af\u03b4\u03b7,N,\n\nwhere \u00af\u03b4\u03b7,N = O( 1\n\nN + \u03b7).\n\nSee Appendix B.3 for the proof and the concrete expres-\nsion of discretization error \u00af\u03b4\u03b7,N. The proof is based on the\nframework in Suzuki et al. (2023a). To obtain discretization\nerror, we need to prove some additional conditions on the\nsmoothness of the objective functional. This is far from triv-\nial due to the non-linear dependency of a\u00b5 on \u00b5. Note that\nwe cannot apply the arguments in Chizat (2022); Nitanda\net al. (2022); Suzuki et al. (2023a) without two-timescale\nlimit since they assume the boundedness or Lipschitz con-\ntinuity of each neuron. See also Remark 3.6 for detailed\ndiscussion.\n\nFurthermore, the convergence of the loss function in the\ndiscretized setting can be transferred to the convergence of\nthe function value of the neural networks as shown in the\nfollowing proposition.\nProposition 3.8. Assume that h(x; w) is cR-Lipschitz con-\ntinuous with respect to w for any x \u2208 S, where S is some\nsubset of Rd. Let \u2206 = c2\n                      R\n\n\u03bb\u03b1(GN(\u00b5(N)\n       k\n          )\u2212NG(\u00b5\u2217))+ c2\n                       RG(\u00b50)\n\nsubset of $\\mathbb{R}^{d}$. Let $\\Delta=\\frac{c_{R}^{r}}{\\lambda\\alpha}(\\mathcal{G}^{N}(\\mu_{k}^{(N)})-N\\mathcal{G}(\\mu^{*}))+\\frac{c_{R}^{r}\\mathcal{G}(\\mu_{0})}{\\lambda_{w}}$. Then, we have_\n\n$$\\mathbb{E}_{W_{k}\\sim\\mu_{k}^{(N)}}\\left[\\sup_{\\begin{subarray}{c}(x,y)\\\\ \\in S\\times S\\end{subarray}}\\left|k_{\\mu_{W_{k}}}(x,y)-k_{\\mu^{*}}(x,y)\\right|^{2}\\right]=O\\bigg{(}\\frac{\\Delta}{N}\\bigg{)}.$$\n\n_In addition, if $l_{i}$ satisfies Assumption 3.3, then we have_\n\n$$\\mathbb{E}_{W_{k}\\sim\\mu_{k}^{(N)}}\\left[\\sup_{x\\in S}(f_{i}(x;\\mu_{W_{k}})-f_{i}(x;\\mu^{*}))^{2}\\right]$$ $$=O\\bigg{(}\\frac{c_{0}^{2}(\\bar{\\lambda}_{a}^{2}+1)}{\\bar{\\lambda}_{a}^{4}}\\cdot\\frac{\\Delta}{N}\\bigg{)}.$$\n\nSee Appendix B.4 for the proof. Note that this result is not\ncovered by Lemma 2 in Suzuki et al. (2023b) since their\nanalysis relies on the Lipschitz continuity of each neuron.\nIn the following sections, we consider infinite-width neural\nnetworks trained by the MFLD for simplicity, but the results\ncan be transferred via this proposition to the finite-width\nneural networks trained by the discretized MFLD.\n"
    },
    {
        "level": "##",
        "title": "4. Generalization Error For Barron Spaces And Superiority To Kernel Methods",
        "content": "\nIn this section, we provide the separation of the general-\nization error between neural networks and kernel methods\nwhich cannot adapt the intrinsic structure of the target func-\ntion.\n\nLet D =\n        \ufffd\n          (x(i), y(i))\n                   \ufffdn\n                    i=1 be training data sampled from\nthe true distribution in an i.i.d. manner. We define X =\n\ufffd\nx(1), . . . , x(n)\ufffd\u22a4 \u2208 Rn\u00d7d, Yi = [y(1)\n                                i\n                                   , . . . , y(n)\n                                         i\n                                           ]\u22a4 \u2208 Rn\n\nand \u02c6\u03a3\u00b5 = E\u00b5[h(X; w)h(X; w)\u22a4]. In the following, we\nwrite the true distribution by \u03c1 and the empirical distribu-\ntion by \u02c6\u03c1. In addition, to distinguish the empirical risk\nand population risk, we write U\u03c1(\u00b5), G\u03c1(\u00b5) for the (regu-\nlarized) population risk and U\u02c6\u03c1(\u00b5), G\u02c6\u03c1(\u00b5) for the empirical\nrisk. Then, we assume the following.\n\nAssumption 4.1. the output yi for each task is generated by\nyi = f \u25e6\n     i (x) + \u03b5i, where f \u25e6\n                      i : Rd \u2192 R is the target function\nand \u03b5i is the noise, which follows \u03c5([\u2212\u03c3, \u03c3]) independently\nfor some \u03c3 \u2265 0.\n\nTo see the benefit of the feature learning or kernel learning,\nwe consider the following function class.\n\nDefinition 4.2 (KL-restricted Barron space). Let PM =\n{\u00b5 \u2208 P | KL(\u03bd | \u00b5) \u2264 M} for some M > 0. Then, we\ndefine the KL-restricted Barron space as\n\n$$\\mathcal{B}_{M}=\\big{\\{}f(x;a,\\mu)\\mid\\mu\\in\\mathcal{P}_{M},a\\in L^{2}(\\mu)\\big{\\}},$$\n\nand the corresponding norm as\n\n$$\\|g\\|_{\\mathcal{B}_{M}}=\\inf_{\\mu\\in\\mathcal{P}_{M},a\\in L^{2}(\\mu)}\\Big{\\{}\\|a\\|_{L^{2}(\\mu)}\\mid g(x)=f(x;a,\\mu)\\Big{\\}}.$$\n\nThis can be seen as a variant of Barron space in E et al.\n(2019); Ma & Wu (2022). Similar function classes are\nalso considered in Bach (2017) but they consider Frank-\nWolfe type optimization algorithm, which is different from\nusual gradient descent. We remark that Barron space can\nbe regarded as a union of RKHS: BM = \ufffd\n\n                                    \u00b5\u2208PM H\u00b5 and\nthe norm \u2225f\u2225BM is equal to the minimum of \u2225f\u2225H\u00b5 over\nall \u00b5 \u2208 PM (Ma & Wu, 2022).\n\nTo obtain the generalization guarantee, we utilize the Rademacher complexity. The Rademacher complexity of a function class $\\mathcal{F}$ of functions $f:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{T}$ is defined by\n\n$$\\mathfrak{R}(\\mathcal{F}):=\\mathbb{E}_{\\sigma}\\left[\\sup_{f\\in\\mathcal{F}}\\frac{1}{nT}\\sum_{i=1}^{n}\\sum_{t=1}^{T}\\sigma_{it}f_{t}(x_{i})\\right],$$\nwhere \u03c3it is an i.i.d. Rademacher random variable (P(\u03c3it =\n1) = P(\u03c3it = \u22121) = 1/2). Then, we have the following bound for the mean-field neural networks.\n\nLemma 4.3. Assume that h(x; w) satisfies Assumption 2.1.\n\nDefine a class of the mean-field neural networks by\n\n$$\\mathcal{F}_{R,M}=\\Big{\\{}f(x;a,\\mu)\\mid\\operatorname{KL}(\\nu\\mid\\mu)\\leq M,\\|a\\|_{L^{2}(\\mu)}^{2}\\leq R\\Big{\\}}.$$\n\n_Then, the Rademacher complexity of $\\mathcal{F}_{R,M}$ is bounded by_\n\n$$\\mathfrak{R}(\\mathcal{F}_{R,M})\\leq\\sqrt{R(4M\\ \\overline{+2T\\log2)}\\over nT}=O\\bigg{(}\\frac{R(M+T)}{nT}\\bigg{)}.$$\nThis is a generalization of the result in Chen et al. (2020).\n\nSee Appendix C.2 for the proof.\n\nThen, we can derive the generalization error bound for the\nempirical risk minimizer \u02c6\u00b5 = argmin\n                                 \u00b5\n                                     G\u02c6\u03c1(\u00b5). Note that the\n\nempirical risk minimizer \u02c6\u00b5 can be obtained by the MFLD\nas shown in Theorem 3.7.\n\nTheorem 4.4. Assume that Assumption 2.1 and 4.1 holds\nwith \u03c3 = 0, T = 1, and f \u25e6\n                     1 \u2208 BM, \u2225f \u25e6\n                               1 \u2225BM \u2264 R for given\nM, R > 0. In addition, let \u03bb = 1/\u221an, and \u03bba = 2M/R.\nThen, with probability at least 1 \u2212 \u03b4 over the choice of\ntraining examples, it holds that\n\n$$\\left\\|f(\\cdot;\\hat{\\mu})-f^{\\circ}\\right\\|_{L^{2}(\\rho_{X})}^{2}=O\\Biggl((R+1)\\sqrt{\\begin{matrix}M+1+\\log1/\\delta\\\\ n\\end{matrix}}\\Biggr).$$\nSee Appendix C.3 for the proof. Therefore, if R = O(1), the mean-field neural networks can learn the Barron space with n = O(M) samples.\n\nNext, we show the lower bound of the estimation error for\nkernel methods. For a given kernel k, a kernel method\nreturns a function of the form f(x) = \ufffdn\n                                       i=1 \u03b1ik(x, xi) for\n\u03b1i \u2208 R. This type of estimator is called linear estimator.\nThe following theorem gives the lower bound of the sample\ncomplexity for any linear estimators.\n\nTheorem 4.5. Fix m \u2208 N and let d \u2265 max{2, m} and \u03c1X\nbe the uniform distribution on {\u22121, 1}d and h(x; w) =\ntanh(u \u00b7 x + b), where w = (u, b) \u2208 Rd+1.\n                                           In addi-\ntion, let Hn \u2282 L2(\u03c1X) be a set of functions of the form\n\ufffdn\n  i=1 \u03b1ihi(x) and d(f, Hn) = inf \u02c6\n                               f\u2208Hn \u2225f \u2212 \u02c6f\u2225L2(\u03c1X).\nThen, there exist constants c1, c2 > 0 which is independent\nof d, such that, for every choice of fixed basis functions\nh1(x), . . . , hn(x), it holds that\n\n$$\\sup_{f\\in\\mathcal{B}_{M},\\|f\\|_{\\mathcal{B}_{M}}^{2}\\leq R}d(f,H_{n})\\geq\\frac{1}{4}$$\n\n_if $n\\leq N/2$ and $M=c_{1}d\\log d,R=c_{2}$ where $N=\\binom{d}{m}=\\Omega(d^{m})$._\nThe proof can be found in Appendix C.4. This theorem implies that any kernel estimator with n = o(dm) cannot learn the Barron space with M = \u03a9(d log d). This is in contrast to the mean-field neural networks which can learn the Barron space with n = O(d log d) samples as shown in Theorem 4.4. This is because the kernel methods cannot adapt the underlying RKHS under the target function.\n\nTherefore, feature learning or kernel learning is essential to obtain good generalization results.\n"
    },
    {
        "level": "##",
        "title": "5. Properties Of The Kernel Induced By The First Layer",
        "content": "\nIn the previous section, we proved that feature learning is\nessential to obtain good generalization results and two-layer\nneural networks trained by the MFLD can excel over kernel\nmethods. In this section, we study the properties of the\nkernel trained via the MFLD. We show that in regression\nproblem, the kernel induced by the first layer moves to in-\ncrease kernel and parameter alignment. We also proved that\nthe presence of the noise \u03b5 induces bias towards the large\ndegrees of freedom. To overcome this issue, we provide\nthe label noise procedure, which provably converges to the\nglobal minima of the objective functional with the degrees\nof freedom regularization.\n"
    },
    {
        "level": "##",
        "title": "5.1. Kernel And Parameter Alignment",
        "content": "\nFor simplicity, we consider the single output setting T = 1\nand define f \u25e6 = f \u25e6\n1 . In addition, we consider tanh activation h(x; w) = tanh(u \u00b7 x + b) (w = (*u, b*)) and assume that \u03c1X = N(0, I). To measure the adaptation of the kernel to the target function, we define the kernel alignment Cristianini et al. (2001). which is commonly used to measure the similarity between kernel and labels.\n\nDefinition 5.1. For \u00b5 *\u2208 P*, the empirical and population kernel alignment is defined by\n\n$$\\hat{A}(\\mu)=\\frac{f^{\\circ}(X)^{\\top}\\hat{\\Sigma}_{\\mu}f^{\\circ}(X)}{\\|f^{\\circ}(X)\\|_{2}^{2}\\Big{\\|}\\hat{\\Sigma}_{\\mu}\\Big{\\|}_{\\mathrm{F}}},$$ $$A(\\mu)=\\frac{\\mathbb{E}_{x\\sim\\rho_{X},x^{\\prime}\\sim\\rho_{X}}\\left[f^{\\circ}(x)k_{\\mu}(x,x^{\\prime})f^{\\circ}(x^{\\prime})\\right]}{\\mathbb{E}_{\\rho_{X}}\\left[f^{\\circ}(x)^{2}\\right]\\sqrt{\\mathbb{E}_{x\\sim\\rho_{X},x^{\\prime}\\sim\\rho_{X}}\\left[k(x,x^{\\prime})^{2}\\right]}}.$$\nNote that \u02c6A(\u00b5) and A(\u00b5) satisfy 0 \u2264 \u02c6A(\u00b5), A(\u00b5) \u2264 1 and larger A(\u00b5) means that the kernel is aligned with the target function. In this section, we consider the regression problem with squared loss. if \u03c3 = 0, the limiting functional U\u02c6\u03c1(\u00b5) has the following explicit formula.\n\n$$U_{\\hat{\\rho}}(\\mu)=\\frac{\\bar{\\lambda}_{a}}{2}f^{\\circ}(X)^{\\top}(\\hat{\\Sigma}_{\\mu}+n\\bar{\\lambda}_{a}I)^{-1}f^{\\circ}(X).$$\nFrom Jensen's inequality, we have\n\n$$\\hat{A}(\\mu)\\geq\\frac{\\bar{\\lambda}_{a}\\|f^{\\circ}(X)\\|_{2}^{2}}{2U_{\\hat{\\rho}}(\\mu)n}-\\bar{\\lambda}_{a}.$$\nSee Lemma D.2 for the detailed derivation. Therefore, the minimization of U\u02c6\u03c1(\u00b5) is equivalent to the maximization of the lower bound of the kernel alignment.\n\nTo derive a concrete expression of the kernel alignment, we\nassume that the target function f \u25e6 is a single-index model\n, which is a common structural assumption on the target\nfunction (Bietti et al., 2022).\n\nAssumption 5.2. There exist \u02dcf\n                              :\n                                R\n                                    \u2192\n                                       R, u\u25e6\n                                              \u2208\nRd (\u2225u\u25e6\u22252 = 1) such that \u02dcf is differentiable, \u2225 \u02dcf \u2032\u2225\u221e,\n\u2225 \u02dcf\u2225\u221e \u2264 1, Ez\u223cN(0,1)[ \u02dcf(z)] = 0, and f \u25e6(x) = \u02dcf(u\u25e6 \u00b7 x).\n\nWe also define the parameter alignment, which measures the similarity between the first layer parameters and the intrinsic direction of the target function.\n\nDefinition 5.3. For \u00b5 *\u2208 P*, the parameter alignment is defined as\n\n$$P(\\mu)=\\mathbb{E}_{(u,b)\\sim\\mu}\\left[\\frac{(u^\\top u^\\diamond)^2}{\\left\\|u\\right\\|^2}\\right].$$\n\nHere, we define $\\frac{(u^\\top u^\\diamond)^2}{\\left\\|u\\right\\|^2}=0$ for $u=0$.\n\nThis is the expected cosine similarity between parameters and the target direction, and thus 0 \u2264 P(\u00b5) \u2264 1. Note that larger P(\u00b5) means that the first layer parameters are aligned with the intrinsic direction of the target function.\n\nThen, we have the following result on the kernel for empirical risk minimizer \u02c6\u00b5.\n\nTheorem 5.4. Assume that Assumption 5.2 holds. Then, there exists universal constants c3, c4, c5 satisfying the following: Let \u02c6\u00b5 be the minimizer of G\u02c6\u03c1(\u00b5) with n \u2265\nc3(d log d+log 1/\u03b4), \u03bb = c4/(d log d)*, and* \u03bba = c5d log d for 0 < \u03b4 < 1 and d \u2265 2. Then, the kernel and parameter alignment for the initial distribution \u00b50 and the empirical risk minimizer \u02c6\u00b5 satisfies\n\n$A(\\mu_{0})=0(1/\\sqrt{d})$, $A(\\hat{\\mu})=0(1)$, $P(\\mu_{0})=0(1/d)$, $P(\\hat{\\mu})=0(1)$,\nwith probability at least 1 \u2212 \u03b4 over the choice of samples.\n\nSee Appendix D.2 for the proof. In high-dimensional setting d \u226b 1, A(\u02c6\u00b5), P(\u02c6\u00b5) = \u03a9(1) is a significant improvement over P(\u00b50) = O(1/d), A(\u00b50) = O(1/\n\u221a\n\n                                d) at the initial-\nization. For the parameter alignment, similar results are\nshown in Mousavi-Hosseini et al. (2022), but they train only\nthe first layer and use the norm of the irrelevant directions as\na measure of the alignment. On the other hand, we consider\n\nthe joint learning of the first and second layers and use the cosine similarity as a measure of the alignment. In addition, Atanasov et al. (2021) studied the kernel alignment of NTK, but their analysis is limited to linear neural networks.\n"
    },
    {
        "level": "##",
        "title": "5.2. Degrees Of Freedom And Label Noise",
        "content": "\nTo measure the complexity of the acquired kernel, we define the (empirical) degrees of freedom by\n\n$$d_{\\lambda}(\\mu)=\\mbox{tr}\\Big{[}\\hat{\\Sigma}_{\\mu}(\\hat{\\Sigma}_{\\mu}+n\\lambda I)^{-1}\\Big{]}$$\n\nfor $\\lambda>0$. This quantity is the effective dimension of the kernel $k_{\\mu}$ and plays a crucial role in the analysis of kernel regression (Caponente to & De Vito, 2007). In addition, it is known that the degrees of freedom is related to the compressibility of neural networks (Suzuki et al., 2020).\n\nUnder Assumption 4.1, each label Yi can be decom-\nposed as Yi = f \u25e6\n              i (X) + \u03b5i, where \u03b5i is the observation\nnoise.\n      Then, taking the expectation of U\u02c6\u03c1(\u00b5) with re-\nspect to \u03b5 yields E\u03b5[U\u02c6\u03c1(\u00b5)] = B \u2212 V + const., where\nB =\n      \u00af\u03bba\n      2T\n        \ufffdT\n          i=1 E\u03b5[f \u25e6\n                 i (X)\u22a4(\u02c6\u03a3\u00b5 + n\u00af\u03bbaI)\u22121f \u25e6\n                                     i (X)] and\nV =\n    \u00af\u03bba\u03c32\n\n      6n d\u00af\u03bba(\u00b5). See Lemma D.3 for the derivation. Here,\nB is the bias term, which corresponds to the alignment with\nthe target function as shown in the previous section, and V is\nthe variance term, which corresponds to the degrees of free-\ndom. Since \u2212V appears in E\u03b5[U\u02c6\u03c1(\u00b5)], minimizing U\u02c6\u03c1(\u00b5)\nleads to the larger variance and the degrees of freedom. We\nverify this phenomenon in Section 6.\n\nTo obtain good prediction performance, we need to mini-\nmize B+V and control the bias-variance tradeoff. Here, we\nconsider the following objective functional with the degrees\nof freedom regularization:\n\n$${\\mathcal{L}}(\\mu):={\\mathcal{G}}_{\\tilde{\\rho}}(\\mu)+\\frac{\\bar{\\lambda}_{a}\\tilde{\\sigma}^{2}}{6m}d_{\\bar{\\lambda}_{a}}(\\mu).$$\n\nHere, \u02dc\u03c3 \u2265 0 controls the strength of the regularization.\nSince this regularization is proportional to the variance V ,\nminimizing L(\u00b5) would lead to smaller variance and better\ngeneralization. To obtain the minimizer of the above func-\ntional L(\u00b5), we provide the label noise procedure, where\nwe add independent label noise to the training data for im-\nplicit regularization. In the discretized MFLD update, we\nadd independent label noise \u02dc\u03b5i \u223c \u03c5([\u2212\u02dc\u03c3, \u02dc\u03c3]n) to Yi at each\ntime step. We use the noisy label \u02dcYi := Yi + \u02dc\u03b5i to train\nthe second layer and obtain \u02dca(i)\n                         \u00b5 := argmin\n                                       1\n                                      nT\n                                         \ufffdT\n                                            i=1 \u2225 \u02dcYi\u2212\nf(X; a, \u00b5)\u22252\n          2+\n              \u00af\u03bba\n              2 E\u00b5[a(w)2]. Here, the noisy limiting func-\ntional G\u02dc\u03b5(\u00b5) is defined as\n\n$$G_{\\tilde{\\varepsilon}}(\\mu):=\\frac{1}{nT}\\sum_{i=1}^{T}\\|Y_{i}-f(X;\\tilde{a}_{\\mu},\\mu)\\|_{2}^{2}$$ $$+\\frac{\\bar{\\lambda}_{a}}{2}\\mathbb{E}_{\\mu}\\Big{[}\\|\\tilde{a}_{\\mu}(w)\\|_{2}^{2}\\Big{]}+\\frac{\\bar{\\lambda}_{w}}{2}\\mathbb{E}_{\\mu}[\\|w\\|_{2}^{2}].$$\nNote that we use the clean label Yi to define G\u02dc\u03b5(\u00b5) instead of \u02dcYi. Then, we update the first layer by the following discretized MFLD.\n\n$$w_{i}^{(k+1)}=w_{i}^{(k)}-\\eta\\mathbf{\\nabla}\\frac{\\delta G_{\\bar{\\varepsilon}^{(k)}}(\\mu w_{k})}{\\delta\\mu}(w_{i}^{(k)})+\\sqrt{2\\eta\\lambda}\\xi_{i}^{(k)},$$\n\nwhere $\\bar{\\varepsilon}^{(k)}$ is an independent noise at the $k$-th iteration. In fact, the expectation of $G_{\\bar{\\varepsilon}}(\\mu)+\\lambda\\operatorname{Ent}(\\mu)$ with respect to $\\bar{\\varepsilon}$ is equal to $\\mathcal{L}(\\mu)$ and the above procedure can be seen as the stochastic MFLD for minimizing $\\mathcal{L}(\\mu)$. Indeed, the following theorem holds.\n\nTheorem\n        5.5.\n            Let\n                \u00b5\u2217\n                    =\n                        argmin\n                          \u00b5\n                              L(\u00b5).\n                                     Then,\n\n_for $\\eta<\\min(1/4,1/(4\\alpha\\lambda))$ and $0\\leq\\tilde{\\sigma}^{2}/3\\leq\\lambda_{\\min}\\Big{(}\\frac{1}{T}\\sum_{i=1}^{T}Y_{i}Y_{i}^{\\top}\\Big{)}$, we have_\n\n$$\\frac{1}{N}\\mathbb{E}[\\mathcal{L}^{N}(\\mu_{k}^{N})]-\\mathcal{L}(\\mu^{*})$$ $$\\leq\\exp(-\\alpha\\lambda\\eta k/2)(\\mathbb{E}[\\mathcal{L}^{N}(\\mu_{0}^{N})]-\\mathcal{L}(\\mu^{*}))+\\tilde{\\delta}^{\\prime}_{\\eta,N},$$\nwhere \u00af\u03b4\u2032\n\u03b7,N = O(\u03b7 + 1\nN ). Here, the expectation is taken with respect to the randomness of the label noise.\n\nSee Appendix D.3 for the proof. Intuitively, the degrees of\nfreedom represents a metric for quantifying the adaptability\nto noise and the first layer performs the robust feature learn-\ning where the second layer is difficult to fit the label noise.\nSuzuki & Suzuki (2023) has shown the Bayes optimality\nof two-layer linear neural networks which minimizes the\nempirical risk with the degrees of freedom regularization.\nHowever, they ignore the optimization aspect and directly\nassume that the optimal solution can be obtained. Note that\nthe condition on \u02dc\u03c32 is needed to ensure the convexity of the\nobjective and the multi-learning setting is necessary to set\n\u02dc\u03c3 > 0 since 1\n\n          T\n            \ufffdT\n              i=1 YiYi\n                     \u22a4 must be full rank. However, as\nshown in Section 6, the label noise procedure is effective\neven for the single output setting.\n"
    },
    {
        "level": "##",
        "title": "6. Numerical Experiments",
        "content": "\nTo validate our theoretical results, we conduct numerical\nexperiments with synthetic data.\n                                   Specifically, we con-\nsider f \u25e6(x) = x1x2 for d = 15.\n                                      Then, the samples\n\ufffd\n (x(i), y(i))\n           \ufffdn\n             i=1 are independently generated so that x(i)\n\nfollows N(0, I) and y(i) = f \u25e6(x(i)) + \u03b5(i), where \u03b5(i) \u223c\n\u03c5([\u03c3, \u03c3]). We consider a finite width neural network with the\nwidth m = 2000. We trained the network via noisy gradient\ndescent with \u03b7 = 0.2, \u03bb = 0.004, \u03bbw = 0.25, \u03bba = 0.25\nuntil T = 10000. The results are averaged over 5 different\nrandom seeds.\n\nFirst, we investigated the training dynamics of the kernel by\nchanging the intrinsic noise \u03c3. As shown in Figure 1, kernel\nmoves to increase the kernel alignment and the degrees\n\nof freedom. In addition, the intrinsic noise increases the degrees of freedom, which is consistent with our arguments in Section 5.2.\n\nNext, we demonstrated the effectiveness of the label noise\nprocedure. Fig. 2 shows the evolution of the degrees of\nfreedom and the test loss during the training for different \u02dc\u03c3.\nAs expected, the label noise procedure reduces the degrees\nof freedom. Moreover, the test loss is also improved, which\nimplies that the degrees of freedom is a good regularization\nfor the generalization error.\n"
    },
    {
        "level": "##",
        "title": "7. Conclusion",
        "content": "\nIn this paper, we studied the feature learning ability of two-\nlayer neural networks in the mean-field regime via kernel\nlearning formulation. For that purpose, we proposed to\nuse the two-timescale limit to analyze the training dynam-\nics of the mean-field neural networks. Then, we provided\nthe linear convergence guarantee to the global optimum by\nshowing the convexity of the limiting functional and derive\nthe discretization error. We also studied the generalization\nability of the empirical risk minimizer and proved that the\nfeature learning is essential to obtain good generalization\nresults for a union of multiple RKHSs. Then, we showed\nthat the kernel induced by the first layer moves to increase\nkernel and parameter alignment and the intrinsic noise in\nlabels induces bias towards the large degrees of freedom.\nFinally, we proposed the label noise procedure to reduce the\ndegrees of freedom and provided the global convergence\nguarantee.\n\nST was partially supported by JST CREST (JPMJCR2015).\n\nTS was partially supported by JSPS KAKENHI (20H00576)\nand JST CREST (JPMJCR2115).\n"
    },
    {
        "level": "##",
        "title": "References",
        "content": "\nArora, S., Du, S., Hu, W., Li, Z., and Wang, R. Fine-\nGrained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks. In\nProceedings of the 36th International Conference on\nMachine Learning, pp. 322\u2013332. PMLR, May 2019a.\nURL https://proceedings.mlr.press/v97/ arora19a.html. ISSN: 2640-3498.\nArora, S., Du, S. S., Hu, W., Li, Z., Salakhutdinov, R. R.,\nand Wang, R. On Exact Computation with an Infinitely\nWide Neural Net. In Advances in Neural Information\nProcessing Systems, volume 32. Curran Associates,\nInc., 2019b.\nURL https://papers.neurips.\ncc/paper_files/paper/2019/hash/ dbc4d84bfcfe2284ba11beffb853a8c4-Abstract.\nhtml.\nAtanasov, A., Bordelon, B., and Pehlevan, C. Neural Networks as Kernel Learners: The Silent Alignment Effect.\nIn *International Conference on Learning Representations*, October 2021. URL https://openreview.net/ forum?id=1NvflqAdoom.\nBach, F. On the Equivalence between Kernel Quadrature\nRules and Random Feature Expansions. Journal of Machine Learning Research, 18(21):1\u201338, 2017. ISSN 1533-\n7928.\nURL http://jmlr.org/papers/v18/\n15-178.html.\nBach, F. R., Lanckriet, G. R. G., and Jordan, M. I. Multiple kernel learning, conic duality, and the SMO algorithm.\nIn Twenty-first international conference on\nMachine learning - ICML '04, pp.\n6, Banff, Alberta, Canada, 2004. ACM Press.\ndoi:\n10.1145/\n1015330.1015424.\nURL http://portal.acm.\norg/citation.cfm?doid=1015330.1015424.\nBakry, D. and \u00b4Emery, M. Diffusions hypercontractives.\nS\u00b4eminaire de probabilit\u00b4*es de Strasbourg*, 19:177\u2013206,\n1985.\nURL https://eudml.org/doc/113511.\nPublisher: Springer - Lecture Notes in Mathematics.\nBaratin, A., George, T., Laurent, C., Hjelm, R. D., Lajoie,\nG., Vincent, P., and Lacoste-Julien, S. Implicit Regularization via Neural Feature Alignment. In Proceedings\nof The 24th International Conference on Artificial Intelligence and Statistics, pp. 2269\u20132277. PMLR, March 2021. URL https://proceedings.mlr.press/\nv130/baratin21a.html. ISSN: 2640-3498.\nBarron, A.\nUniversal approximation bounds for superpositions of a sigmoidal function.\nIEEE Transactions on Information Theory, 39(3):930\u2013945, May\n1993. ISSN 0018-9448, 1557-9654. doi: 10.1109/18.\n256500. URL https://ieeexplore.ieee.org/\ndocument/256500/.\nBartlett, P. L. and Mendelson, S. Rademacher and Gaussian Complexities: Risk Bounds and Structural Results. In Goos, G., Hartmanis, J., Van Leeuwen, J., Helmbold, D., and Williamson, B. (eds.), Computational\nLearning Theory, volume 2111, pp. 224\u2013240. Springer\nBerlin Heidelberg, Berlin, Heidelberg, 2001.\nISBN\n978-3-540-42343-0 978-3-540-44581-4. doi: 10.1007/ 3-540-44581-1 15. URL http://link.springer.\ncom/10.1007/3-540-44581-1_15. Series Title: Lecture Notes in Computer Science.\nBietti, A., Bruna, J., Sanford, C., and Song, M. J. Learning\nsingle-index models with shallow neural networks. In\nAdvances in Neural Information Processing Systems, May\n2022. URL https://openreview.net/forum?\nid=wt7cd9m2cz2.\nBietti, A., Bruna, J., and Pillaud-Vivien, L.\nOn Learning Gaussian Multi-index Models with Gradient Flow, November 2023. URL http://arxiv.org/abs/\n2310.19793. arXiv:2310.19793 [cs, math, stat].\nCaponnetto, A. and De Vito, E.\nOptimal Rates for\nthe Regularized Least-Squares Algorithm.\nFoundations\nof\nComputational\nMathematics,\n7(3):\n331\u2013368,\nJuly 2007.\nISSN 1615-3375,\n1615-\n3383.\ndoi:\n10.1007/s10208-006-0196-8.\nURL\nhttp://link.springer.com/10.1007/\ns10208-006-0196-8.\nChen, F., Ren, Z., and Wang, S. Uniform-in-time propagation of chaos for mean field Langevin dynamics, November 2023.\nURL http://arxiv.org/abs/2212.\n03050. arXiv:2212.03050 [math, stat].\nChen, Z., Cao, Y., Gu, Q., and Zhang, T. A Generalized\nNeural Tangent Kernel Analysis for Two-layer Neural Networks. In Advances in Neural Information Processing\nSystems, volume 33, pp. 13363\u201313373. Curran Associates, Inc., 2020.\nURL https://proceedings.\nneurips.cc/paper/2020/hash/ 9afe487de556e59e6db6c862adfe25a4-Abstract.\nhtml.\nChizat, L. Mean-Field Langevin Dynamics : Exponential Convergence and Annealing. Transactions on Machine Learning Research, May 2022.\nISSN 2835-\n8856. URL https://openreview.net/forum?\nid=BDqzLH1gEm.\nChizat, L. and Bach, F.\nOn the global convergence\nof gradient descent for over-parameterized models using optimal transport.\nAdvances in neural information processing systems, 31, 2018.\nURL https:\n//proceedings-neurips-cc.utokyo.idm.\noclc.org/paper_files/paper/2018/hash/ a1afc58c6ca9540d057299ec3016d726-Abstract.\nhtml.\nCristianini, N., Shawe-Taylor, J., Elisseeff, A., and Kandola,\nJ. On Kernel-Target Alignment. In Advances in Neural\nInformation Processing Systems, volume 14. MIT Press,\n2001.\nURL https://proceedings.neurips.\ncc/paper_files/paper/2001/hash/\n1f71e393b3809197ed66df836fe833e5-Abstract.\nhtml.\nDamian, A., Ma, T., and Lee, J. D. Label noise sgd provably\nprefers flat global minimizers.\nAdvances in Neural\nInformation Processing Systems, 34:27449\u201327461, 2021.\nURL\nhttps://proceedings-neurips-cc.\nutokyo.idm.oclc.org/paper/2021/hash/ e6af401c28c1790eaef7d55c92ab6ab6-Abstract.\nhtml.\nDamian, A., Lee, J., and Soltanolkotabi, M.\nNeural Networks can Learn Representations with Gradient Descent.\nIn Proceedings of Thirty Fifth Conference on Learning Theory, pp. 5413\u20135452. PMLR, June 2022. URL https://proceedings.mlr.press/\nv178/damian22a.html. ISSN: 2640-3498.\nE, W., Ma, C., and Wu, L.\nA priori estimates\nof the population risk for two-layer neural networks.\nCommunications\nin\nMathematical\nSciences, 17(5):1407\u20131425, 2019.\nISSN 15396746,\n19450796.\ndoi:\n10.4310/CMS.2019.v17.n5.a11.\nURL\nhttps://www.intlpress.com/site/\npub/pages/journals/items/cms/content/\nvols/0017/0005/a011/.\nFang, C., Dong, H., and Zhang, T. Over Parameterized Twolevel Neural Networks Can Learn Near Optimal Feature Representations, October 2019. URL http://arxiv. org/abs/1910.11508. arXiv:1910.11508 [cs, math, stat].\nHayakawa, S. and Suzuki, T. On the minimax optimality and\nsuperiority of deep neural network learning over sparse parameter spaces. *Neural Networks*, 123:343\u2013361, March 2020. ISSN 0893-6080. doi: 10.1016/j.neunet.2019.12. 014. URL https://www.sciencedirect.com/ science/article/pii/S089360801930406X.\nHolley, R. and Stroock, D. Logarithmic Sobolev inequalities and stochastic Ising models.\nJournal of Statistical Physics, 46(5):1159\u20131194, March 1987.\nISSN\n1572-9613. doi: 10.1007/BF01011161. URL https:\n//doi.org/10.1007/BF01011161.\n\nHsu, D. Dimension lower bounds for linear approaches to\nfunction approximation. *Daniel Hsu's homepage*, 2021.\nHu, K., Ren, Z., Siska, D., and Szpruch, L. Mean-Field\nLangevin Dynamics and Energy Landscape of Neural Networks, December 2020. URL http://arxiv.org/\nabs/1905.07769. arXiv:1905.07769 [math, stat].\nJacot, A., Gabriel, F., and Hongler, C. Neural Tangent\nKernel:\nConvergence and Generalization in Neural\nNetworks.\nIn Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.\nURL https://proceedings.neurips.\ncc/paper_files/paper/2018/hash/\n5a4be1fa34e62bb8a6ec6b91d2462f5a-Abstract.\nhtml.\nLi, Z., Wang, T., and Arora, S. What Happens after SGD\nReaches Zero Loss? \u2013A Mathematical Framework. In\nInternational Conference on Learning Representations,\nOctober 2021. URL https://openreview.net/ forum?id=siCt4xZn5Ve.\nMa, C. and Wu, L.\nThe Barron space and the flowinduced function spaces for neural network models.\nConstructive Approximation, 55(1):369\u2013406, 2022. URL\nhttps://link.springer.com/article/10.\n1007/s00365-021-09549-y. Publisher: Springer.\nMahankali, A. V., HaoChen, J. Z., Dong, K., Glasgow, M.,\nand Ma, T. Beyond NTK with Vanilla Gradient Descent:\nA Mean-Field Analysis of Neural Networks with Polynomial Width, Samples, and Time. In Thirty-seventh\nConference on Neural Information Processing Systems,\n2023. URL https://openreview-net.utokyo. idm.oclc.org/forum?id=Y2hnMZvVDm.\nMarion, P. and Berthier, R. Leveraging the two timescale\nregime to demonstrate convergence of neural networks, October 2023.\nURL http://arxiv.org/abs/\n2304.09576. arXiv:2304.09576 [cs, math, stat].\nMaurer, A. A Vector-Contraction Inequality for Rademacher\nComplexities. In Ortner, R., Simon, H. U., and Zilles, S. (eds.), *Algorithmic Learning Theory*, Lecture Notes in Computer Science, pp. 3\u201317, Cham, 2016. Springer International Publishing. ISBN 978-3-319-46379-7. doi:\n10.1007/978-3-319-46379-7 1.\nMei, S., Montanari, A., and Nguyen, P.-M.\nA mean\nfield view of the landscape of two-layer neural networks. *Proceedings of the National Academy of Sciences*,\n115(33), August 2018.\nISSN 0027-8424, 1091-6490.\ndoi: 10.1073/pnas.1806579115. URL https://pnas.\norg/doi/full/10.1073/pnas.1806579115.\nMousavi-Hosseini, A., Park, S., Girotti, M., Mitliagkas,\nI., and Erdogdu, M. A.\nNeural Networks Efficiently\nLearn\nLow-Dimensional\nRepresentations\nwith SGD.\nIn The Eleventh International Conference on Learning Representations,\n2022.\nURL\nhttps://openreview-net.utokyo.idm.\noclc.org/forum?id=6taykzqcPD.\nNitanda,\nA. and Suzuki,\nT.\nStochastic Particle\nGradient Descent for Infinite Ensembles,\nDecember 2017.\nURL http://arxiv.org/abs/1712.\n05438. arXiv:1712.05438 [cs, math, stat].\nNitanda, A. and Suzuki, T.\nOptimal Rates for Averaged Stochastic Gradient Descent under Neural\nTangent Kernel Regime.\nIn International Conference on Learning Representations,\n2020.\nURL\nhttps://openreview-net.utokyo.idm.\noclc.org/forum?id=PULSD5qI2N1.\nNitanda, A., Wu, D., and Suzuki, T. Convex Analysis of\nthe Mean Field Langevin Dynamics. In Proceedings\nof The 25th International Conference on Artificial Intelligence and Statistics, pp. 9741\u20139757. PMLR, May 2022. URL https://proceedings.mlr.press/\nv151/nitanda22a.html. ISSN: 2640-3498.\nSuzuki, K. and Suzuki, T. Optimal criterion for feature\nlearning of two-layer linear neural network in high dimensional interpolation regime. In The Twelfth International Conference on Learning Representations, October 2023. URL https://openreview.net/forum? id=Jc0FssXh2R.\nSuzuki, T. Fast generalization error bound of deep learning from a kernel perspective. In Proceedings of the\nTwenty-First International Conference on Artificial Intelligence and Statistics, pp. 1397\u20131406. PMLR, March 2018. URL https://proceedings.mlr.press/\nv84/suzuki18a.html. ISSN: 2640-3498.\nSuzuki, T., Abe, H., Murata, T., Horiuchi, S., Ito, K., Wachi,\nT., Hirai, S., Yukishima, M., and Nishimura, T. Spectral Pruning: Compressing Deep Neural Networks via Spectral Analysis and its Generalization Error. In Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, pp. 2839\u20132846, Yokohama,\nJapan, July 2020. International Joint Conferences on Artificial Intelligence Organization. ISBN 978-0-9992411-6- 5. doi: 10.24963/ijcai.2020/393. URL https://www. ijcai.org/proceedings/2020/393.\nSuzuki, T., Nitanda, A., and Wu, D.\nUniform-intime propagation of chaos for the mean-field gradient Langevin dynamics.\nIn The Eleventh International\nConference on Learning Representations, September\n2022. URL https://openreview.net/forum?\n\nid=_JScUk9TBUn.\n\nSuzuki, T., Wu, D., and Nitanda, A.\nConvergence of\nmean-field Langevin dynamics: time-space discretization, stochastic gradient, and variance reduction.\nIn\nThirty-seventh Conference on Neural Information Processing Systems, November 2023a. URL https:// openreview.net/forum?id=9STYRIVx6u.\nSuzuki, T., Wu, D., Oko, K., and Nitanda, A. Feature\nlearning via mean-field Langevin dynamics: classifying sparse parities and beyond. In Thirty-seventh Conference on Neural Information Processing Systems, November 2023b. URL https://https://openreview. net/forum?id=tj86aGVNb3.\nVivien,\nL.\nP.,\nReygner,\nJ.,\nand\nFlammarion,\nN.\nLabel\nnoise\n(stochastic)\ngradient\ndescent\nimplicitly\nsolves\nthe\nLasso\nfor\nquadratic\nparametrisation.\nIn Proceedings of Thirty Fifth Conference\non Learning Theory, pp. 2127\u20132159. PMLR, June 2022. URL https://proceedings.mlr.press/\nv178/vivien22a.html. ISSN: 2640-3498.\nWainwright, M. J.\nHigh-dimensional statistics: A nonasymptotic viewpoint, volume 48. Cambridge university press, 2019.\nYehudai, G. and Shamir, O. On the Power and Limitations of Random Features for Understanding Neural Networks.\nIn Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.\nURL https://proceedings.neurips.\ncc/paper_files/paper/2019/hash/ 5481b2f34a74e427a2818014b8e103b0-Abstract.\nhtml.\n"
    },
    {
        "level": "##",
        "title": "A. Auxiliary Lemmas",
        "content": "\nLemma A.1 (Holley & Stroock (1987)). Assume that a probability distribution p(w) satisfies the LSI with a constant \u03b1 > 0.\n\nFor a bounded perturbation B(w) : Rd\u2032 \u2192 R*, define* p\u2032(w) = p(w) exp(B(w))/Ep[exp(B(w))]. Then, p\u2032 satisfies the LSI\nwith a constant\n\u03b1\nexp(4\u2225B\u2225\u221e).\n\nLemma A.2. The optimal i*-th second layer* a(i)\n\u00b5 satisfies\n\n$$a_{\\mu}^{(i)}(w)=-\\frac{1}{\\bar{\\lambda}_{a}}\\mathbb{E}_{\\rho}[\\partial_{1}l_{i}(f_{i}(x;a_{\\mu},\\mu),y_{i})h(x;w)].$$\nfor any w \u2208 Rd\u2032.\n\nProof. From the optimality condition on a(i)\n\u00b5 , it holds that\n\n$$\\frac{\\partial L}{\\partial a_{\\mu}^{(i)}}(a^{(i)},\\mu)(w)+\\lambda\\partial_{u},r(a_{\\mu}(w),w)\\mu(w)=\\mathbb{E}[\\partial_{t}l_{t}(f_{i}(x;a_{\\mu}^{(i)},\\mu),y_{i})h(x;w)]\\mu(w)+\\bar{\\lambda}_{u}a_{\\mu}^{(i)}(w)\\mu(w)=0.$$\nThus, we have\n\n$$a_{\\mu}^{(i)}(w)=-\\frac{1}{\\bar{\\lambda}_{a}}\\mathbb{E}_{\\rho}[\\partial_{1}l_{i}(f_{i}(x;a_{\\mu},\\mu),y)h(x;w)],$$\nwhich completes the proof.\n\nLemma A.3. Define T : L2(\u00b5) \u2192 L2(\u03c1X) by\n\n$$T(a)=\\int a(w)h(x;w)\\mathrm{d}\\mu(w)$$\nand its adjoint operator T \u2217 : L2(\u03c1X) \u2192 L2(\u00b5) by\n\n$$T^{*}(f)=\\int f(x)h(x;w)\\mathrm{d}\\rho(x).$$\nFor l2-loss, the optimal i*-th second layer* a(i)\n\u00b5 has the following explicit formula.\n\n$$a_{\\mu}^{(i)}(w)=(T^{i}T+\\bar{\\lambda}_{\\mu}\\mathbf{I}_{\\mu})^{-1}T^{i}f_{i}^{2}$$ $$=T^{*}(T^{*}+\\bar{\\lambda}_{\\mu}\\mathbf{I}_{\\mu})^{-1}f_{i}^{2},$$\n\n_where $f_{i}^{2}(x^{\\prime}):=\\mathbb{E}_{\\mu}[y_{i}]\\mid x=x^{\\prime}]$ is the conditional expectation of $y$, given $x$. In addition, if $p_{X}$ is the empirical distribution, $\\frac{1}{n}\\sum_{j=1}^{n}\\delta_{x_{j}}$, then, $a_{\\mu}^{(i)}$ is written by_\n\n$$a_{\\mu}^{(i)}(w)=h(X;w)^{\\top}(\\bar{\\lambda}_{\\mu}+n\\bar{\\lambda}_{\\mu}T)^{-1}Y_{i}.$$\nProof. For l2-loss, the optimality condition on a is given by\n\n$$\\mathbb{E}_{\\rho}[(T(a_{\\mu}^{(i)})-y)h(x;w)]+\\bar{\\lambda}_{a}a_{\\mu}^{(i)}(w)=0.$$\nUsing E\u03c1[yih(x; w)] = E\u03c1X[f \u25e6\ni (x)h(x; w)] = T \u2217f \u25e6\ni , we have\n\n$$T^{*}T(a_{\\mu}^{(i)})+\\bar{\\lambda}_{a}a_{\\mu}^{(i)}=T^{*}f_{i}^{\\circ}.$$\nSince \u00af\u03bba > 0 and (T \u2217T + \u00af\u03bba Id) is invertible, we arrive at\n\n$$a_{\\mu}^{(i)}=(T^{*}T+\\bar{\\lambda}_{a}\\,\\mathrm{Id})^{-1}T^{*}f_{i}^{\\circ}.$$\nSince (T \u2217T + \u00af\u03bba Id)T \u2217 = T \u2217(TT \u2217 + \u00af\u03bba Id), we have T \u2217(TT \u2217 + \u00af\u03bba Id)\u22121 = (T \u2217T + \u00af\u03bba Id)\u22121T \u2217, and thus a(i)\n\u00b5\n=\nT \u2217(TT \u2217 + \u00af\u03bba Id)\u22121f \u25e6\ni .\n\nProof. Let \u00afl\u2032\u2032\ni (x\u2032) := E\u03c1[\u22022\n1li(f(x), yi) | x = x\u2032]. Define \u039b, \u039b1/2 : L2(\u03c1) \u2192 L2(\u03c1) by\n\n$$\\begin{array}{c}{{\\Lambda_{i}(f)(x)=f(x)\\overline{{{l}}}_{i}^{\\prime\\prime}(x),}}\\\\ {{{\\Lambda_{i}^{1/2}}(f)(x)=f(x)\\overline{{{l}}}_{i}^{\\prime\\prime}(x)^{1/2},}}\\end{array}$$\nand A(i)(w) \u2208 L2(\u03c1) by\n\n$$A^{(i)}(w)(x)=a_{\\mu}^{(i)}(w)h(x;w)\\bar{l}^{\\prime\\prime}(x)^{1/2}$$\nfor a given w \u2208 Rd\u2032. Note that \u039b, \u039b1/2.A are well-defined since \u00afl\u2032\u2032\ni (x) \u2265 0 from the convexity of li with respect to the first argument.\n\nThe second variation of U(\u00b5) is given by\n\n$$\\frac{\\delta}{\\delta\\mu}\\frac{\\delta U(\\mu)}{\\delta\\mu}(w,w^{\\prime})=-\\frac{\\bar{\\lambda}_{a}}{T}\\sum_{i=1}^{T}a_{\\mu}^{(i)}(w)\\frac{\\delta a_{\\mu}^{(i)}(w)}{\\delta\\mu}(w^{\\prime}).$$\nTaking the first variation of the both sides of the optimality condition on a(i)\n\u00b5 (w) for a given w, we have\n\n$$\\mathbb{E}_{\\mu}[l_{i}^{\\prime\\prime}(f(x;a_{\\mu},\\mu),y)\\Bigg{(}a_{\\mu}^{(i)}(w^{\\prime})h(x;w^{\\prime})+\\int\\frac{\\delta a_{0}^{(i)}(w^{\\prime\\prime})}{\\delta\\mu}(w^{\\prime})h(x;w^{\\prime\\prime})\\mathrm{d}\\mu(w^{\\prime\\prime})\\Bigg{)}h(x;w)]+\\tilde{\\lambda}_{a}\\frac{\\delta a_{0}^{(i)}(w)}{\\delta\\mu}(w^{\\prime})$$ $$=\\Bigg{[}(T^{*}\\Lambda_{i}T+\\tilde{\\lambda}_{a}\\,\\mathrm{Id})\\frac{\\delta a_{0}^{(i)}(\\cdot)}{\\delta\\mu}(w^{\\prime})\\Bigg{]}(w)+\\Big{[}T^{*}\\Lambda_{i}^{1/2}A^{(i)}(w^{\\prime})\\Big{]}(w)$$ $$=0.$$\nThus, we obtain\n\n\u03b4 \u03b4\u00b5 \u03b4U(\u00b5) \u03b4\u00b5 = \u2212 \u00af\u03bba T i=1 a(i) \u00b5 (w)\u03b4a(i) \u00b5 (w) \u03b4\u00b5 (w\u2032) T \ufffd = \u2212 \u00af\u03bba T i=1 a(i) \u00b5 (w)(T \u2217\u039biT + \u00af\u03bba Id)\u22121\ufffd T \u2217\u039b1/2 i A(i)(w\u2032) \ufffd (w) T \ufffd = \u2212 \u00af\u03bba T i=1 a(i) \u00b5 (w) \ufffd T \u2217\u039b1/2 i (\u039b1/2TT \u2217\u039b1/2 i + \u00af\u03bba Id)\u22121A(i)(w\u2032) \ufffd (w) T \ufffd = \u2212 \u00af\u03bba T i=1 a(i) \u00b5 (w) \ufffd \ufffd (\u039b1/2 i TT \u2217\u039b1/2 i + \u00af\u03bba Id)\u22121A(i)(w\u2032) \ufffd (x)h(x; w)\u00afl\u2032\u2032(x)1/2d\u03c1(x) T \ufffd = \u2212 \u00af\u03bba T i=1 \ufffd \ufffd (\u039b1/2 i TT \u2217\u039b1/2 i + \u00af\u03bba Id)\u22121A(w\u2032) \ufffd (x)A(w)(x)d\u03c1(x) T \ufffd = \u2212 \u00af\u03bba T i=1 \u27e8A(i)(w), (\u039b1/2 i TT \u2217\u039b1/2 i + \u00af\u03bba Id)\u22121A(i)(w\u2032)\u27e9 T \ufffd\nThe second equality follows from the equality A(A\u2217A + Id)\u22121 = (AA\u2217 + Id)\u22121A for any operator A such that (A\u2217A +\nId), (AA\u2217 + Id) are invertible.\n\nIn addition, the first variations of L w.r.t. \u00b5 and a are given by\n\n$$\\partial L(a_{\\mu},\\mu)=\\frac{1}{T}\\sum_{i=1}^{T}\\mathbb{E}_{\\rho}[\\partial_{1}l_{i}(f_{i}(x;a,\\mu),y)h(x;w)]a_{\\mu}^{(i)}(w),$$ $$\\partial L(a_{\\mu},\\mu)=\\frac{1}{T}\\mathbb{E}_{\\rho}[\\partial_{1}l_{i}(f_{i}(x;a,\\mu),y)h(x;w)]\\mu(w),$$\nrespectively. Therefore, we have\n\n$$\\partial L(\\frac{a_{\\mu},\\mu)}{\\partial\\mu}(w)=\\sum_{i=1}^{T}\\frac{a_{\\mu}^{(i)}(w)}{\\mu(w)}\\frac{\\partial L(a_{\\mu},\\mu)}{\\partial a^{(i)}}(w).$$\nThe first-order optimality condition on a\u00b5 yields\n\n$$\\frac{\\partial L(a_{\\mu},\\mu)}{\\partial a^{(i)}}(w)=-\\lambda\\frac{\\partial r(a_{\\mu}(w),w)}{\\partial a^{(i)}}\\mu(w),\\tag{4}$$\nwhich implies\n\n$$\\partial L(a_{\\mu},\\mu)\\over\\partial\\mu}(w)=-\\lambda\\sum_{i=1}^{T}\\partial r(a_{\\mu}(w),w)\\over\\partial a^{(i)}a_{\\mu}^{(i)}(w)$$ $$=-\\lambda\\langle\\mathbf{\\nabla}_{a}r(a_{\\mu}(w),w),a_{\\mu}(w)\\rangle.$$\nCombining above arguments, we arrive at\n\n$$\\delta G(\\mu)\\over\\delta\\mu}(w)=-\\lambda\\langle\\mathbf{\\nabla}_{a}r(a_{\\mu}(w),w),a_{\\mu}(w)\\rangle+\\lambda r(a_{\\mu}(w),w)\\tag{5}$$ $$=\\lambda\\biggl{(}-{\\lambda_{a}\\over2T}\\|a_{\\mu}(w)\\|_{2}^{2}+{\\lambda_{w}\\over2}\\|w\\|_{2}^{2}\\biggr{)}.$$\nNext, we prove the convexity of G(\u00b5). From the convexity of L(*a, \u00b5*) w.r.t. a, we have\n\n$$L(a_{\\mu_{1}},\\mu_{1})+\\int\\sum_{i=1}^{T}\\frac{\\partial L(a_{\\mu_{1}},\\mu_{1})}{\\partial a^{(i)}}(w)\\bigg{(}\\frac{\\mu_{2}(w)}{\\mu_{1}(w)}a_{\\mu_{2}}^{(i)}(w)-a_{\\mu_{1}}^{(i)}(w)\\bigg{)}\\mathrm{d}w\\leq L\\bigg{(}\\frac{\\mu_{2}(w)}{\\mu_{1}(w)}a_{\\mu_{2}},\\mu_{1}\\bigg{)}=L(a_{\\mu_{2}},\\mu_{2}),$$\n\nfor any $\\mu_{1},\\mu_{2}\\in\\mathcal{P}$. Therefore, it holds that\n\n$$G(\\mu_{1})+\\int\\sum_{i=1}^{T}\\frac{\\partial L(a_{\\mu_{1}},\\mu_{1})}{\\partial a^{(i)}}(w)\\bigg{(}\\frac{\\mu_{2}(w)}{\\mu_{1}(w)}a_{\\mu_{2}}^{(i)}(w)-a_{\\mu_{1}}^{(i)}(w)\\bigg{)}\\mathrm{d}w$$ $$+\\lambda r(a_{\\mu_{1}}(w),w)\\mu_{2}(w)-\\lambda r(a_{\\mu_{1}}(w),w)\\mu_{1}(w)\\mathrm{d}w$$ $$\\leq G(\\mu_{2}).$$\nThus, it is sufficient to show that\n\n$$\\delta\\frac{G(\\mu_{1})}{\\delta\\mu}(w)(\\mu_{2}(w)-\\mu_{1}(w))\\leq\\sum_{i=1}^{T}\\,\\partial L\\frac{(a_{\\mu_{1}},\\mu_{1})}{\\partial a^{(i)}}(w)\\bigg{(}\\frac{\\mu_{2}(w)}{\\mu_{1}(w)}a_{\\mu}^{(i)}(w)-a_{\\mu_{1}}^{(i)}(w)\\bigg{)}\\mathrm{d}w$$ $$+\\lambda r(a_{\\mu_{2}}(w),w)\\mu_{2}(w)-\\lambda r(a_{\\mu_{1}}(w),w)\\mu_{1}(w)$$\nfor any w. To simplify the notation, we denote the LHS by \u03c11(w) and the RHS by \u03c12(w). Substituting Eq. (5) to \u03c11(w), we have\n\n$\\rho_{1}(w)=\\lambda[-\\langle\\mathbf{\\nabla}_{a}r(a_{\\mu_{1}}(w),w),a_{\\mu_{1}}(w)\\rangle+r(a_{\\mu_{1}}(w),w)]\\left(\\mu_{2}(w)-\\mu_{1}(w)\\right)$.\n\nwhich yields\n\n$$\\|(\\Sigma_{\\mu_{W}}-\\Sigma_{\\mu^{*}})(f)\\|_{L^{2}(\\rho_{X})}^{2}\\leq\\|f\\|_{L^{2}(\\rho_{X})}^{2}\\int(k_{\\mu_{W}}(x,x^{\\prime})-k_{\\mu^{*}}(x,x^{\\prime}))^{2}\\mathrm{d}\\rho_{X}(x^{\\prime})\\mathrm{d}\\rho_{X}(x).$$\nThis implies \u2225\u03a3\u00b5W \u2212 \u03a3\u00b5\u2217\u22252\nop \u2264 Ex,x\u2032[(k\u00b5W (x, x\u2032) \u2212 k\u00b5\u2217(*x, x*\u2032))2] \u2264 \u2206/N.\n\nSince fi(x; \u00b5) is the optimal sofor any invertible operator *A, A*\u2032, we have\n\nlution of $\\min_{f\\in I_{N}}\\mathbb{E}_{\\mu}[l_{i}(f(x);y)]+\\frac{\\lambda}{2\\mu}\\|f\\|_{\\mu_{i}}$, where $l_{i}$ is the squared loss, $f_{i}(x;\\mu)=\\Sigma_{\\mu}(\\Sigma_{\\mu}+\\lambda_{\\mu}\\operatorname{Id})^{-1}\\tilde{y}=\\int k(x,x^{\\prime})\\alpha(x^{\\prime})d\\rho_{\\mu}(x^{\\prime})$, where $\\alpha_{\\mu}(x)=(\\Sigma_{\\mu}+\\tilde{\\lambda}_{\\mu}\\operatorname{Id})^{-1}\\tilde{y}$. From the identity $A^{-1}-A^{\\prime-1}=-A^{-1}(A-A^{\\prime})A^{\\prime-1}$ for any invertible operator $A,A^{\\prime}$, we have\n\n$$\\|\\alpha_{\\mu\\nu}-\\alpha_{\\mu}{}^{2}\\|_{\\nu_{\\mu}}^{2}=\\|(\\Sigma_{\\mu\\nu}+\\tilde{\\lambda}_{\\mu}\\operatorname{Id})^{-1}(\\Sigma_{\\mu\\nu}-\\Sigma_{\\mu^{\\prime}})(\\Sigma_{\\mu^{\\prime}}+\\tilde{\\lambda}_{\\mu}\\operatorname{Id})^{-1}\\tilde{y}\\|_{L^{1}(\\rho,\\kappa)}^{2}$$ $$\\leq\\frac{\\sigma_{\\mu}^{2}\\|\\Sigma_{\\mu\\nu}-\\Sigma_{\\mu}{}^{2}\\|_{\\rho_{\\mu}}^{2}}{\\tilde{\\lambda}_{\\mu}^{4}}.$$\nThus, for any x \u2208 S, we have\n\n$$(f_{t}(x;\\mu_{W})-f_{t}(x;\\mu^{*}))^{2}\\leq2\\bigg{(}\\int k_{\\mu^{*}_{W}}(x,x^{\\prime})\\alpha_{\\mu_{W}}(x^{\\prime})-k_{\\mu^{*}}(x,x^{\\prime})\\alpha_{\\mu_{W}}(x^{\\prime})\\mathrm{d}\\rho_{X}(x^{\\prime})\\bigg{)}^{2}$$ $$+2\\bigg{(}\\int k_{\\mu^{*}}(x,x^{\\prime})\\alpha_{\\mu_{W}}(x^{\\prime})-k_{\\mu^{*}}(x,x^{\\prime})\\alpha_{\\mu^{*}}(x^{\\prime})\\mathrm{d}\\rho_{X}(x^{\\prime})\\bigg{)}^{2}$$ $$\\leq2\\sup_{x,x^{\\prime}\\in S_{N}}(k_{\\mu_{W}}(x,x^{\\prime})-k_{\\mu^{*}}(x,x^{\\prime}))^{2}\\|\\alpha_{\\mu_{W}}\\|_{L^{2}(\\rho_{X})}^{2}$$ $$+2\\|\\alpha_{\\mu_{W}-\\rho_{\\mu^{*}}}\\|_{\\rho_{X}}^{2}$$ $$\\leq\\frac{2c_{0}^{2}\\sup_{x,x^{\\prime}\\in S_{N}S}(k_{\\mu_{W}}(x,x^{\\prime})-k_{\\mu^{*}}(x,x^{\\prime}))^{2}}{\\lambda_{a}^{2}}+\\frac{2c_{0}^{2}\\|\\Sigma_{\\mu_{W}}-\\Sigma_{\\mu^{*}}\\|_{\\mathrm{op}}^{2}}{\\lambda_{a}^{4}}.$$\nBy taking the supremum over x and expectation over W, we obtain the result.\n"
    },
    {
        "level": "##",
        "title": "C. Proofs For Section 4 C.1. Lemmas For Section 4",
        "content": "\nLemma C.1. For a given \u03b4, \u03c4 > 0, we have\n\n$$|\\operatorname{tanh}(\\tau z)-(1[z\\geq0]-1[z<0])|\\leq2e^{-2\\tau|z|}$$\nfor any z \u2208 R.\n\nProof.: From the definition of $\\tanh$, we have, for any $z\\geq0$,\n\n$$|\\tanh(\\tau z)-(1[z\\geq0]-1[z<0])|=1-\\frac{e^{\\tau z}-e^{-\\tau z}}{e^{\\tau z}+e^{-\\tau z}}$$ $$=\\frac{2e^{-\\tau z}}{e^{\\tau z}+e^{-\\tau z}}$$ $$\\leq2e^{-2\\tau z}.$$\nSimilarly, for *z <* 0, we have\n\n$$\\tanh(\\tau z)-(1[z\\geq0]-1[z<0])|=1+\\frac{e^{\\tau z}-e^{-\\tau z}}{e^{\\tau z}+e^{-\\tau z}}$$ $$=\\frac{2e^{\\tau z}}{e^{\\tau z}+e^{-\\tau z}}$$ $$\\leq2e^{2\\tau z}.$$\nCombining above arguments, we obtain the result.\n\nLemma C.2. Let \u03c1X be the uniform distribution on [0, 1]d and S :=\n\ufffd\nsin(2\u03c0w \u00b7 x) | w \u2208 {0, 1}d, \u2225w\u22251 = k\n\ufffd\nbe a subset\n\nof L2(\u03c1X). Furthermore, for any fixed basis functions {hj}n\n                                                             j=1 \u2282 L2(\u03c1X), let Hn be a span of {hj}n\n                                                                                                       j=1. Then, for any\n\u03b5 \u2208 [0, 1], we have\n\n$$\\operatorname*{sup}_{\\psi\\in S}d(\\psi,H_{n})\\geq\\varepsilon$$\nif n \u2264 N(1 \u2212 \u03b5), where N = *|S|* =\n\ufffdd k\n\ufffd\n.\n\nProof. Assume that d(\u03c8, Hn) *< \u03b5* for any \u03c8 *\u2208 S*. From Theorem 1 in Hsu (2021), we have *n > N*(1 \u2212 \u03b5) since S is an orthonormal system in L2(\u03c1X) and *|S|* = N. This contradicts n \u2264 N(1 \u2212 \u03b5), which completes the proof.\n\nLemma C.3. For \u03b5, r, rx > 0*, let* \u03bbw = 1, h(x; w) = tanh(x \u00b7 u + b) (w = (u, b)). Then, for any u\u25e6 \u2208 Rd and\n\u02dcf : R \u2192 R which is 1-Lipschitz continuous and differentiable almost everywhere, there exists \u00b5, a *such that* KL(\u03bd | \u00b5) =\n\nO\n \ufffd\n r2\n \u03b52 + d log drrx\n\nd.\n\n_in $\\mathbb{R}$ is a $1$-simplex; commutes with an upper measure $u$_\n\n$$\\begin{array}{l}\\frac{\\sigma_{u}}{\\tau},\\,\\big{\\|}a\\big{\\|}_{L^{2}(u)}=r\\text{and}\\\\ \\\\ \\left|\\hat{f}\\big{(}u^{\\circ}\\cdot x\\big{)}-\\left[f(x;u,\\mu)+\\frac{1}{2}(\\hat{f}(r)+\\hat{f}(-r))\\right]\\right|\\leq\\varepsilon\\end{array}$$\n\n_for any $x\\in\\mathbb{R}^{d}$ such that $|u^{\\circ}\\cdot x|\\leq r$ and $\\|x\\|\\leq r\\sqrt{d}$._\n\nProof.: Let $a(u,b)=r\\hat{a}(b/r)$ for $\\hat{a}(b):\\mathbb{R}\\to\\mathbb{R},\\|\\hat{a}\\|_{\\leq}\\leq1$ and $\\mu(u)=\\mu(u,b):=\\mu(u)\\mu(b)$, where $\\mu(u)=N(ru^{\\circ},a^{\\circ}I),\\mu(b)=v([-\\tau r,\\tau r])$ for $\\tau,\\sigma>0$. In addition, let $\\bar{g}(x)=\\mathbb{E}_{b,\\mu_{0}}[r\\hat{a}(b/r)\\tanh(\\tau\\cdot u^{\\circ}+b)]$. Then, we have\n\n$$|g(x)-f(x;a,\\mu)|\\leq\\int|v\\hat{a}(b/r)\\|\\tanh(x\\cdot u+b)-\\tanh(\\tau x\\cdot u^{\\circ}+b)|d\\mu(\\bar{u},b)$$ $$\\leq\\int|v\\hat{b}(\\sqrt{\\|x\\|}\\,\\|x-u\\cdot\\tau u^{\\circ})|d\\mu(u)$$ $$\\leq r\\sqrt{\\int\\|v\\|^{2}(u-\\tau u^{\\circ})\\|^{2}d\\mu(u)}$$ $$\\leq r\\sqrt{\\int\\|x\\|^{2}\\|u-\\tau u^{\\circ}\\|^{2}d\\mu(u)}$$ \\[\\leq r\\,r_{x}\\sqrt{d}\\omega.\\\nit holds that\n\n$$|\\tilde{g}(x)-\\tilde{g}(x)|\\leq\\int_{-\\tau}^{\\tau}\\frac{1}{2}|\\tilde{a}(b)||\\tanh(\\tau(x\\cdot u^{\\circ}+b^{\\prime}))-(1[x\\cdot u^{\\circ}+b^{\\prime}\\geq0]-1[x\\cdot u^{\\circ}+b^{\\prime}<0])|\\mathrm{d}b^{\\prime}$$ $$\\leq\\int_{-\\infty}^{\\infty}\\frac{1}{2}[\\tanh(\\tau(x\\cdot u^{\\circ}+b^{\\prime}))-(1[x\\cdot u^{\\circ}+b^{\\prime}\\geq0]-1[x\\cdot u^{\\circ}+b^{\\prime}<0])|\\mathrm{d}b^{\\prime}$$ $$\\leq\\int_{0}^{\\infty}e^{-2\\tau z}\\mathrm{d}z$$ $$=1/(2\\tau)$$\nFor any f =\n\ufffd\na(w)h(x; w)d\u00b5(w) *\u2208 F*2R,2M, we have\n\n\u2225f\u2225\u221e \u2264 \ufffd |a(w)|d\u00b5(w) \u2264 \u2225a\u2225L2(\u00b5) 2R. \u2264 \u221a\nThus, for any f\n\u2208\nF2R,2M, l(f(x), y)\n=\nl(f(x), f \u25e6(x))\n\u2264\n4R and |l\u2032(f(x), y)|\n\u2264\n2\n\u221a\n2R.\n\nLet F\u2032\n=\n{(x, y) \ufffd\u2192 l(f(x), y) | f *\u2208 F*2R,2M}. Utilizing the standard uniform bound (Wainwright, 2019), for any \u03b4 \u2208 [0, 1], we have\n\nlog 2/\u03b4 n 2n , \u2264 2R(F\u2032) + 12R E\u03c1[g(*x, y*)] \u2212 1 \ufffd \ufffd\n\ufffd\n\n$$\\operatorname*{sup}_{g\\in\\mathcal{F}^{\\prime}}\\qquad\\qquad\\qquad\\sum_{i=1}^{n}g(x^{(i)},y^{(i)})$$\n2016), we have\n\nwith probability at least 1\u2212\u03b4 over the choice of n i.i.d. samples\n                                                                    \ufffd\n                                                                     (x(i), y(i))\n                                                                                 \ufffdn\n                                                                                  i=1 \u223c \u03c1. From the contraction lemma (Maurer,\n\nR(F\u2032) = E\u03c3 \ufffd \ufffd i=1 \u03c3il(f(xi), yi) sup f\u2208F2R,2M n \ufffd \u2264 2 \u221a 2RR(F2R,2M) (M + 1) = O R , n \ufffd \ufffd \ufffd\n2R,\n\u221a\n2R]. Combining above arguments, we arrive at since l(\u00b7, yi) is 2\n\u221a\n2R-Lipschitz continuous in [\u2212\n\u221a\n\n$$\\bar{L}(a_{\\hat{\\mu}},\\mu)\\leq L(a_{\\hat{\\mu}},\\hat{\\mu})+2\\Re(\\mathcal{F}^{\\prime})+12R\\sqrt{\\log2/\\delta}$$ $$=O\\Bigg{(}\\sqrt{\\frac{M}{n}}+R\\sqrt{(\\frac{M+1}{n})}+R\\sqrt{\\log1/\\delta}\\Bigg{)}$$ $$=O\\Bigg{(}(R+1)\\Bigg{(}\\sqrt{(\\frac{M+1}{n})}+\\sqrt{\\log1/\\delta}\\Bigg{)}\\Bigg{)}$$ $$=O\\Bigg{(}(R+1)\\sqrt{(M+1)}+\\log1/\\delta\\Bigg{)},$$\nsince we set \u03bb = 1/\u221an. This completes the proof.\n"
    },
    {
        "level": "##",
        "title": "C.4. Proof Of Theorem 4.5",
        "content": "\nLet S :=\n\ufffd\nsin(2\u03c0u \u00b7 x) | u \u2208 {0, 1}d, \u2225u\u22251 = k\n\ufffd\nbe a subset of L2(\u03c1X). Note that S is an orthonormal system in L2(\u03c1X).\n\nAssume that\n\n$$\\operatorname*{sup}_{f\\in{\\mathcal{B}}_{M},\\|f\\|_{{\\mathcal{B}}_{M}}^{2}\\leq R}d(f,H_{n})<1/4.$$\nThen, from Lemma C.3, for any \u03c8 = sin(2\u03c0u \u00b7 x) *\u2208 S* (\u2225u\u22251 = k), there exists *a, \u00b5* such that KL(\u03bd | \u00b5) = O(d log dk +\nk2), \u2225a\u2225L2(\u00b5) = k, and\n|\u03c8(x) \u2212 f(x)| \u2264 1/4\nfor any x \u2208 [0, 1]d since |u \u00b7 x*| \u2264 \u2225*u\u22251\u2225x\u2225\u221e \u2264 k and sin(2\u03c0k) = sin(\u22122\u03c0k) = 0. Therefore, we have\n\n$d(\\psi,H_{n})\\leq\\|\\psi-f\\|_{L^{2}(\\rho_{X})}+d(f,H_{n})$\n\n$\\leq1/4+1/4=1/2$.\n\nThis contradicts Lemma C.2. Thus, we obtain the result.\n\nby setting \u03b5 to a sufficiently small constant and *r, r*x sufficiently large constants which are independent of d. Thus, there exists M = O(d log d), R = O(1) such that \u2225f \u25e6 \u2212 f\u22252\nL2(\u03c1X), where f \u2208 FR,M. By the same argument in the proof of Theorem 4.4, we have\n\n(M + 1) + log 1/\u03b4 R n L\u02c6\u03c1(a, \u00b5) \u2264 L\u03c1(*a, \u00b5*) + O \ufffd \ufffd \ufffd\n\nwith probability at least 1 \u2212 \u03b4 over the choice of training data. Thus, by setting n \u2265 c\u2032\n                                                                                          3(d log d + log 1/\u03b4) for sufficiently\nlarge c\u2032\n       3, we have\n\n$$L_{\\hat{\\rho}}(a,\\mu)\\leq L_{\\rho}(a,\\mu)+\\varepsilon^{2}\\leq2\\varepsilon^{2}$$\nFrom the optimality of \u02c6\u00b5 and \u02c6a = a\u02c6\u00b5, we have\n\n$$\\begin{split}\\mathcal{G}_{\\hat{\\rho}}(\\hat{\\mu})&=L(\\hat{a},\\hat{\\mu})+\\frac{\\bar{\\lambda}_{a}}{2}\\|\\hat{a}\\|_{L^{2}(\\hat{\\mu})}^{2}+\\lambda\\operatorname{KL}(\\nu\\mid\\hat{\\mu})\\\\ &\\leq\\mathcal{G}(a,\\mu)\\\\ &\\leq2\\bar{\\varepsilon}^{2}+2\\lambda M\\\\ &\\leq3\\bar{\\varepsilon}^{2},\\end{split}$$\nby setting \u03bb = \u00af\u03f52/2M and \u03bba = *M/R*. Thus, it holds that\n\n$$\\begin{array}{c}{{\\|\\hat{a}\\|_{L^{2}(\\hat{\\mu})}^{2}\\leq12R,}}\\\\ {{\\mathrm{KL}(\\nu\\mid\\hat{\\mu})\\leq6M.}}\\end{array}$$\nThen, by the same reasoning as in the proof of Theorem 4.4, we have\n\n$$U_{\\rho}(\\hat{\\mu})\\leq E_{\\rho}[l(f(x;\\hat{\\mu}),y)]+\\frac{\\bar{\\lambda}_{a}}{2}\\left\\|a_{\\hat{\\mu}}\\right\\|_{L^{2}(\\mu)}^{2}$$ $$\\leq E_{\\hat{\\mu}}[l(f(x;\\hat{\\mu}),y)]+\\frac{\\bar{\\lambda}_{a}}{2}\\left\\|a_{\\hat{\\mu}}\\right\\|_{L^{2}(\\mu)}^{2}+O\\!\\left((R+1)\\sqrt{M+1+\\log1/\\delta}\\right)$$ $$\\leq\\mathcal{G}_{\\hat{\\rho}}(\\hat{\\mu})+O\\!\\left((R+1)\\sqrt{M+1+\\log1/\\delta}\\right)$$ $$\\leq3\\bar{\\varepsilon}^{2}+O\\!\\left((R+1)\\sqrt{M+1+\\log1/\\delta}\\right)$$ $$\\leq4\\bar{\\varepsilon}^{2}$$\nby setting n = c3(d log d + log 1/\u03b4) for a sufficiently large constant c3 \u2265 c\u2032\n3 with probability at least 1 \u2212 \u03b4 over the choice of training data. Therefore, it holds that\n\n$$\\begin{split}A(\\hat{\\mu})&\\geq A'(\\hat{\\mu})\\geq\\frac{\\bar{\\lambda}_a}{2U(\\hat{\\mu})}-\\bar{\\lambda}_a\\\\ &\\geq\\frac{\\bar{\\varepsilon}^2/(2R)}{8\\bar{\\varepsilon}^2}-\\bar{\\varepsilon}^2/2R\\\\ &\\geq\\frac{1-8\\bar{\\epsilon}^2}{16R}\\\\ &=\\Omega(1).\\end{split}$$\n\u2225w\u22252\n2 d\u00b50(w) is equal Let {ui}d i=1 (u1 = u) be an orthonormal basis of Rd. Then, the symmetry of \u00b50 = \u03bd implies that\n\ufffd \u27e8ui,w\u27e92\n\u2225w\u22252\n2 d\u00b50(w) = 1/d. In addition let fi(x) = \u02dcf(ui \u00b7 x). Then, we\n\u2225w\u22252\n2 d\u00b50(w) = 1, we have\n\ufffd \u27e8u,w\u27e92\nfor any i. Since \ufffdd i=1\n\ufffd \u27e8ui,w\u27e92\n\n\u2022\n\u2022\n\ufffd\ufffd\ufffd\u2207 \u03b4U\u02dc\u03b5\n\u03b4\u00b5 (\u00b5)(w)\n\ufffd\ufffd\ufffd \u2264 RUV ,\n\n\ufffd\ufffd\ufffd\u2207w\n\u03b4U\u02dc\u03b5\n\u03b4\u00b5 (\u00b5)(w)\n\ufffd\ufffd\ufffd\n2 \u2264 RUV ,\n\u2022\n\u03b4\u00b52 (\u00b5)(*w, w*\u2032)\n\ufffd\ufffd\ufffd\nop \u2264 LUV .\n\ufffd\ufffd\ufffd\u2207w\u2207\u22a4\nw\n\u03b4U\u02dc\u03b5\n\u03b4\u00b5 (\u00b5)(w)\n\ufffd\ufffd\ufffd\nop \u2264 LUV ,\n\ufffd\ufffd\ufffd\u2207w\u2207\u22a4\nw\u2032 \u03b42U\u02dc\u03b5\nsince U\u02dc\u03b5(\u00b5) = U(\u00b5) \u2212 V (\u00b5) + const. Combining above arguments, Theorem 3 in Suzuki et al. (2023a) yields\n\u03bb\u03b1\n\u00afL2C1(\u03bb\u03b7 + \u03b72) +\n4\n\u03bb\u03b1\u03b7\n\u00af\u03a5 + 2C\u03bb\n\u03bb\u03b1N ,\n\n1\nN E[LN(\u00b5(N)\n        k\n           )] \u2212 L(\u00b5\u2217) \u2264 exp(\u2212\u03bb\u03b1\u03b7k)\n                                 \ufffd 1\n\nN E[LN(\u00b5(N)\n     k\n       )] \u2212 L(\u00b5\u2217)\n            \ufffd\n             + 2\n\n\ufffd\n +\n    1\n   \u00af\u03bbw\n\n\ufffd\ufffd\n  1\n  4 +\n      1\n     \u00af\u03bbw\n\n\ufffd\n R2\n  UV + \u03bbd\u2032\ufffd\n         , \u00afL = LUV + \u00af\u03bbw, C1 = 8[R2\n                            UV + \u00af\u03bbw \u00afR2 + d\u2032], C\u03bb = 2\u03bbLUV \u03b1 +\n\n2\u03bb2L2\n    UV \u00afR2, and\n\nwhere \u00afR2 = E\n            \ufffd\ufffd\ufffd\ufffdw(0)\n               i\n                  \ufffd\ufffd\ufffd\n                   2\n\n\u03bb/\u03b7)\u03b73R2\n       UV .\n\n\u00af\u03a5 := 4\u03b7\u03b4\u03b7 + [RUV + \u03bbw \u00afR + (LUV + \u00af\u03bbw)2](1 +\n                                                 \ufffd\n\n\u03bb/\u03b7)\u03b72R2\n       UV + (RUV + \u00af\u03bbw \u00afR)RUV (1 +\n                               \ufffd\n\nThis completes the proof.\n\n"
    }
]