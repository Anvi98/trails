[
    {
        "level": "##",
        "title": "Controlled Training Data Generation With Diffusion Models",
        "content": "\nTeresa Yeo* Andrei Atanov* Harold Benoit\u2020 Aleksandr Alekseev\u2020\nRuchira Ray Pooya Esmaeil Akhoondi Amir Zamir Swiss Federal Institute of Technology Lausanne (EPFL)\nhttps://adversarial-prompts.epfl.ch/\nrepresented by orange circles. Since there can be many adversarial examples not relevant to a particular target distribution, we introduce a second feedback mechanism that guides the prompts towards a certain target distribution (green circle). Combining the two mechanisms results in Guided Adversarial Prompts, represented by S\u2217. This results in generations that are both *relevant to the target distribution* and where f does not perform well. *Right:* We show classification), and Waterbirds [66] (bird classification). An exemplar training image is shown in the first row and the third row illustrates examples of target distributions. The second row shows the generations from Adversarial Prompts which uses only model feedback. It finds novel \"styles\" that fool a given model f, but may not match a target distribution (third row). Guided Adversarial Prompts (fourth row) uses feedback from the target distribution and generates images that both fool the model and are similar to the target.\n\n                 Abstract\n  In this work, we present a method to control a text-to-\nimage generative model to produce training data specif-\nically \"useful\" for supervised learning.\n                                Unlike previ-\nous works that employ an open-loop approach and pre-\ndefine prompts to generate new data using either a lan-\nguage model or human expertise, we develop an automated\nclosed-loop system which involves two feedback mecha-\nnisms.\n      The first mechanism uses feedback from a given\nsupervised model and finds adversarial prompts that result\nin image generations that maximize the model loss. While\nthese adversarial prompts result in diverse data informed by\nthe model, they are not informed of the target distribution,\nwhich can be inefficient. Therefore, we introduce the second\n\nfeedback mechanism that guides the generation process to-\nwards a certain target distribution. We call the method com-\nbining these two mechanisms Guided Adversarial Prompts.\nWe perform our evaluations on different tasks, datasets and\narchitectures, with different types of distribution shifts (spu-\nriously correlated data, unseen domains) and demonstrate\nthe efficiency of the proposed feedback mechanisms com-\npared to open-loop approaches.\n\n1. Introduction\nThe quality of data plays a crucial role in training general-\nizable deep learning models [21, 52, 74]. For a model to\ngeneralize well, its training data should be representative of\nthe test distribution where it will be deployed. However,\n\nreal world test conditions change over time, while training datasets are typically collected once and remain static due to high collection costs. We, therefore, focus on generating datasets that can adapt to novel test distributions and are more cost-efficient.\n\nDiffusion generative models [33, 55, 61, 67, 70] are trained on large-scale collections of images [69] and exhibit remarkable generalization abilities by being able to produce realistic images not seen during training. Additionally, unlike static datasets that they are trained on, these generative models allow us to *adapt* the generation process to produce images that follow a certain conditioning. For example, they can be conditioned on textual prompts [61] or geometric information such as depth maps [82].\n\nRecent works explore the use of diffusion models to generate training data for supervised learning with promising results [18, 28, 68]. They guide the generation process using text prompts to accomplish two goals: produce aligned image-label pairs for supervised training and adapt the generated images to a certain target distribution. These proposed methods, however, find conditioning text prompts in an open-loop way by either using a language model [18] or heuristics [68]. Therefore, they lack an automatic feedback mechanism that can refine the found text prompts to produce more curated and useful training data. Furthermore, it has been argued that being able to control the input data is a key contributor to how children are able to learn with few examples [6, 44].\n\nIn this work, we propose two feedback mechanisms to find prompts for generating useful training data. The first mechanism finds prompts that result in generations that maximize the loss of a particular supervised model, thus, *reflecting its failure modes*. We call them Adversarial Prompts (AP). This mechanism ensures that we find not only novel prompts, which may produce images that the model already performs well on, but adversarial prompts that produce images with high loss, and, thus, useful for improving the model [14] (see exemplar generations for AP in Figs. 1, 3 and 5)\nA given model can perform poorly on multiple distribution shifts, and traversing all of them with adversarial optimization to adapt it to a specific target distribution can be inefficient (e.g., see the difference between AP generations and the illustrated target distributions in Fig. 1-right). Therefore, we introduce an additional *target-informed* feedback mechanism that finds prompts that generate images similar to those from the target distribution we want to adapt to. To implement this, we assume either access to a textual description of the target distribution or a few sampled (unlabeled) images from it. We then optimize a similarity metric between CLIP [57] embeddings of the generated examples and the target description. We call these prompts Guided Adversarial Prompts (GAP). Compare the columns Adversarial Prompts and Guided Adversarial Prompts in Figs. 1 and 3 to 5 to see the effect of CLIP guidance in steering the generations towards a specific target distribution.\n\nWe demonstrate the effectiveness of our method on different tasks (image classification, depth estimation), datasets with distribution shifts (Waterbirds [66], iWild- Cam [4, 41], Common Corruptions [29], 3D Common Corruptions [37]), and architectures (convolutional and transformer) with supportive results.\n"
    },
    {
        "level": "##",
        "title": "2. Related Work",
        "content": "\nOpen-loop data generation methods use pre-defined controls to guide the generative process and produce novel training examples. One line of work uses GANs [8, 35, 59]\nand pre-define a perturbation in their latent space to generate novel examples.\n\nMore recent works adopt text-toimage diffusion models and use pre-defined prompt templates [27, 68, 77] or use a language model to generative variations of a given prompt [77]. These methods require anticipating the kind of data that will be seen at test-time when defining the prompts. On the other hand, our CLIP guidance mechanism allows us to generate images similar to the target distribution. [18] also approach this problem by using a captioning and language model to summarize a target distribution shift into a text prompt. However, this summarization process is not informed of the generations, and, thus, does not guarantee that the text prompt will guide the generation process to images related to the target distribution. Finally, these methods are not model-informed and do not necessarily generate images *useful* for training a given model.\n\nClosed-loop data generation methods guide the generation process via an automatic feedback mechanism. They control the latent space of GANs [5] or VAEs [76] models, NeRF [15], or the space of hand-crafted augmentations [10] to generate data that maximizes the loss of the network on the generated data. Similarly, [36] uses an SVM to identify the failure modes of a given model and uses this information to generate training data with a diffusion model. Our method employs a similar adversarial formulation (in conjunction with target distribution guidance) but performs the optimization in the text prompt space of recently developed diffusion models.\n\n\"Shallow\" data augmentation techniques apply simple hand-crafted transformations to training images to increase data diversity and improve the model's generalization.\n\nExamples of such transformations are color jitter, random crop, and flipping, etc. To produce more diverse augmentations, methods like RandAugment [11] and Aug- Mix [30] combine multiple of such simple transformations, and Mixup [81] and CutMix [78] methods use transformations that can combine multiple images. AutoAugment [10] and adversarial training [49] build a closed system to tune the parameters of the applied augmentations but are inherently limited by the expressiveness of the simple transformations. In contrast, our method uses expressive diffusion models, which results in images that are more diverse and realistic than those produced by \"shallow\" augmentations.\n\nControlling diffusion models. Methods like Control-\nNet [82] and T2I-Adapter [54] adapt a pre-trained diffusion model to allow for additional conditioning e.g., edge, segmentation, and depth maps. We employ these models for generation as it allows us to generate paired data for different tasks, given the labels from an existing dataset. Editing methods aim to modify a given image, either via the prompt [32], masks [9], instructions [7] or inversion of the latent space [34, 53]. In contrast, personalization methods aim to adapt diffusion models to a given concept e.g., an object, individual, or style. Popular examples include textual inversion [22] and DreamBooth [64], which aim to find a token to represent a concept given several images of that concept. The former freezes the diffusion model, while the latter fine-tunes it. Extensions of these works learn to represent multiple concepts [1, 24]. In our work, we adopt an approach similar to textual inversion to steer the diffusion model, but our method can also be used with other controlling mechanisms.\n"
    },
    {
        "level": "##",
        "title": "3. Method",
        "content": "\nWe begin this section by formalizing our problem setting and describing how diffusion models can be used to generate training data (Sec. 3.1). We then introduce two feedback mechanisms to find prompts that are informed of the failure modes of a given model (Sec. 3.2) and relevant to a given target distribution (Sec. 3.3).\n"
    },
    {
        "level": "##",
        "title": "3.1. Preliminaries",
        "content": "\nProblem Formulation. We consider the problem of supervised learning, where a model f : *X \u2192 Y* learns a mapping from the image space X, to a target space Y, e.g., a depth estimation or semantic classification problem. The model f is trained using training dataset Dtrain and tested on a new set Dtest that exhibits a distribution shift w.r.t. the training data. Our goal is to generate additional *synthetic* training data Dsyn to adapt the model and improve its performance under the distribution shift. To apply the target-informed feedback mechanism described in Sec. 3.3, we assume access to some information about the test distribution, either from text descriptions or a few samples of *unlabeled* images.\n\nText-to-image Diffusion Models.\n\nWe use the Stable Diffusion [62] text-to-image diffusion model as the basis for our generator g. Given a textual prompt c, Stable Diffusion is capable of synthesizing realistic images following the textual conditioning. However, in general, for a given task, e.g., depth estimation, a textual prompt alone may not be sufficient for controlling the generation well enough to produce aligned image-label examples.\n\nGenerating aligned training examples.\n\nWe employ the following two approaches to condition the generative model g on the label y and sample aligned training examples\n(\u02dc*x, y*). For the depth estimation task, we use the Control- Net [83] model which extends the conditioning mechanisms of the Stable Diffusion to accept various spatial modalities, e.g., depth maps, segmentation masks, or edges. Specifically, we use ControlNet v1.0 with depth conditioning1.\n\nFor semantic classification tasks, we utilize the foreground object masks and use an in-painting technique proposed in [47] that preserves the masked region throughout the denoising process, essentially keeping it intact. These mechanisms provide us with a generative model conditioned both on a text prompt c and label y. We denote the resulting distribution modeled by this generative model as g(*y, c*).\n"
    },
    {
        "level": "##",
        "title": "3.2. Model-Informed Generation With Adversarial Prompt Optimization",
        "content": "\nOur first feedback mechanism aims at generating training examples that reflect the failure modes of a given model f.\n\nAn automatic way to do so is via adversarial optimization, which finds the \"worst case\" failure modes of f.\n\nMore precisely, we find a text prompt c that generates images \u02dcx \u223c g(*y, c*) that maximize the supervised loss L(f(\u02dcx), y), e.g., l1 loss for depth estimation.\n\nSince the usual prompt space is discrete (text tokens) and challenging to optimize over, we employ the approach introduced in Textual Inversion [22] and instead optimize over the corresponding continuous embedding space. For ease of notation, \"prompt space\" will implicitly refer to the continuous embedding space instead of the discrete token space.\n\nWe construct a prompt cw out of n new \"placeholder\" tokens, i.e., cw = (cw1*, . . . , c*wn), and find their corresponding embedding weights {wi}n i=1 by solving the following optimization problem:\n\n$$w_{\\rm AP}=\\arg\\min_{w}\\mathbb{E}_{y}\\ \\mathbb{E}_{\\tilde{x}\\sim g(y,c_{w})}\\mathcal{L}_{\\rm adv}(f(\\tilde{x}),y),\\tag{1}$$\nwhere Ladv = \u2212L and y is sampled from Dtrain. Note that the sample \u02dcx is differentiable w.r.t. the embeddings w which allows us to use gradient-based optimization. We call the prompts that result from solving the above optimization problem Adversarial Prompts (AP).\n\nAvoiding (\u02dc*x, y*) alignment collapse.\n\nThe adversarial objective in Eq. (1) aims to fool the model f. However, it may instead fool the label-conditioning mechanism of the generative model g, resulting in cwadv generating samples\n\u02dcx \u223c g(*y, c*wadv) that are not faithful to y (see Fig. 2). To avoid this, we further constrain the expressiveness of the generation process. There are several ways to do so.\n\n1https://github.com/lllyasviel/ControlNet One way is to use the SDEdit method [50], which conditions the generation process on the original image by starting the denoising process from a noised version of x instead of pure noise. Thus, it constrains the expressive power of the generative model to produce samples closer to the original image x.\n\nAdditionally, some constraints can be implemented w.r.t Ladv. For the depth estimation task, we employ an early stopping criterion and stop the adversarial optimization when the loss reaches a certain task-specific threshold. For semantic classification, choosing Ladv to be the negative cross-entropy loss, although natural, may not be a good choice. Indeed, for iWildCam, although we keep the class mask intact, we observed that optimizing the negative crossentropy loss may lead to the generation of another class somewhere else in the image, e.g. an elephant is generated next to a giraffe, destroying the (\u02dc*x, y*) alignment. Thus, for iWildCam, we choose to maximize the uncertainty or entropy of the model's prediction on the generated images. We provide more details in the Appendix Sec. 7.5.2.\n\nFinally, our CLIP [57] guidance loss introduced in Sec. 3.3 further constrains possible perturbations to a target distribution and helps to avoid the generation of nonrealistic images.\n"
    },
    {
        "level": "##",
        "title": "3.3. Target Distribution Informed Generation",
        "content": "\nThe adversarial formulation above finds prompts that reflect the failure modes of f. Without any information about the target distribution, improving the model on the worstperforming distributions is one of the best strategies one can do and, indeed, improves performance in some cases (see Fig. 4 and Fig. 6a). However, there are typically multiple failure modes of a given model and many possible distribution shifts that can occur at test-time. Adapting to all of them using only the first feedback mechanism could be inefficient when the goal is to adapt to a specific target distribution instead of improving the performance on average.\n\nThus, we introduce the second feedback mechanism to inform the prompt optimization process of the target distribution. This only requires access to simple text descriptions (e.g., 'fog' to adapt to foggy images) or a small number (\u223c 100) of **unlabelled** images.\n\nWe implement the target-informed feedback mechanism using CLIP [57] guidance. Specifically, we assume access to either textual descriptions of the target image distribution\n{tj}, a few unlabeled image samples {xj} or both. We then construct the corresponding text and image guidance embeddings as et = avg({Et(tj)}) and ei = avg({Ei(xj)}), where Et and Ei denote, respectively, the CLIP text and image encoders, and avg stand for averaging. We then use the following guidance loss:\nLCLIP(\u02dc*x, c*w) = \u03bbtLt(Et(cw), et) + \u03bbiLi(Ei(\u02dcx), ei), (2)\nwhere we take Lt to be l2 norm between two embeddings and Li to be the negative cosine similarity, as we found it to perform the best. See the Appendix Sec. 7.7 for the results of this ablation. Note, that based on the available information, one can also use only one of the two guidance losses. Finally, we combine both adversarial, Eq. (1), and CLIP guidance, Eq. (2), losses to form the final objective:\n\n$$w_{\\rm GAP}=\\arg\\min_{w}\\mathbb{E}_{y}\\,\\mathbb{E}_{\\tilde{x}\\sim g(y,c_{w})}\\left[\\mathcal{L}_{\\rm adv}(f(\\tilde{x}),y)+\\mathcal{L}_{\\rm CLIP}(\\tilde{x},c_{w})\\right]\\tag{3}$$\n\nWe call the prompts that result from solving Eq. (3), Guided Adversarial Prompts\n\n(GAP). See the Appendix Sects. 7.4.2 and 7.5.3 for further implementation details.\n"
    },
    {
        "level": "##",
        "title": "4. Experiments",
        "content": "\nWe perform experiments in three settings: domain generalization via camera trap animal classification on the iWild- Cam [4] dataset, bird classification with spurious correlation with the Waterbirds [66] dataset, and depth estimation with the Taskonomy dataset [79, 80]. For depth estimation, the considered distribution shifts are Common Corruptions [29] (CC), 3D Common Corruptions [37] (3DCC) applied on the Taskonomy [80] test set and cross dataset shift from the Replica [73] dataset.\n"
    },
    {
        "level": "##",
        "title": "4.1. Semantic Classification",
        "content": "\nWaterbirds [66] is a dataset constructed by pasting an image of either a waterbird or landbird from the CUB [75] dataset, which represents the label y, onto a \"land\" or \"water\" background image from the Places [84] dataset. We follow [18] and take only images of waterbirds appearing on water and landbirds on land background as Dtrain, i.e., in the training data, the background alone is predictive of the bird class. The examples from the test distribution Dtest contain all four combinations of bird class and background.\n\niWildCam [3, 42] is a domain generalization dataset, made up of a large-scale collection of images captured from camera traps placed in various locations around the world. We seek to learn a model that generalizes to photos taken from new camera deployments. We follow [18] and subsample the dataset to create a 7-way classification task (background, cattle, elephant, impala, zebra, giraffe, dik-dik), with 2 test locations that are not in the training or validation set. We also fixed the number of additional generated images for finetuning to 2224 images in order to match the setting of [18].\n\nFor each dataset, we compare the following methods (we provide more details in Appendix Sec. 7):\nNo Extra Data: We train a ResNet50 [26] model using the original training data Dtrain without using any extra data.\n\nAugmentation baselines: We compare to two data augmentation baselines taken from recent literature: CutMix [78] and RandAugment [12]. Agnostic Prompts: We use a prompt that is not informed of the model or the target distribution. Similar to ALIA [18], we use a prompt *\"nature\"* for Waterbirds and the prompt template \"a camera trap photo of {*class name*}\".\n\nGuided Prompts (ALIA [18]): This approach uses a captioning and language model to summarize a target distribution shift into text prompts. Specifically, we use the prompts found by the ALIA method. This results in seven prompts for Waterbirds and four prompts for iWildCam. See Appendix Sec. 7.2 for more details and a discussion on the differences between ALIA and our method. Adversarial Prompts: We use the model trained without extra data as the target model f and find adversarial prompts following Eq. (1). We find four prompts per class for Waterbirds, eight in total, and four prompts in total applied to all classes for iWildCam. Guided Adversarial Prompts: We use the same setting as in Adversarial Prompts and apply additional CLIP guidance to adapt to a target distribution shift, following Eq. (3).\n\nFor Waterbirds, we apply text guidance using ALIA prompts as the textual description of the target distribution. For iWildCam, we use image guidance and partition the target test distribution into four groups based on two attributes that have significant impact on the visual characteristics of the data; the test location (first or second) and time of the day (day or night). We sample 64 unlabelled images randomly from each group. We optimize for one guided adversarial prompt per group.\n\nTraining details. For both datasets, we perform adversarial optimization with constant learning rate of 1e-3 using Adam [38]. We use the DDIM [72] scheduler. See Tab. 1 for a summary of the experimental parameters. The adversarial loss Ladv from Eq. (1) is defined as the negative cross-entropy loss for Waterbirds. As mentioned in Sec. 3.2, we optimize the entropy loss for iWildCam. More precisely, this loss is equal to the cross entropy loss where the target label y is replaced by the soft label \u02dcy = { 1\n|Y|, . . . ,\n1\n|Y|}, the uniform distribution over all classes. This loss explicitly encourages generations that either (1) do not contain new animals (2) contain new animals that are not accounted for in the label space Y. For more details, see the Appendix.\n\nWaterbirds results. Fig. 3-left shows the performance\n\n| Dataset    |   SDEdit strength |   denoising steps |   guidance scale |   # placeholder tokens | Opt. steps (AP/GAP)   |\n|------------|-------------------|-------------------|------------------|------------------------|-----------------------|\n| \u03bb          |                   |                   |                  |                        |                       |\n| t          |                   |                   |                  |                        |                       |\n| /\u03bb         |                   |                   |                  |                        |                       |\n| i          |                   |                   |                  |                        |                       |\n| Waterbirds |               1   |                 5 |                7 |                      5 | 1000/1000             |\n| iWildCam   |               0.8 |                 5 |                5 |                     10 | 2000/10000            |\n\nof each method, and Fig. 3-right demonstrates the corresponding generated examples for each method. First, we find that while Adversarial Prompts works on par with the Agnostic Prompts in a low-data regime, it performs worse with more generated data. Looking at the corresponding generations, we see that, while being adversarial, these images are different from the target distribution, which can explain the inferior performance on this particular target distribution. Second, using text prompts informed of the target leads to a consistent improvement over the Agnostic Prompts, and the corresponding images look more similar to the real images from the target distribution.\n\nFinally, Guided Adversarial Prompts combining both feedback mechanisms results in more data-efficient generations outperforming all other methods in the low-data regime. Looking at the corresponding generations, we can see that GAP tends to generate only useful combinations, i.e., waterbirds on land and landbirds on water that are missing in the training dataset. This can explain its higher sample efficiency.\n\nFinetuned model\nData\nResNet\nViT\nAgnostic Prompts\n72.94\n73.07\nGAP from ResNet\n83.97\n72.61\nGAP from ViT\n83.87\n77.21\n\niWildCam results.\n\nFig. 4 shows that having modelinformed feedback (Adversarial Prompts) helps to generate more useful data than no feedback mechanism (Agnostic Prompts) and also improves the performance of the target-only informed Guided Prompts method in the lowdata regime.\n\nGuided Adversarial Prompts combines the benefits of both model- and target-informed feedback mechanisms, consistently outperforming other methods. Fig. 4 shows exemplar generations for each method. We find that while AP generates images distinct from the target distribution and images generated by target-informed methods (snow background vs. grass background), training a model using these examples in the low-data regime performs better than GP and similar to GAP.\n\nFine-tuning with Guided Adversarial Prompts from a different model.\n\nTo evaluate our method's ability to customize data to a model, f, and assess its resilience to changes in model architecture, we changed the model from a ResNet50 [26] to a ViT-B-16 [16] model.\n\nIn Tab. 2, we investigate whether prompts optimized using one model feedback can be useful for finetuning another model, and conclude that model feedback does indeed generate data more useful for that particular model. Furthermore, the data generated from one model when used to fine-tune another model can still significantly improve performance over Agnostic Prompts. See Appendix Fig. 14 for the results of Fig. 4 with a ViT-B-16 model, similar trends hold.\n"
    },
    {
        "level": "##",
        "title": "4.2. Depth Estimation",
        "content": "\nFor depth estimation, we consider the following pre-trained models as f: 1) a U-Net [63] model trained on the Taskonomy dataset [79, 80] and 2) a dense prediction transformer (DPT) [58] model trained on Omnidata [20].\n\nWe compare the following methods. They all involve fine-tuning f, but on different datasets. We use Control-\nNet v1.0 with depth conditioning for the experiments in this section. See Fig. 5 for a comparison of the generations:\nControl (No extra data): We fine-tune f on the original training data. This baseline is to ensure that the difference in performance is due to the generated data, rather than e.g., longer training or optimization hyperparameters. Agnostic Prompts: This baseline generates data that is agnostic to the model or the target distribution. We generate images with the prompt \"*room*\" as the datasets consist of indoor images from mostly residential buildings. Agnostic Prompts (Random): We generate data with \"random\" prompts. In our proposed method, we optimize for n embedding vectors, resulting in a prompt, c. Thus, to match this setting, from a Gaussian distribution fitted on the embeddings from the vocabulary, we sample n random embeddings to create a random prompt to be used in the data generation. Adversarial Prompts: We perform the optimization as described in Eq. (1) and fine-tune on this data. Guided Prompts: We perform the optimization using only the loss described in Eq. (2) and fine-tune on this data. Guided Adversarial Prompts: In addition to optimizing the adversarial loss, we also optimize the CLIP guidance loss as described in Eq. (3). This allows us to generate data that is also informed of a certain distribution shift.\n\nTraining details. The adversarial optimization was done with AdamW [46], learning rate of 5.0 \u00d7 10\u22124, weight decay of 1.0 \u00d7 10\u22123, and batch size of 8. We set the early stopping threshold (mentioned in Sec. 3.2) to 0.08 for the UNet model and 1.0 for the DPT model. They were trained with \u21131 and Midas loss [20] respectively. We perform a total of 30 runs to get different Adversarial Prompts. We use the DDIM [72] scheduler. During optimization, we use only 5 denoising steps, as it is more stable. For Guided Adversarial Optimization, the guidance coefficient for text and image guidance is 1 and 5 respectively. For fine-tuning, we generate images with 15 steps. For the GP runs with SDEdit, we used strength 0.6, for the GAP runs, strength 0.9. See the Appendix Sec. 8.1 for further details.\n\nComparing the generated images with different prompts. Fig. 5-left shows the results of the generations for the baselines and our method, optimized on the Taskonomy dataset. The generation with Agnostic Prompts are visually different from that of the original image, however, they tend to have similar styles. In contrast, the generations with Adversarial Prompts have more complex styles and are more diverse. Using SDEdit during the optimization and generation results in generations that are closer to the original image, as it was also used as conditioning. The last four columns show the results of using CLIP text guidance for the target distribution shift *fog* and *blur*, as described in Sec. 3.3, with and without adversarial optimization. The generations with Guided Prompts involve passing the depth conditioning and prompt \"fog\" or \"blur\" to the diffusion model. In both cases, the generations result in a mild level of fog or blur. In contrast, Guided Adversarial Prompts results in more severe fog and blur corruptions. See Appendix Sec. 8.4.2 for generations using image guidance.\n\nNote that all the generations follow the conditioning, i.e., depth labels (see first column). See Sec. 3.2 for the discussion on how we prevent the generations from collapsing. Thus, this gives us *aligned training data*, i.e., RGB images and depth labels that we can use for fine-tuning.\n\nPerformance on OOD data. We evaluate our method and the baselines after fine-tuning on their respective generated datasets. Tab. 6a demonstrated that data generated with Adversarial Prompts improve the performance of the model under different distribution shifts. Furthermore, we show that the trend holds with the DPT model. Thus, our method is able to successfully find useful Adversarial Prompts for different architectures.\n\nPerformance of GAP against amount of generated data. Fig. 5-right shows the performance of our method on the *defocus blur* corruption over the amount of extra generated data (See Appendix Fig. 18 for results on other corruptions).\n\nGuided Prompts or Guided Adversarial Prompts results in a large improvement in performance, compared to Adversarial Prompts or the baseline with only 10 extra data points. This suggests that the guidance loss successfully steered the generations toward producing training data relevant to the distribution shift. This experiment was performed with image guidance. In Appendix Sec. 8.4.2, we compare the qualitative and quantitative differences from image and text guidance. Text guided prompts tends to generate corruptions that are more realistic\n(a) **Quantitative results on depth estimation.** \u21131 errors on the depth prediction task for a pre-trained U-Net and DPT model. (Lower is better. U-Net losses are multiplied by 100 and DPT losses by 10, for readability). Note that these models were trained with different losses, \u21131 for the former and Midas loss [19] for the latter, thus their performance is not comparable. We evaluate on distribution shifts from Common Corruptions (CC), 3D Common Corruptions (3DCC), and cross-datasets (CDS), Replica. The results from CC and 3DCC are averaged over all distortions and severity levels on Taskonomy. Our method is able to generate training data that can improve results over the baselines on several distribution shifts. Performing AP with SDEdit gives better results than AP under distribution shifts. Thus, conditioning on the original image seems to be helpful for these shifts. For the DPT model, the trends are similar, AP performs better than the baselines. See Appendix Tab. 6 for results on other baselines, in particular, data augmentation ones.\n\n|                         | U-Net    | DPT       |\n|-------------------------|----------|-----------|\n| Taskonomy               | Replica  | Taskonomy |\n| Shift                   | Clean    | CC        |\n| Control (No extra data) |          |           |\n| 2.35                    |          |           |\n| 4.93                    | 4.79     | 5.38      |\n| Agnostic Prompts        | 2.47     | 5.03      |\n| Agnostic Prompts        | (Random) | 2.38      |\n| Adversarial Prompts     | 2.49     | 4.36      |\n| Adversarial Prompts     | (SDEdit) | 2.59      |\n| 4.20                    | 3.88     | 4.96      |\n\n(see Appendix Fig. 18 for comparisons). However, image guidance tends to perform better quantitatively across the different target distributions.\n\nUnlike classification, the performance of Guided Adversarial Prompts for depth is not consistent across all distribution shifts. The diffusion model was not able to generate certain shifts e.g., noise corruptions, as the text descriptions and unlabelled images was too ambiguous e.g., 'noise' or having common attributes other than the corruption. We leave further analysis to future work.\n\n(b) Comparing the performance from running multiple iterations versus a single iteration of adversarial optimization, generation, and fine-tuning. The plot shows the \u21131 loss (\u00d7100) of the U-Net model against the number of iterations on the depth prediction task. The \u21131 loss is computed on the Taskonomy dataset under common corruptions, averaged over all corruptions and severity levels. The single- and multi-iteration runs have similar settings in terms of the total number of AP used, the same number of tokens per prompt, etc. See Appendix Sec. 8.4.1 for further details. The first iteration of the multi-iteration run resulted in the Running multiple iterations of adversarial optimization vs a single iteration. We define an *iteration* as one round of adversarial optimization, i.e. optimizing Eq. (1) or Eq. (3), generation and fine-tuning. Given that all of the above results were obtained with a single iteration, we aim to see if there are benefits in performing multiple iterations. We perform a total of 8 iterations and we compare this to performing a single iteration. The experimental settings for 8 iterations and a single iteration are similar, e.g., in total, over the 8 iterations, we optimize for the same number of Adversarial Prompts, and fine-tune on the same datapoints, etc. Fig. 6b shows that the first iteration of the 8 iterations setting resulted in the largest improvement in performance, eventually converging to the performance of the single iteration approach. Thus, we chose to perform a single iteration for the results in Fig. 6a. See the Appendix Sec. 8.4.1 for further implementation details and results.\n"
    },
    {
        "level": "##",
        "title": "5. Conclusion And Limitations",
        "content": "\nIn this work, we aim to generate training data useful for training a supervised model by steering a text-to-image generative model. We introduced two feedback mechanisms to find prompts that are informed by both the given model and the target distribution. Evaluations on a diverse set of tasks and distribution shifts show the effectiveness of the proposed closed-loop approach in comparison to open-loop ones. Below we briefly discuss some of the limitations:\nLabel shift: In this work, we focus on generating novel images. However, some distribution shifts can also change the label distribution, e.g., for depth estimation, changing from indoor to outdoor scenes would result in a shift in depth maps. One possible approach could be learning a generative model over the label space [43] to control the generation in both the label and image space. Computational cost: Estimating the gradient of the loss in Eq. (3) requires backpropagation through the denoising process of the diffusion model, which can be computationally demanding. Using approaches that reduce the number of denoising steps [48, 71] may be able to reduce this computational cost. Label Conditioning: As discussed in Sec. 3.2, our method is limited by the faithfulness of the generation conditioned on the given label. For example, we found that the semantic segmentation ControlNet does not follow the conditioning accurately enough to be useful for the supervised model. Further developments in more robust conditioning mechanisms are needed to successfully apply our method to other tasks.\n"
    },
    {
        "level": "##",
        "title": "References",
        "content": "\n[1] Omri Avrahami, Kfir Aberman, Ohad Fried, Daniel Cohen-\nOr, and Dani Lischinski.\nBreak-a-scene:\nExtracting\nmultiple concepts from a single image.\narXiv preprint\narXiv:2305.16311, 2023. 3\n[2] Sara Beery, Dan Morris, and Siyu Yang. Efficient Pipeline\nfor Camera Trap Image Review, 2019. arXiv:1907.06772 [cs]. 15\n[3] Sara Beery, Arushi Agarwal, Elijah Cole, and Vighnesh\nBirodkar. The iwildcam 2021 competition dataset. arXiv\npreprint arXiv:2105.03494, 2021. 1, 5, 14, 15\n[4] Sara Beery, Arushi Agarwal, Elijah Cole, and Vighnesh\nBirodkar. The iwildcam 2021 competition dataset. arXiv preprint arXiv:2105.03494, 2021. 2, 4\n[5] Victor Besnier, Himalaya Jain, Andrei Bursuc, Matthieu\nCord, and Patrick P\u00b4erez. This dataset does not exist: training models from generated images. In ICASSP 2020-2020\nIEEE international conference on acoustics, speech and signal processing (ICASSP), pages 1\u20135. IEEE, 2020. 2\n[6] Oliver Braddick and Janette Atkinson.\nVisual control of\nmanual actions: brain mechanisms in typical development and developmental disorders. Developmental Medicine and Child Neurology, 55 Suppl 4:13\u201318, 2013. 2\n[7] Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 18392\u201318402, 2023. 3, 13\n[8] Lucy Chai, Jun-Yan Zhu, Eli Shechtman, Phillip Isola, and\nRichard Zhang. Ensembling with Deep Generative Views, 2021. arXiv:2104.14551 [cs]. 2\n[9] Guillaume Couairon, Jakob Verbeek, Holger Schwenk,\nand Matthieu Cord.\nDiffedit:\nDiffusion-based semantic image editing with mask guidance.\narXiv preprint\narXiv:2210.11427, 2022. 3\n[10] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning augmentation policies from data. *arXiv preprint arXiv:1805.09501*, 2018.\n2\n[11] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V\nLe.\nRandaugment:\nPractical automated data augmentation with a reduced search space.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition workshops, pages 702\u2013703, 2020. 2\n[12] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V\nLe.\nRandaugment:\nPractical automated data augmentation with a reduced search space.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition workshops, pages 702\u2013703, 2020. 5\n[13] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009. 14\n[14] Zhun Deng,\nLinjun Zhang,\nKailas Vodrahalli,\nKenji\nKawaguchi,\nand\nJames\nZou.\nAdversarial\nTraining\nHelps Transfer Learning via Better Representations, 2021. arXiv:2106.10189 [cs]. 2\n[15] Yinpeng Dong, Shouwei Ruan, Hang Su, Caixin Kang,\nXingxing Wei, and Jun Zhu.\nViewfool: Evaluating the\nrobustness of visual recognition to adversarial viewpoints. Advances in Neural Information Processing Systems, 35:\n36789\u201336803, 2022. 2\n[16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, and others.\nAn image is worth 16x16 words:\nTransformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 6, 18\n[17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, and others.\nAn image is worth 16x16 words:\nTransformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 13\n\n[18] Lisa Dunlap, Alyssa Umino, Han Zhang, Jiezhi Yang,\nJoseph E Gonzalez, and Trevor Darrell. Diversify your vision datasets with automatic diffusion-based augmentation. arXiv preprint arXiv:2305.16289, 2023. 2, 4, 5, 13, 14, 15,\n20\n[19] Ainaz Eftekhar, Alexander Sax, Jitendra Malik, and Amir\nZamir. Omnidata: A scalable pipeline for making multi-task mid-level vision datasets from 3D scans. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10786\u201310796, 2021. 8\n[20] Ainaz Eftekhar, Alexander Sax, Jitendra Malik, and Amir\nZamir. Omnidata: A Scalable Pipeline for Making Multi- Task Mid-Level Vision Datasets from 3D Scans. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10786\u201310796, 2021. 6, 7, 20\n[21] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan\nHayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim Entezari, Giannis Daras, Sarah Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen Mussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, Pang Wei Koh, Olga Saukh, Alexander Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh, Alex Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, and Ludwig Schmidt.\nDataComp: In\nsearch of the next generation of multimodal datasets, 2023. arXiv:2304.14108 [cs]. 1\n[22] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen- Or.\nAn image is worth one word: Personalizing text-toimage generation using textual inversion.\narXiv preprint\narXiv:2208.01618, 2022. 3, 13, 18\n[23] Robert Geirhos,\nPatricia Rubisch,\nClaudio Michaelis,\nMatthias Bethge, Felix A Wichmann, and Wieland Brendel. ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness. arXiv\npreprint arXiv:1811.12231, 2018. 20\n[24] Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar,\nDimitris Metaxas, and Feng Yang.\nSvdiff: Compact parameter space for diffusion fine-tuning.\narXiv preprint\narXiv:2303.11305, 2023. 3\n[25] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 770\u2013778, 2016. 14\n[26] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 770\u2013778, 2016. 5, 6, 13\n[27] Ruifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing\nZhang, Philip Torr, Song Bai, and Xiaojuan Qi. Is synthetic data from generative models ready for image recognition? arXiv preprint arXiv:2210.07574, 2022. 2\n[28] Ruifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing\nZhang, Philip Torr, Song Bai, and Xiaojuan Qi. Is synthetic\ndata from generative models ready for image recognition? arXiv preprint arXiv:2210.07574, 2022. 2\n\n[29] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. *arXiv preprint arXiv:1903.12261*, 2019. 2, 4, 21\n[30] Dan Hendrycks, Norman Mu, Ekin D Cubuk, Barret Zoph,\nJustin Gilmer, and Balaji Lakshminarayanan. Augmix: A simple data processing method to improve robustness and uncertainty. *arXiv preprint arXiv:1912.02781*, 2019. 2\n[31] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, and others. The many faces of robustness: A critical analysis of out-of-distribution generalization. In Proceedings of the IEEE/CVF international conference on computer vision, pages 8340\u20138349, 2021. 20\n[32] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control.\narXiv preprint\narXiv:2208.01626, 2022. 3\n[33] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020. 2\n[34] Inbar Huberman-Spiegelglas, Vladimir Kulikov, and Tomer\nMichaeli. An edit friendly DDPM noise space: Inversion and manipulations. *arXiv preprint arXiv:2304.06140*, 2023. 3\n[35] Ali Jahanian, Xavier Puig, Yonglong Tian, and Phillip Isola.\nGenerative models as a data source for multiview representation learning. *arXiv preprint arXiv:2106.05258*, 2021. 2\n[36] Saachi Jain, Hannah Lawrence, Ankur Moitra, and Aleksander Madry. Distilling model failures as directions in latent space. *arXiv preprint arXiv:2206.14754*, 2022. 2\n[37] O\u02d8guzhan Fatih Kar, Teresa Yeo, and Amir Zamir. 3D Common Corruptions for Object Recognition.\nIn ICML 2022\nShift Happens Workshop, 2022. 2, 4, 21\n[38] Diederik P. Kingma and Jimmy Ba. Adam: A Method for\nStochastic Optimization, 2017. arXiv:1412.6980 [cs]. 5, 15\n[39] Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes, 2022. arXiv:1312.6114 [cs, stat]. 18, 20\n[40] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll\u00b4ar, and\nRoss Girshick. Segment Anything, 2023. arXiv:2304.02643 [cs]. 14\n[41] Pang\nWei\nKoh,\nShiori\nSagawa,\nHenrik\nMarklund,\nSang Michael Xie,\nMarvin Zhang,\nAkshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, and others.\nWilds: A benchmark of\nin-the-wild distribution shifts. In International conference on machine learning, pages 5637\u20135664. PMLR, 2021. 2\n[42] Pang\nWei\nKoh,\nShiori\nSagawa,\nHenrik\nMarklund,\nSang Michael Xie,\nMarvin Zhang,\nAkshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, and others.\nWilds: A benchmark of\nin-the-wild distribution shifts. In International Conference on Machine Learning, pages 5637\u20135664. PMLR, 2021. 5\n[43] Guillaume Le Moing, Tuan-Hung Vu, Himalaya Jain, Patrick\nP\u00b4erez, and Matthieu Cord.\nSemantic palette:\nGuiding\nscene generation with class proportions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9342\u20139350, 2021. 9\n\n[44] Terri L. Lewis and Daphne Maurer. Multiple sensitive periods in human visual development: evidence from visually deprived children. *Developmental Psychobiology*, 46(3):\n163\u2013183, 2005. 2\n[45] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\nBLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation, 2022. arXiv:2201.12086 [cs]. 13\n[46] Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay\nRegularization, 2019. arXiv:1711.05101 [cs, math]. 7, 20\n[47] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher\nYu, Radu Timofte, and Luc Van Gool.\nRePaint: Inpainting using Denoising Diffusion Probabilistic Models, 2022. arXiv:2201.09865 [cs]. 3, 13\n[48] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and\nHang Zhao.\nLatent Consistency Models:\nSynthesizing\nHigh-Resolution Images with Few-Step Inference, 2023. arXiv:2310.04378 [cs]. 9\n[49] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt,\nDimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017. 2\n[50] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. *arXiv preprint arXiv:2108.01073*, 2021. 4, 13, 16\n[51] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. *arXiv preprint arXiv:2108.01073*, 2021. 4\n[52] John P Miller, Rohan Taori, Aditi Raghunathan, Shiori\nSagawa, Pang Wei Koh, Vaishaal Shankar, Percy Liang, Yair Carmon, and Ludwig Schmidt. Accuracy on the line: On the strong correlation between out-of-distribution and indistribution generalization. In International conference on machine learning, pages 7721\u20137735. PMLR, 2021. 1\n[53] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and\nDaniel Cohen-Or.\nNull-text inversion for editing real images using guided diffusion models.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 6038\u20136047, 2023. 3\n[54] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. *arXiv preprint arXiv:2302.08453*, 2023. 3\n[55] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models, 2022. arXiv:2112.10741 [cs]. 2\n[56] OpenAI. GPT-4 Technical Report, 2023. arXiv:2303.08774\n[cs]. 13\n[57] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, and others. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021. 2, 4\n\n[58] Ren\u00b4e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vision, pages 12179\u201312188, 2021. 6, 13\n[59] Suman Ravuri and Oriol Vinyals.\nClassification Accuracy Score for Conditional Generative Models, 2019. arXiv:1905.10887 [cs, stat]. 2\n[60] Sashank J Reddi, Satyen Kale, and Sanjiv Kumar.\nOn\nthe convergence of adam and beyond.\narXiv preprint\narXiv:1904.09237, 2019. 20\n[61] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022. 2, 13, 18\n[62] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022. 3\n[63] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\nnet: Convolutional networks for biomedical image segmentation. In International conference on medical image computing and computer-assisted intervention, pages 234\u2013241. Springer, 2015. 6, 13\n[64] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation.\nIn Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pages 22500\u2013 22510, 2023. 3\n[65] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and\nPercy Liang.\nDistributionally robust neural networks for\ngroup shifts: On the importance of regularization for worstcase generalization. *arXiv preprint arXiv:1911.08731*, 2019.\n14\n[66] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and\nPercy Liang.\nDistributionally robust neural networks for\ngroup shifts: On the importance of regularization for worstcase generalization. *arXiv preprint arXiv:1911.08731*, 2019. 1, 2, 4, 13\n[67] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi.\nPhotorealistic Text-to-\nImage Diffusion Models with Deep Language Understanding, 2022. arXiv:2205.11487 [cs]. 2\n[68] Mert Bulent Sariyildiz, Karteek Alahari, Diane Larlus, and\nYannis Kalantidis. Fake it till you make it: Learning transferable representations from synthetic ImageNet clones. In\nCVPR 2023\u2013IEEE/CVF conference on computer vision and\npattern recognition, 2023. 2\n[69] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5B: An open large-scale dataset for training next generation image-text models, 2022. arXiv:2210.08402 [cs]. 2\n[70] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics.\nIn International conference on machine learning, pages 2256\u20132265. PMLR, 2015.\n2\n[71] Jiaming\nSong,\nChenlin\nMeng,\nand\nStefano\nErmon.\nDenoising diffusion implicit models.\narXiv preprint\narXiv:2010.02502, 2020. 9, 14\n[72] Jiaming\nSong,\nChenlin\nMeng,\nand\nStefano\nErmon.\nDenoising diffusion implicit models.\narXiv preprint\narXiv:2010.02502, 2020. 5, 7, 20\n[73] Julian Straub, Thomas Whelan, Lingni Ma, Yufan Chen, Erik\nWijmans, Simon Green, Jakob J. Engel, Raul Mur-Artal, Carl Ren, Shobhit Verma, Anton Clarkson, Mingfei Yan, Brian Budge, Yajie Yan, Xiaqing Pan, June Yon, Yuyang Zou, Kimberly Leon, Nigel Carter, Jesus Briales, Tyler Gillingham, Elias Mueggler, Luis Pesqueira, Manolis Savva, Dhruv Batra, Hauke M. Strasdat, Renzo De Nardi, Michael Goesele, Steven Lovegrove, and Richard Newcombe. The Replica Dataset: A Digital Replica of Indoor Spaces. arXiv\npreprint arXiv:1906.05797, 2019. 4, 21\n[74] Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, and Ludwig Schmidt. Measuring robustness to natural distribution shifts in image classification. arXiv preprint arXiv:2007.00644, 2020. 1\n[75] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie.\nThe Caltech-UCSD Birds-200-\n2011 Dataset. California Institute of Technology, 2011. 4\n[76] Eric Wong and J Zico Kolter. Learning perturbation sets for\nrobust machine learning. *arXiv preprint arXiv:2007.08450*,\n2020. 2\n[77] Jianhao Yuan, Francesco Pinto, Adam Davies, Aarushi\nGupta, and Philip Torr.\nNot just pretty pictures: Textto-image generators enable interpretable interventions for robust representations.\narXiv preprint arXiv:2212.11237,\n2022. 2\n[78] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk\nChun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In Proceedings of the IEEE/CVF international conference on computer vision, pages 6023\u20136032, 2019. 2, 5\n[79] Amir Zamir, Alexander Sax, Teresa Yeo, O\u02d8guzhan Kar,\nNikhil Cheerla, Rohan Suri, Zhangjie Cao, Jitendra Malik, and Leonidas Guibas. Robust Learning Through Cross-Task Consistency. *arXiv preprint arXiv:2006.04096*, 2020. 4, 6, 20\n[80] Amir R Zamir, Alexander Sax, William Shen, Leonidas J\nGuibas, Jitendra Malik, and Silvio Savarese. Taskonomy: Disentangling task transfer learning. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition, pages 3712\u20133722, 2018. 1, 4, 6\n\n[81] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and\nDavid Lopez-Paz. mixup: Beyond empirical risk minimization. *arXiv preprint arXiv:1710.09412*, 2017. 2\n[82] Lvmin Zhang and Maneesh Agrawala. Adding conditional\ncontrol to text-to-image diffusion models.\narXiv preprint\narXiv:2302.05543, 2023. 2, 3\n[83] Lvmin Zhang and Maneesh Agrawala. Adding conditional\ncontrol to text-to-image diffusion models.\narXiv preprint\narXiv:2302.05543, 2023. 3\n[84] Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, and Aude Oliva. Learning Deep Features for Scene Recognition using Places Database.\nIn Advances in Neural Information Processing Systems. Curran Associates, Inc., 2014. 4\n"
    },
    {
        "level": "##",
        "title": "Controlled Training Data Generation With Diffusion Models Appendix 6. Outline",
        "content": "\nWe provide further discussions, details, and evaluations in the appendix, as outlined below. - Secs. 7 and 8 describe additional implementation details\nfor our classification and depth estimation experiments, respectively.\n- Sec. 7.6 describes an image guidance mechanism using\nTextual Inversion [22] and compares it with the CLIP guidance mechanism, on Waterbirds.\n- Sec. 8.2 provide additional results for \"standard\" augmentation baselines for the depth estimation experiments.\n- Secs. 7.4.3, 7.5.4 and 8.3 provide qualitative generations from all the Adversarial Prompts and Guided Adversarial Prompts used in the Waterbirds, iWildCam,\nand depth estimation experiments.\n- Additionally, for depth estimation, we provide a qualitative comparison of Adversarial Prompts generations optimized on different models (UNet [63], DPT [58]). For iWildCam, we also provide additional results using a ViT-B-16 [17] instead of a ResNet50 [26].\n- Sec. 8.4 provides additional analysis on the depth estimation experiments:\n- the single iteration vs. multi-iteration setting\n- a comparison of CLIP image and text guidance - an assessment of the generalization of Adversarial\nPrompts from one model to another.\n"
    },
    {
        "level": "##",
        "title": "7. Classification 7.1. Training Data Generation",
        "content": "\nInpainting.\n\nAs mentioned in main paper Sec. 3.1, for semantic classification tasks, we utilize the foreground object masks and use an in-painting technique proposed in [47] that preserves the masked region throughout the denoising process. In this section, we briefly describe this procedure and refer the reader to the original work for more details.\n\nLet m be a binary pixel mask, where a pixel is equal to 1\nif the pixel contains the object and 0 otherwise, and x be the original image from a training dataset. During generation, after obtaining a denoised sample \u02dcxt at time t we update it as \u02dcxt \u2190 m\u2299xorig t\n+(1\u2212m)\u2299\u02dcxt, where xorig t is the original image noised to have the correct properties of the expected Gaussian distribution at time t.\n\nHowever, because we are using Stable Diffusion [61], the denoising process is done in latent space (using an encoder E), not pixel space. This means that to apply inpainting, we must resize the mask m to the latent space dimensions, and apply the above-described procedure in the la-\n\ntent space: \u02dczt \u2190 mz \u2299 zorig\n                            t\n                                + (1 \u2212 mz) \u2299 \u02dczt, where\nzorig\n 0\n     = E(xorig) and zorig\n                       t\n                           is its corresponding noised ver-\nsion. While this procedure usually performs well in preserv-\ning the original region of interest, we also paste the original\nmasked region in the pixel space to obtain the final sample\n\u02dcx = m \u2299 xorig + (1 \u2212 m) \u2299 \u02dcx0.\n\nSDEdit [50].\n             In addition to inpainting, depending on the\nsetting, we also use SDEdit [50], a mechanism available\nto all diffusion models that allows to use an initial image\nto condition the generation of new images to be closer to\nthe initial image. The mechanism is parametrized by the\nSDEdit strength s, which indicates the extent by which the\nmodel can deviate from the original image.\n\nText-to-image model.\n                      For our diffusion model, we use\nStable Diffusion v1.5 2.\n"
    },
    {
        "level": "##",
        "title": "7.2. Alia",
        "content": "\nHere, we give more details on the ALIA [18] (Automated Language-guided Image Augmentation) baseline method, which aims at generating images targeting a particular test distribution similar to our guidance mechanism (main paper Sec. 3.3).\n\nGiven exemplar images from the test distribution, ALIA\nfirst captions each image using the BLIP [45] captioning model. Then, it uses the GPT-4 [56] LLM to summarize these captions into a list of domains asking it to produce descriptions that are agnostic to the class information. [18] then use these prompts to generate additional training data. In order to preserve the original class information in their generations, they use SDEdit [50] or Instruct Pix2Pix [7]. We refer the original paper for further implementation details. Below, we summarize resulting prompts we use for comparison in our results.\n\nFor Waterbirds [66], we found that removing the prefix \"a photo of a {*class name*}\" from the original prompts when using the inpainting technique (Sec. 7.1) to work slightly better for both the ALIA baseline and our CLIP text guidance (main paper Eq. (2)). We, therefore, use the following prompts: - \"in a bamboo forest with a green background.\" - \"flying over the water with a city skyline in the background.\"\n\n- \"perched on a car window.\" - \"standing in the snow in a forest.\", - \"standing on a tree stump in the woods.\"\n\n|                 |                |               | Dataset         | ALIA           | Ours          |\n|-----------------|----------------|---------------|-----------------|----------------|---------------|\n| SDEdit strength | sampling steps | text guidance | SDEdit strength | sampling steps | text guidance |\n| Waterbirds      | 0.3            | 50            | 7.0             | 1.             | 15            |\n| iWildCam        | 0.5            | 50            | 7.5             | 0.8            | 5             |\n|               | Dataset      | Training   |\n|---------------|--------------|------------|\n| learning rate | weight decay | epochs     |\n| Waterbirds    | 0.001        | 1e-4       |\n| iWildCam      | 0.0001       | 1e-4       |\n\n- \"swimming in a lake with mountains in the background.\",\n- \"standing on the beach looking up.\"\nFor iWildCam [3], we keep the original prompts intact:\n- \"a camera trap photo of a {class name} in a grassy field\nwith trees and bushes.\"\n- \"a camera trap photo of a {class name} in a forest in the\ndark.\"\n- \"a camera trap photo of a {class name} near a large body\nof water in the middle of a field.\"\n- \"a camera trap photo of a {class name} walking on a dirt\ntrail with twigs and branches.\"\nThere are two main differences between ALIA and our\nmethod:\n1. **The target distribution feedback**.\nALIA aligns its\nprompts with the target distribution by utilizing captioning and summarizing. However, this summarizing process is not informed of the produced generations when using such prompts, and, thus, does not guarantee that the text prompt will accurately guide the generation process to images related to the target distribution.\n2. **Model feedback.** ALIA is not model-informed. Thus, it\ndoesn't necessarily generate images *useful* for training a\ngiven model.\nThose two differences originate from the fact that ALIA\nis an **open-loop** method, i.e, it lacks the mechanism to refine the prompt based on the generated images. In contrast, our method uses model and target distribution feedback in a closed-loop. This allows our method to outperform ALIA\nand be more data-efficient.\n"
    },
    {
        "level": "##",
        "title": "7.3. General Implementation Details",
        "content": "\nGeneration.\n\nWe report our generation parameters in Tab. 3. We use the DDIM [71] scheduler. We generate 384x384 resolution images. Those parameters were chosen based on visual inspection, ease of optimization and downstream performance (validation accuracy). Training data.\n\nAfter generation, ALIA's method consists of an additional filtering step to remove \"bad\" generations. This step relies on using a pretrained model to measure confidence on the generated images. However, given our method creates images that are adversarial to an iWildCam pretrained model, the filtering part of ALIA's pipeline is not usable on our data. Thus, to keep things comparable, we decided not to apply filtering both our method generated data and ALIA's generated data. However, it must be noted that [18] only reports a 2% absolute accuracy drop between fitlering and no filtering on iWildCam (1.4% on Waterbirds), thus we do not expect a big difference in performance with ALIA's reported results and our results.\n\nSupervised Training.\n\nWe report our training parameters in Tab. 4. We use ALIA's codebase to finetune our models, which ensures fair comparison to the ALIA baselines. For everything except the generated data, the settings are the same as in ALIA. For both datasets, the starting model is a ResNet50 [25] model, pretrained on ImageNet [13].\n\nThe reported test accuracy is chosen according to the best checkpoint, measured by validation accuracy.\n"
    },
    {
        "level": "##",
        "title": "7.4. Waterbirds 7.4.1 Dataset Details",
        "content": "\nFig. 7 demonstrates the shift between train and test distributions in the Waterbirds dataset [65]. We follow the setting suggested in [18] and use 1139 images as Dtr, where waterbirds appear only on water background and landbirds on land background. We add additional 839 examples either from the original dataset, where waterbirds appear only on land background and landbirds on water background (\"Real OOD data\"), or generated by Stable Diffusion with prompts obtained by one of the methods. For the data-efficiency plots (e.g., Fig. 3) we reduced the number of added examples by a factor of {1/2, 1/4, 1/8, 1/16}.\n\nSince the original Waterbirds dataset does not provide masks for the exact generated images, we used the SAM [40] segmentation model to obtain bird segmentation masks for training images. We use these masks to condition the generative model on the class by using inpainting as described in Sec. 7.1.\n"
    },
    {
        "level": "##",
        "title": "7.4.2 Implementation Details",
        "content": "\nAdversarial Optimization.\n\nFor adversarial feedback, we use the model trained only using the original training data Dtrain with complete spurious correlation. It is taken from ALIA checkpoints3. As the task is the binary classification, we use the cross-entropy loss for the opposite class as the adversarial loss: Ladv(\u02dc*x, y*) = Lx\u2212ent(f(\u02dcx), 1 \u2212 y), assuming y *\u2208 {*0, 1}. This is equivalent to the negative crossentropy loss referred in the text. We find four prompts per each class, i.e., eight prompts in total. Each prompt is composed of five new learnable tokens. We perform adversarial optimization for 1000 steps with learning rate 1e-3 using Adam [38]. We use five denoising steps during adversarial optimization and generate images for training with 15 steps. We do not use SDEdit for Waterbirds. See Tab. 3 for summary.\n\nCLIP Guidance.\n\nFor Waterbirds, we use CLIP text guidance by encoding each of ALIA's summarized prompts (see Sec. 7.2) with the CLIP text encoder as described in main paper Sec. 3.3. In addition, we renormalize the averaged target text embedding to have the norm equal to the mean norm of the original prompts, and use the resulting vector as the target et. We use l2 guidance loss: Lt(Et(cw), et) =\n\u2225Et(cw)\u2212et\u22252\n2. We use \u03bbt = 20 and \u03bbi = 0 (i.e., no image guidance).\n\nIn Fig. 8 and Fig. 9, we show a few generations using all 8 prompts used in the Waterbirds experiments for Guided Adversarial Prompts and Adversarial Prompts, respectively.\n"
    },
    {
        "level": "##",
        "title": "7.5. Iwildcam 7.5.1 Dataset Details.",
        "content": "\nThe original iWildCam [3] dataset is subsampled to create a 7-way classification task (background, cattle, elephant, impala, zebra, giraffe, dik-dik).\n\nThe training set has 6,000 images with some classes having as few as 50 images per example.\n\nThere are 2 test locations that are not in the training or validation set. Additionally, given h, the hour at which an image was taken, we define an image to be during \"daytime\" if 9 \u2264 h \u2264 17, and\n\"nighttime\" if h \u2264 5 \u2228 h \u2265 20.\n\nAs said in main paper Sec. 4.1, for image CLIP guidance, we separate the target test locations into four groups (location={1,2}, time={daytime, nighttime}). We provide visualisation of the test locations (at day & night) in Fig. 10. For more details on the iWildCam subset construction, we refer to [18] Section 8.3. For inpainting, the object masks are obtained from MegaDetector [2].\n"
    },
    {
        "level": "##",
        "title": "7.5.2 Alignment Collapse Solution For Iwildcam.",
        "content": "\nAs mentioned in main paper Sec. 3.1, choosing Ladv to be the negative cross entropy loss, i.e. minimizing the probability that the model predicts y, may not be the best choice. Indeed, given we use a random sample of 64 images to create our target embedding for the image CLIP guidance, the likelihood that animals were present on these 64 images is very high.\n\nThis means that the target embedding, although mostly containing the \"location concept\", also partly contains an \"animal concept\".\n\nThis means that the image CLIP guidance does not explicitly forbid the generation of new animals. Combined with optimizing the negative cross entropy loss, this leads to adversarial animal insertions at generation time, where a new animal of class\n\u02c6y appears alongside the original animal of class y, destroying the (\u02dc*x, y*) alignment. In Fig. 11, we provide qualitative examples for this behaviour. To counter this behaviour, we choose Ladv to be the \"entropy\" loss, or uncertainty loss.\n\nMore precisely, this loss is equal to the cross entropy loss where the target label y is replaced by the soft label \u02dcy = [ 1\n|Y|, \u00b7 \u00b7 \u00b7 ,\n1\n|Y|], the uniform distribution over all classes. This loss explicitly encourages generations that either (1) do not contain new animals (2) contain new animals that are not accounted for in the label space Y.\n"
    },
    {
        "level": "##",
        "title": "7.5.3 Implementation Details",
        "content": "\nAdversarial optimization.\n\nWe describe here the parameters and settings used for optimization. If not precised, the same parameters were used for Adversarial Prompts and Guided Adversarial Prompts.\n\nAs said in main paper Sec. 4.1, we optimize 4 prompts. Each prompt is composed of 10 placeholder tokens.\n\nFor optimization, we use a constant learning rate of\n0.001, and a batch size of 8.\n\nWe use the \"entropy\" loss, described previously. For adversarial prompts, we train for 2000 steps. For guided adversarial prompts, we use CLIP guidance coefficient with\n\u03bbi = 10 and and \u03bbt = 0 (i.e., no text guidance). We train for a total of 10000 gradient steps. However, we don't optimize the adversarial loss for the first 2000 steps to allow the prompt to first converge to the target distribution region.\n\nFor adversarial prompts, to generate 4 different prompts, we simply change the seed.\n\nFor guided (adversarial)\nprompts, each prompt is w.r.t a new location & time of the day of the test distribution.\n\nTraining data.\n\nThe generation settings are the same as the ones used during adversarial optimization. For each target domain-guided adversarial prompt, (i.e. location & time of the day), the source images (used to condition the generation with an object mask and through SDEdit [50]) are only images that match the time of the day of the target domain used during generation. Furthermore, for each prompt, we only generate one image per source image.\n\nFor ALIA, for each prompt, we generate one image per source image, from the whole training dataset. For the generation settings, given we use a slightly different generation process (inpainting) compared to their original implementation, we search ALIA's best-performing generation parameters (according to validation accuracy) over SDEDit strength [0.4, 0.5, 0.8] and guidance scale [5.0, 7.5]. We found the best-performing parameters for ALIA to be the same as the one reported by ALIA in their Github4 i.e.\n\nSDEdit strength of 0.5 and guidance of 7.5.\n\nFinetuning.\n\nThe learning rate scheduler is a cosine scheduler, updated every epoch. The batch size is 128.\n\nOur iWildCam pretrained model is taken from ALIA\ncheckpoints5. ALIA trains the model from \"scratch\" (i.e.\n\nthe model has never seen iWildCam data), for 100 epochs, on the combination of real + generated data.\n\nFor our method, given we optimize the prompts based on a finetuned model feedback, it may not make as much sense to train the model from \"scratch\". Thus, we also introduce the variant where the iWildCam pretrained model is finetuned on the combination of real + generated data for 20 epochs, where finetuning means that every layer, except the last, is frozen.\n\nFor a fair comparison, both training settings are tested for ALIA and our method. We found that ALIA worked best when training from scratch and our method worked best when using the finetuning setting.\n\nFinally, in their iWildCam experiment, ALIA fixed the number of extra generated points to be used in combination with real data during training to 2224 images. For the sake of comparison, we adopt the same limit in our experiments, with the added variant where the limit is 556 images, showcasing the data efficiency of our method.\n\nIn Fig. 12 and Fig. 13, we show a few generations using each of the 4 Guided Adversarial Prompts and Adversarial Prompts used in the iWildCam experiments.\n"
    },
    {
        "level": "##",
        "title": "7.5.5 Using Vit Model.",
        "content": "\nIn Fig. 14, we repeat the iWildCam experiment from the main paper (Fig. 4) with a ViT-B-16 [16] model. Additionally, we provide qualitative results for generations from adversarial prompts optimized with a ViT-B-16 model in Fig. 15.\n"
    },
    {
        "level": "##",
        "title": "7.6. Image Guidance Using Textual Inversion",
        "content": "\nIn addition to the CLIP image guidance introduced in main paper Sec. 3.3, we also explore using Textual Inversion (TI) [22] as an image guidance mechanism. Similar to the CLIP guidance, we use a few images {xj} from the target distribution. Now, instead of the similarity in a CLIP embedding space, we use the denoising loss between a generated image and one of the target images as in [22] (see Eq. (2)):\n\n$$\\mathcal{L}_{\\mathrm{TI}}(c_{w})=\\mathbb{E}_{x_{j},z\\sim\\mathcal{E}(x_{j}),\\epsilon\\sim\\mathcal{N}(0,I),t\\sim U(0,1)}\\tag{4}$$ $$\\left[\\|\\epsilon-\\epsilon_{\\theta}(z_{t},t,c_{w})\\|_{2}^{2}\\right],\\tag{5}$$\nwhere E is the VAE [39] image encoder and \u03f5\u03b8 is the denoising UNet model from the Stable Diffusion model [61], and xj is sampled randomly from the set of available target images.\n\nWe test the TI image guidance on the Waterbirds dataset.\n\nWe use the guidance loss from Eq. (4) with the weight 1000 (we found lower values to result in generations less faithful to the target images) and randomly sample 50 (unlabeled) images from the validation split of the original Waterbirds dataset where both types of birds appear on both types of backgrounds. We keep other settings the same as for GAP and AP.\n\nFig. 16 shows that TI guidance works on par with or better than CLIP guidance on the Waterbirds dataset. We found, however, that the TI guidance does not result in faithful generations for iWildsCam dataset, and further investigations are needed.\n"
    },
    {
        "level": "##",
        "title": "7.7. Additional Analysis",
        "content": "\nWe ablate hyperparameters like 1) using \u21132 or cosine loss for LCLIP, 2) different ways of incorporating guidance e.g., text or image with CLIP or textual inversion (T.I).\n\nThe table shows the accuracy from different combinations of 1 and 2 on the Waterbirds dataset. Using \u21132 and text guidance worked best, thus, we used this setting for our results in Fig. 3. T.I. compares generations to the original images in the pixel space, and Image in the CLIP embedding space, resulting in different guidance mechanisms and, hence, performance.\n\nance when used with adversarial optimization (TI GAP). However, using only TI guidance (TI GP) results in worse performance than using only text guidance with prompts found by the ALIA method [18].\n\nLoss\\Guide.\nText\nImage\nT.I.\n\u21132\n84.0\n77.3\n83.6\nCosine sim.\n82.5\n72.7\n\u2014\n"
    },
    {
        "level": "##",
        "title": "8. Depth Estimation 8.1. Depth Training Details",
        "content": "\nAdversarial Optimization. The adversarial optimization was done with AdamW [46], learning rate of 5.0 \u00d7 10\u22124, weight decay of 1.0 \u00d7 10\u22123, and batch size of 8. The token embeddings at the start of optimization are randomly sampled from N(\u00b5emb, \u03c3emb) where \u00b5emb and \u03c3emb is the mean and standard deviation of all embeddings in the vocabulary. We set the early stopping threshold to 0.08 for the UNet model and 1.0 for the DPT model. Note that these models were trained with different losses, \u21131 for the former and Midas loss [20**] for the later.** Adversarial optimization is performed with the same loss as was used for training these models. One run takes approximately 30 mins on one A100. We perform a total of 32 runs, to get 32\nAdversarial Prompts for the UNet model and 30 runs for the DPT model. As the DPT model was trained on Omnidata, which is a mix of 5 datasets, we have 6 runs for each dataset. Different number of placeholder tokens were also used for each run as suggested in Fig. 6b of the main paper. For the DPT model, we do 1, 8, 16 tokens runs for each dataset and also 3 runs with 32 tokens for each dataset. For the UNet model, 4 runs of 1, 8 and 16 tokens each and 16 runs of 32 tokens were used, to get a total of 32 prompts. We also use a reduced number of denoising steps during optimization i.e., 5, as we found it to be more stable.\n\nGuided Adversarial Optimization. The CLIP guidance coefficient for text and image guidance is set to 1 and 5 respectively. For image guidance, we randomly sampled 100 images from the target distribution. For text guidance, we used target distribution's name in the prompt, e.g., \"fog\" for the fog corruption from CC.\n\nGeneration.\n\nGeneration is performed with the DDIM [72] scheduler and 15 sampling steps. We generate 80k images for the UNet model and 60k images for the DPT model for fine-tuning.\n\nFine-tuning.\n\nFor fine-tuning, we optimize the UNet model with AMSGrad [60] with a learning rate of 5.0 \u00d7\n10\u22124, weight decay of 2.0 \u00d7 10\u22126 and batch size 128. For the DPT model, a learning rate of 1.0\u00d710\u22125, weight decay of 2.0 \u00d7 10\u22126 and batch size 32.\n\nPerformance of non-SD baselines. In Tab. 6, we show the results for depth estimation for two additional baselines, deep augmentation [31] and style augmentation [23] that do not make use of generative models. Deep augmentation distorts a given image by passing it through an image-to-image model e.g., VAE [39], while perturbing its representations. Style augmentation involves involves applying style transfer to the original training images. They perform comparably to Adversarial Prompts.\n\nGenerations from all adversarial prompts & comparison of generations from different models. We show the generations from all Adversarial Prompts from the UNet model, without SDEDit (Fig. 19), with SDEdit (Fig. 20), and multi-iteration (Fig. 21). Additionally, we provide the generations from two DPT models, allowing us to assess the difference the model feedback has on generations. The first DPT model was only trained on Omnidata (Fig. 22) and second was trained on Omnidata with augmentations from CC and 3DCC and with consistency constrains [79] (Fig. 23). The quantitative results in the paper were reported only on the former DPT model.\n\nThere does not seem to be obvious differences in the styles generated between the two DPT models. However,\n\n|                         | U-Net    | DPT       |\n|-------------------------|----------|-----------|\n| Taskonomy               | Replica  | Taskonomy |\n| Shift                   | Clean    | CC        |\n| Control (No extra data) | 2.35     | 4.93      |\n| Agnostic Prompts        | 2.47     | 5.03      |\n| Agnostic Prompts        | (Random) |           |\n| 2.38                    | 4.96     | 4.11      |\n| Adversarial Prompts     | 2.49     | 4.36      |\n| Adversarial Prompts     | (SDEdit) | 2.59      |\n| Deep Augmentation       | 2.42     | 4.24      |\n| Style Augmentation      | 2.42     | 4.15      |\n\n(3DCC) and cross-datasets (CDS), Replica [73]. The results from CC and 3DCC are averaged over all distortions and severity levels on Taskonomy. Our method is able to generate training data that can improve results over the baselines on several distribution shifts. Generations with AP (SDEdit) gives better results than AP under distribution shifts. Thus, also conditioning on the original image seems to be helpful for these shifts. For the DPT model, the trends are similar, AP performs better than the baselines. Deep augmentation and style augmentation do not make use of a generative model for generating extra data. They perform comparably to AP.\n\nbetween the Adversarial Prompts from the UNet model with and without multi-iteration, the Adversarial Prompts from the latter seems to result in much more diverse styles.\n"
    },
    {
        "level": "##",
        "title": "8.4. Additional Analysis 8.4.1 Running Multiple Iterations Of Adversarial Optimization Vs A Single Iteration",
        "content": "\nHere, we provide additional analysis for the multi-iteration experiments in Fig. 6b in the main paper. We optimize for 4 prompts in each iteration and noticed that if the number of placeholder tokens in a given prompt is kept fixed throughout the iterations, the optimization to find new Adversarial Prompts becomes more difficult. However, if we increase the number of tokens at each iteration e.g., 1 token per prompt for 1st, 8 per prompt for 2nd, etc, we are able to consistently find new Adversarial Prompts. Thus, we aim to investigate the generalization of a given model to different Adversarial Prompts, e.g., is a model more likely to generalize to Adversarial Prompts with the same number of tokens.\n\nTo perform this analysis, we generated data Dn, Dm using AP with n and m tokens per prompt respectively and measured the performance of a model fine-tuned on Dn on Dm.\n\nResults for generalization to the same number of tokens.\n\nIn this setting, n = m, we use n = m *\u2208 {*1, 8, 16, 32}.\n\nFor every n, we construct Dn and Dm to be generated using\n4 Adversarial Prompts. We fine-tune the model on Dn and and validate both on Dn and Dm validation sets during the fine-tuning. The results are shown in Fig. 17.\n\nResults for generalization to different number of tokens.\n\nIn this setting,\n(*n, m*)\nare\n(1, 8), (8, 1), (8, 16), (16, 8), (16, 32), (32, 16) respectively.\n\nFor every n we fine-tune on Dn and compute the validation loss on Dn and Dm. The results are shown in Fig. 17. As the loss for n = m tends to be more similar then when n \u0338= m, we chose to increase the number of tokens used per prompt in our multi-iteration setting.\n"
    },
    {
        "level": "##",
        "title": "8.4.2 Clip Image Vs Text Guidance.",
        "content": "\nIn Fig. 18, we compare the qualitative (top) and quantitative (bottom) differences in generations from text guidance and image guidance on defocus blur and fog. Note that image guidance uses sample (unlabelled) images from the corresponding target distribution that it is evaluated on, i.e., fog samples images from the fog corruption from the CC benchmark and fog (3D) samples images from the fog corruption of the 3DCC benchmark. If the target distribution name has (3D) appended to it, it is from the CC benchmark, otherwise it is from the 3DCC benchmark.\n\nWe observed some differences in the generations with text vs.\n\nimage guidance (Fig. 18, top).\n\nText Guided Prompts generates corruptions that are more realistic that image Guided Prompts.\n\nFor example, fog gets denser further away from the camera or around the floor when text Guided Prompts are used for generations. For image Guided Prompts, as it was guided by the image samples from CC where the corruption is applied uniformly over the image, it learns to also apply a more uniform corruption over the image.\n\nQuantitatively, we observed that image guidance tends to perform the best across the target distributions, with large enough extra data (Fig. 18, bottom).\n"
    },
    {
        "level": "##",
        "title": "8.4.3 Generalization Of Adversarial Prompts To Different Models.",
        "content": "\nWe show how adversarial generations from Adversarial Prompts found for one model are for another model in Tab. 7. The generations from Adversarial Prompts found for e.g., the UNet model result in the highest loss when evaluated on the UNet model. However, the generations from Adversarial Prompts from the DPT model also result in similar loss. Similar trends hold for the DPT model. Thus, Adversarial Prompts found for one model are also able to result in high loss for another model.\n\nAP from\\Eval on\nOriginal data\nUNet\nDPT\nUNet\n2.55\n7.63\n5.39\nDPT\n1.76\n7.17\n6.46\n"
    }
]