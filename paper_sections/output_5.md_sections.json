[
    {
        "level": "##",
        "title": "Deep Active Learning: A Reality Check",
        "content": "\nEdrina Gashi\nIndependent Researcher\nJiankang Deng\nHuawei Noah's Ark\nIsmail Elezi\nHuawei Noah's Ark\n"
    },
    {
        "level": "##",
        "title": "Abstract",
        "content": "\nWe conduct a comprehensive evaluation of state-of-theart deep active learning methods. Surprisingly, under general settings, no single-model method decisively outperforms entropy-based active learning, and some even fall short of random sampling. We delve into overlooked aspects like starting budget, budget step, and pretraining's impact, revealing their significance in achieving superior results. Additionally, we extend our evaluation to other tasks, exploring the active learning effectiveness in combination with semi-supervised learning, and object detection. Our experiments provide valuable insights and concrete recommendations for future active learning studies. By uncovering the limitations of current methods and understanding the impact of different experimental settings, we aim to inspire more efficient training of deep learning models in real-world scenarios with limited annotation budgets. This work contributes to advancing active learning's efficacy in deep learning and empowers researchers to make informed decisions when applying active learning to their tasks.\n"
    },
    {
        "level": "##",
        "title": "1. Introduction",
        "content": "\nAll animals samples are equal, but some are more equal than others. [35]\nDeep Learning models have achieved a remarkable feat by attaining near-human accuracy in a multitude of tasks. This prowess is attributed to their ability to harness extensive datasets, enabling them to conquer challenges like image classification [15], object detection [37], and image segmentation [6]. These accomplishments primarily reside within the domain of supervised learning, where the models rely on vast repositories of labeled data such as ImageNet [40] or MS-COCO [28]. While sourcing image data might seem straightforward, the process of meticulously annotating them is a laborious, time-consuming, and error-prone task.\n\nThis challenge becomes even more pronounced in specialized fields like medical imaging, or forensics, where data annotation necessitates expert skills and can lead to significant errors.\n\nRecent empirical investigations [19, 31] have unveiled an intriguing aspect of deep neural models' performance \u2013\nit is not yet saturated concerning the size of the training data. The adage \"more data, more accuracy\" holds true, but it comes at a cost. This realization has propelled researchers to explore a semi-supervised approach [32, 34], which seeks to strike a balance between labeled and unlabeled data. However, despite these efforts, the performance of semi-supervised learning models still lags behind their fully-supervised counterparts [36]. Regardless of whether a learning scenario is fully-supervised or semi-supervised, a universal constraint persists: the annotation budget is finite. Hence, the imperative to adopt an intelligent labeling strategy - often referred to as *active learning* - becomes evident. This approach involves selectively labeling the most informative samples, a technique that has been shown to significantly enhance the performance of both fully-supervised [2,46,52] and semi-supervised [11] models.\n\nThe landscape of research in this domain is marked by a surge of recent papers proposing novel methods, each vying to establish its state-of-the-art credentials. However, a common theme emerges - these proposed methods often carry certain shortcomings. Some employ testing sets for validation, others make methodological testing errors, and unfair comparisons are not uncommon. Though such issues are not unique to active learning, analogous subfields have witnessed the emergence of works aimed at rectifying these pitfalls. For instance, in metric learning, [33] conducted an exhaustive study of various cutting-edge algorithms, revealing that the latest methods only marginally outperform classical techniques like contrastive and triplet loss in most scenarios. This scrutiny has catalyzed more meticulous research and rigorous evaluations, thereby fostering advancements in the field.\n\nThe present study embarks on a comprehensive exploration of several renowned deep active learning methods. In the experimental design, we subject these methods to uniform conditions across multiple data splits and datasets, thereby ensuring a fair comparison. Intriguingly, our findings challenge the supremacy of claimed state-of-the-art techniques. Contrary to expectations, the results reveal that, in a general setting, none of the proposed methods decisively outperform active learning based on entropy. Furthermore, some of these methods fail to consistently surpass the performance of even random sampling. This research delves further by investigating overlooked parameters such as the starting budget, budget step, and the impact of pretraining on the methods' efficacy.\n\nTo comprehensively assess the landscape, we also examine the benefits of active learning in the semi-supervised learning setting, showing that a combining these two techniques leads to better results than either of them in isolation. We then extrapolate these findings to another pertinent task - object detection. We finish this study by offering valuable insights and recommendations for future active learning endeavors, drawing parallels with the evolution of research in related subfields.\n\nOur **contribution** is the following:\n\n- **Thorough Evaluation of Active Learning Methods**:\nWe conduct an exhaustive and unbiased assessment of various cutting-edge active learning techniques. The results unveil a significant insight: in a general context, no single-model approach can outperform the *entropy*-\nbased strategy.\n- **Expanded Experimental Scope**: Our investigation\nextends beyond the mainstream by examining the behaviors of active learning methods across varying starting budgets, budget step sizes, and their performance in conjunction with pre-trained models. This broader exploration offers a more comprehensive understanding of the methods' dynamics.\n- **Other modalities**: We investigate the interplay between active learning and semi-supervised learning, shedding light on their synergistic potential. Furthermore, we extend our study to the realm of object detection, showcasing the applicability of the methods in different contexts.\n- **Guidelines for Future Work**: Our research does not\nconclude with results; we distill our findings into actionable recommendations. These insights provide a roadmap for upcoming research endeavors in the domain of active learning, aiding researchers in charting a more effective course of exploration.\nIn essence, our work significantly enriches the understanding of active learning's intricacies and its potential across various dimensions, ensuring that future advancements in this field are better informed and more impactful.\n"
    },
    {
        "level": "##",
        "title": "2. Related Work",
        "content": "\nActive learning (AL) has garnered extensive attention over the past two decades. It centers on selecting uncertain samples for classifier prediction or those where independent classifiers disagree. A comprehensive survey [42] aptly delves into this issue, particularly within the context of low-level data.\n\nThe survey covers a spectrum of active learning approaches, encompassing uncertainty-based [26, 27, 30, 38, 43], SVM-based [49, 51], and query-bycommittee methods [17,44].\n\nDeep Active Learning (DAL) has garnered substantial attention in recent years, resulting in a multitude of approaches addressing the problem from diverse angles. One notable strategy, presented in [2], involves training an ensemble of neural networks followed by the selection of samples exhibiting the highest acquisition scores. These scores are determined through acquisition functions such as entropy [45] or BALD [16].\n\nConcurrent research [10, 22]\ndelves into similar territory, approximating uncertainty by leveraging Monte-Carlo dropout [9]. A comparative analysis performed in [2] sheds light on these approaches, resolutely concluding that the ensemble-based methodology yields superior results albeit with increased computational demands. An alternate Bayesian approach is presented by [50], wherein data augmentation is combined with Bayesian networks. The authors employ a variational autoencoder (VAE) [21] on real and augmented samples, selecting unlabeled samples based on the VAE's reconstruction error. An analogous strategy is outlined in [46], wherein a latent space is learned using a VAE in tandem with an adversarial network trained to distinguish between labeled and unlabeled data. The VAE and adversarial network engage in a minimax game, with the former attempting to deceive the latter into classifying all data points as labeled, while the adversarial network refines its ability to discern dissimilarities within the latent space. A divergent approach is presented in [41], wherein the active learning challenge takes the form of core-set selection. This entails the identification of a subset of points such that a model trained on this subset remains competitive when applied to the broader dataset. Another unique perspective is offered by the work of [52]. Here, the authors present a heuristic yet elegant solution: a network is trained for classification while concurrently predicting cross-entropy loss. During the sample acquisition phase, samples with the highest prediction loss are deemed the most intriguing and subsequently earmarked for labeling.\n\nDeep Semi-Supervised Actove Learning (DSSL). constitutes a profound approach in deep learning that merges a limited set of labeled data with a substantial pool of unlabeled data during neural network training. This contrasts with active learning (AL), where the utilization of unlabeled data is typically restricted to the acquisition phase. Within semi-supervised learning (SSL), these unlabeled data play a role throughout the training process. Several methods have demonstrated exceptional outcomes [24,25,32,48] by framing semi-supervised learning as a regularization challenge.\n\nThis involves introducing an additional loss component for the unlabeled samples, effectively enhancing the learning process. Subsequent endeavors have significantly advanced SSL's performance in object classification [3\u20135,34,47,54]. These efforts have contributed to refining the SSL paradigm and achieving remarkable results across diverse applications.\n\nInterestingly, despite the apparent overlap between active learning and semi-supervised learning, the fusion of these methodologies has been a relatively unexplored territory. A pioneering attempt to unify these concepts surfaced in [11]. This work employs a consistency-based semisupervised learning technique during training, establishing a connection between these two pivotal domains. The work was later extended in the domain of object detection [8] giving similar conclusions.\n"
    },
    {
        "level": "##",
        "title": "3. Representative Active Learning Works",
        "content": "\nNotation: Let D be a dataset divided into a labeled set L\nand a pool of unlabeled data U. Each sample in the dataset belongs to a class y, and in total there are c classes. The Active Learning acquisition function consists of mining a subset of samples from the pool of unlabeled data U and transferring them to the labeled set L, incurring a labeling cost. For a sample x (e.g., an image), a neural network \u03b8\ngenerates a feature vector f and a softmax probability distribution pi, where p represents the likelihood of the sample belonging to class i. We define the labeling budget as b and typically *b <<* |L|. The training procedure is done for n active learning cycles, and in each cycle, we label *b/n* the most promising samples.\n\nActive Learning methods: In our framework, we consider the following representative active learning works: random, entropy, variational ratio, Bayesian Active Learning (BALD), Core-Set, and Learning Loss for Active Learning (LLAL).\n\nRandom a(x) = unif() with unif() is a function returning a draw from a uniform distribution over the interval [0, 1]. Using this acquisition function is equivalent to choosing a point uniformly at random from the pool. It is the default baseline in active learning.\n\nEntropy is an active learning method, where the classifier trained in this current iteration computes the softmax predictions of the unlabeled samples, and we choose to label the samples with the highest entropy (on the softmax predictions). The entropy of a sample is computed as:\n\n$$H(x)=\\sum_{i=1}^{c}p_{i}log(p_{i}).\\tag{1}$$\nVariation Ratio describes the lack of confidence of a classifier. It is computed as:\n\n$\\pi(x)=1-max(p|x)$.\n\nBALD scores a data point based on how well the model's predictions inform us about the model parameters \u03b8. For this, it computes the mutual information I(y|\u03b8). The formula is given as:\n\n$$B(x)=H(y|x)-\\mathbb{E}_{p(\\theta)}[H[y|x,\\theta]]=\\mathbb{I}(y,\\theta|x).\\tag{3}$$\nwhere E represents the expectation.\n\nLLAL divides a minibatch of size B into B/2 data pairs\n(xj, xk). Then, it learns the loss prediction module by considering the difference between a pair of loss predictions, which completely makes the loss prediction module discard the overall scale changes. For each pair, the loss function of the loss prediction module is defined as:\n\n$$L_{loss}(\\hat{l},l)=max(0,1(l_{k},l_{j})\\cdot(\\hat{l}_{k}-\\hat{l}_{j})+\\xi)\\tag{4}$$\nwhere \u03be is a pre-defined positive margin, and 1 is an indicator variable that takes values +1 if li *> l*j, and \u22121 otherwise. Then, during inference, only the learned loss Lloss is predicted, with the labeled samples being those with the highest L*loss*.\n\nCore-Set divides the labeled pool into k clusters, in such a way as to maximize the spread of the labeled data. It uses a complex optimization procedure based on Gurobi [13] optimizer.\n\nAll the mentioned methods, except Core-set, compute the acquisition function and then choose to label the b/n samples with the highest acquisition score. For the most part, these methods are also known as *uncertainty* methods. In the case of Core-set, it chooses the *b/n* samples that maximize the representation of the data, and it is known as a *diversity* method.\n"
    },
    {
        "level": "##",
        "title": "3.1. Flaws In The Methodology",
        "content": "\nActive Learning research is not very standardized. Different methods use different backbones and typically minimal comparison with other methods. Furthermore, most of the methods do not use a validation set, and perform experiments in only one or two datasets, usually in simple ones like CIFAR-10. Additionally, most methods show experiments in a very limited setting (e.g., very few AL cycles). LLAL [52] in particular improves the backbone to be higher-performing for low-resolution images, and achieves better results than the other methods. However, while we were able to reproduce their results, the results of entropybased AL were higher than those reported in the paper, and\n\n|                 | CIF10   | CIF100   | CAL101   | CAL256   |\n|-----------------|---------|----------|----------|----------|\n| dataset size    | 50000   | 50000    | 6084     | 30607    |\n| initial labeled | 1000    | 1000     | 1000     | 5000     |\n| num cycles      | 20      | 20       | 5        | 10       |\n| label cycle     | 1000    | 1000     | 1000     | 1000     |\n| num epochs      | 200     | 200      | 200      | 200      |\n| optimizer       | SGD     | SGD      | SGD      | SGD      |\n| learning rate   | 0.1     | 0.1      | 0.1      | 0.1      |\n| momentum        |         |          |          |          |\n| 0.9             | 0.9     | 0.9      | 0.9      |          |\n| scheduler       | step    | step     | step     | step     |\n| scheduler step  | 160     | 160      | 160      | 160      |\n| weight decay    | 5e-4    | 5e-4     | 5e-4     | 5e-4     |\n\nactually similar to those of LLAL. Other methods like Variational Adversarial Active Learning (VAAL) [46], as can be seen from the official code, by mistake, report the results in the training set. Our experiments in the method show that it does not outperform random acquisition. The Power of Ensembles method [2], while reaching very high results, comes with a significantly higher computational cost. It also is unclear if the improvement comes from ensembles improving AL, or from the higher computational cost.\n\nIn this work, we strive for simplicity. We train all methods under the same hyperparameter configuration, seed, backbone, and training tricks. We also use a larger number of datasets, being diverse in their complexity and image resolution. Furthermore, we apply a larger number of AL cycles, to have more complete results.\n"
    },
    {
        "level": "##",
        "title": "4. Experiments 4.1. Experimental Setup",
        "content": "\nWe perform our experiments in four standard classification benchmarks: CIFAR-10, CIFAR-100 [23], Caltech- 101, and Caltech-256 [12]. We keep a unified setting, where we keep the same hyperparameters for all datasets. Changing the hyperparameters for every dataset, while it can come with a slight improvement, is unrealistic in the active learning setup. Based on the dataset size, we do a different number of active learning cycles. We add 1000 images for labeling in each cycle. We train each network from scratch at every active learning cycle. We report the mean and standard deviation based on the training of five trials. For each method, we use the same initial split and random seed. We give the main hyperparameters for each dataset in Table 1. We also provide the exact numbers (mean and standard deviation) in the supplementary.\n"
    },
    {
        "level": "##",
        "title": "4.2. Main Result: Comparisons",
        "content": "\nWe show the results of our experiments performed in CIFAR-10 in Figure 1a. All methods start at roughly the same point, subject to network fluctuations. We show that immediately in the second AL cycle, the entropy and LLAL acquisition functions outperform the random baseline by 1.4 percentage points (pp), respectively 1.1pp. In the third cycle, both methods outperform the random baseline by over 2.5pp. Both methods continue outperforming random by circa 3pp for the remaining of the training, with entropy having a slight advantage over LLAL, outperforming LLAL by around 1pp in the last 10 AL cycles. Interestingly, the other methods need more AL cycles to start outperforming the random baseline. For example, the random baseline outperforms BALD and Variation Ratio in the first four AL cycles, and Core-set in the first five AL cycles. After that, Core-set and BALD consistently outperform the random baseline, but always track behind LLAL and entropy, in the case of the latter, by up to 3pp. However, the Variation Ratio, after a couple of steps where it slightly outperforms the random baseline, quickly converges to the same performance and usually lags behind the entropy by 2.5 \u2212 3pp.\n\nWe see a more interesting picture when we train in the more difficult CIFAR-100, and show its results in Figure 1b. In the first few steps, the random baseline outperforms all the acquisition functions except the Core-set. In fact, the entropy starts clearly outperforming the random baseline only after the 12th AL step, LLAL starts outperforming the random baseline only after the 14th AL step, while the performance of BALD and Variation Ratio typically is only as good as that of the random baseline. Core-set starts performing better than all the other methods, outperforming the random baseline by 1pp and entropy by 2pp in the first step. In the second step, it outperforms the random baseline by 1.5pp and entropy by almost 3pp. It continues outperforming entropy until the 13th step, after which it consistently is outperformed by entropy, finishing in the 20th step by almost 2pp worse than the entropy.\n\nWe continue our experiments in the Caltech-101 dataset, which contains images with a significantly larger resolution. We show the results in Figure 1c. In the second AL step, entropy already outperforms the random baseline by close to 5.5pp. Core-set outperforms the random baseline by 2.5pp while the other three methods reach only as good results as the random baseline. Then, on the next cycle, the entropy and Core-set outperform the random baseline by around 9pp and respectively 6pp. At this stage, the other three methods start outperforming the random baseline, in the case of LLAL by circa 2.5pp with the other two methods by around\n1 \u2212 1.5pp. At the last cycle, entropy outperforms random by over 7pp, Core-set outperforms random by around 4pp, and LLAL outperforms random by 2.2pp, with BALD and Variation Ratio improving over the baseline by around 1pp.\n\nWe now show the results in the most challenging dataset, Caltech-256 in Figure 1d. Because of the complexity of the dataset, all the methods perform around the same in the first few AL steps. Interestingly, the random baseline outperforms all the other methods in the fourth and fifth steps. After that, the entropy takes the lead and finishes the training outperforming the random by over 4pp. BALD, Coreset and Variation Ratio finish strongly, outperforming the random baseline by around 2.5pp. LLAL never manages to outperform the random baseline and finishes the training getting outperformed by random by almost 1pp.\n\nRecommendation 1: Entropy is all you need.\n\nTo our biggest surprise, despite the rapid research in the field of active learning, entropy is arguably still the best active learning method.\n\nWhile in specific scenarios, some other methods might outperform entropy, in general, entropy reaches at least competitive performances and more often than not, outperforms the other methods. Some methods such as LLAL perform nearly as well as the entropy in the dataset they were developed. However, in more challenging datasets like CIFAR-100 and Caltech-256, they fail to show much, if any, improvement over the random baseline. The Core-set, in general, shows the second-best performance after the entropy and usually performs highly in challenging datasets. BALD and Variation Ratio are shown to be very dataset-specific, and in general, tend to perform much worse than the entropy. We recommend that the practitioners use the entropy acquisition function before going to more complicated solutions. More often than not, this acquisition function is the best they can get, and it tends to perform well in most if not all, settings.\n"
    },
    {
        "level": "##",
        "title": "4.3. Ablation Study",
        "content": "\nWe do a series of ablation studies, evaluating some of the design choices in active learning. We do the experiments in the CIFAR-10 dataset, using the best-found acquisition function, the entropy.\n"
    },
    {
        "level": "##",
        "title": "4.3.1 The Budget Of Each Cycle",
        "content": "\nMost AL research papers arbitrarily choose the number of samples labeled in every cycle. In this ablation study, we show the performance when the labeling budget for each cycle is small (500 samples), medium (1000 samples) and large (2000 samples). We present the results in Figure 2a.\n\nAs shown, until we label 4000 samples, the medium case works significantly better than the large and especially small cases. In particular, where we have 2000 and 3000 labeled samples, the medium case outperforms the small case by\n3.5 \u2212 4pp. After that, the performance difference between the medium and small/large diminishes, albeit the medium case shows a slightly better performance in almost all AL cycles. At the end of the training, with 20000 labeled samples, medium outperforms the other two cases by around half a percentage point.\n\nRecommendation 2: Use a medium-sized budget for each AL cycle.\n\nWhile using small-sized budgets might sound tempting, it does not perform very well in practice. This is because by choosing only the very hard samples, the network gets biased towards hard samples, thus, not performing well in the not-hard samples. This can be especially seen in the first few AL cycles. Similarly, using large budgets is equivalent to choosing a large number of easy, thus not very informative samples. We recommend that the practitioners use medium-sized acquisition budgets for each AL cycle.\n"
    },
    {
        "level": "##",
        "title": "4.3.2 To Train Vs. To Finetune",
        "content": "\nSome AL methods, in each AL cycle, continue training the network from the previous AL cycle. The intuition behind this is that the network has already shown some good performance, and by adding more data and continuing training, the performance of the network will improve. On the other hand, some other works train every network from scratch. The motivation behind it is that the networks trained on a small amount of data are potentially stuck in local minima. While adding more data will improve their performance, they might still struggle to escape the local minima, thus, it is better to train them from scratch. We perform a full training loop, in one case training the network from *scratch*, and in one training the network from the previous cycle, thus finetuning it. We perform the experiments in CIFAR-10, using the entropy acquisition function. We present the results in Figure 2b.\n\nAs we can clearly see, the networks trained from scratch tend to outperform the finetuned network. In the second and third AL cycles, the networks trained from scratch outperform the finetuned networks by an average of 4pp. The performance gain by training from scratch is lower in the next few cycles, until it completely diminishes, with both the methods performing the same.\n\nRecommendation 3: Train networks from scratch.\n\nWe recommend the practitioners reinitialize the network at each AL cycle. This is important, especially in the first few AL cycles where the number of data is smaller. In later cycles, it does not much difference if the network is trained from scratch or it is finetuned.\n"
    },
    {
        "level": "##",
        "title": "4.3.3 The Question Of Diversity",
        "content": "\nMost of the methods we used in this work do not consider the diversity between the selected samples (except the Core-set). To consider the diversity, we adopt the heuristic of [52] where we first randomly pre-select 10000 samples to be considered, and then from them we choose the 1000\nsamples to be labeled, based on the acquisition score. This method probabilistically adds the concept of diversity to the model. We present the results in Figure 3a, showing that the method has a positive effect in the first few AL cycles. In particular, it improves the results by 1.5pp in the second cycle, and by a maximum of 2pp in the fourth AL cycle. After the sixth cycle, we do not see any difference between the method that uses diversity and the one that does not.\n\nWe now perform another experiment where we emulate a dataset with repetition. We duplicate each sample twice (CIFAR-10x2) and five times (CIFAR-10x5). Naturally, any method that does not consider diversity will struggle with datasets that have repetition. This is because, for every sample that has a high acquisition score, it will also select the other identical samples to label, despite that they do not contain any extra information. We present the results in Figure 3b for Cifar-10x2, and Figure 3c for Cifar-10x5 We show that the method that does not use any diversity not only suffers low results but actually performs worse than the random baseline. Further, the higher the repetition, the worse the results of the method. On the other hand, considering the diversity helps in not selecting identical samples, reaching significantly better results than the random baseline.\n\nRecommendation 4: Diversity matters.\n\nWe recommend that practitioners add the notion of diversity to their AL framework. While some methods based on constrained optimization [13] are computationally expensive, even simple heuristics help, especially in the early AL cycles. Diversity is important in cases of sample repetition, and in such cases, uncertainty-based AL without diversity methods reach lower performance than the random baseline.\n"
    },
    {
        "level": "##",
        "title": "4.3.4 Extension To Object Detection",
        "content": "\nWe now do a small study of AL methods in object detection. We extend the same six methods for object detection, not considering the AL methods that are completely tailored for object detection. We do experiments in the PASCAL VOC07+12 dataset, using SSD object detector [29], and the training framework of [7]. We present our results in Figure 4a. In the second AL cycle, we see that all methods except Core-Set perform roughly the same. Core-Set starts very strongly, improving over the random baseline by almost 2pp. In the third cycle, all methods outperform the random baseline by at least 0.5pp with Core-set reaching the best results outperforming the random baseline by more than 1pp. In the other cycles, Core-Set, LLAL and entropy reach around the same results, all of them ending the training by at least 1pp better than the random baseline. However, BALD and Variational Ratio just slightly improved over the random baseline.\n\nRecommendation 5: Use AL for object detection.\n\nHowever, do not expect the performance improvement to be as big as in classification. From the evaluated methods entropy and Core-set seem to perform the best. From other studies, methods tailored for object detection tend to outperform our baselines [1, 7, 14, 20, 39, 53], so we recommend considering them in addition to mentioned methods.\n"
    },
    {
        "level": "##",
        "title": "4.3.5 Consistency Helps Active Learning",
        "content": "\nWe combine Active Learning with consistency-based Semi- Supervised Learning (AL-SSL) and see their combined effects. For classification, we follow previous work and use [4] as the SSL algorithm of choice. We use an adaption of the method [18] for object detection. We show the results of the SSL when done without active learning (random baseline), with entropy acquisition function and with inconsistency acquisition function [11]. For the inconsistency acquisition function, we use the consistency loss as done in [8]. We provide all the implementation details in the supplementary material. We present the classification results in Figure 4c. We see that the results of all methods are significantly higher than those in the previous experiments. This is because of the effect of the semi-supervised learning, that considerably improves the results. However, we can also see the effect of Active Learning, which comes at the top of Semi-Supervised Learning. We see that both the entropy and consistency considerably improve over the random baseline, with inconsistency-based AL reaching the best results Similarly, we present the results of object detection in Figure 4b. We observe that while in the second AL\ncycle, there is just a small improvement between the AL-\nSSL methods, and the SSL method with random acquisition score, in the later cycles, we see a significant improvement. For example, entropy outperforms random by more than 1pp in the last three AL cycles, with a peak of 1.7pp performance improvement in the last cycle. We also observe that the performance of the entropy acquisition function and the inconsistency AL are very close to each other, with the methods performing as well as each other.\n\nRecommendation 6: Combine SSL with AL. The results will significantly improve, and the two methodologies improve each other.\n\n5. Conclusion In this study, we performed a fair empirical study of various Active Learning methods and ablated some of the most important parts of the models. We performed the study in a controlled setting, where we took care to minimize the possible fluctuations of the network training, hyperparameters and initial labeling set. Our most interesting finding is surprising: in general, no method tends to outperform entropy, the simplest acquisition function. We do ablations in the budget for each AL cycle, training or finetuning the network, and handling the diversity in the dataset, giving recommendations for each of them. We end our study by showing experiments that combined AL with SSL, and extensions in object detection.\n"
    },
    {
        "level": "##",
        "title": "Broader Impact",
        "content": "\nLabeling data is one of the biggest pitfalls in machine learning cycles. It is an expensive process and subject to human errors. In some settings, such as autonomous driving, the majority of data is redundant, thus labeling more of it, does not come with a performance improvement. AL is one of the most promising approaches that helps practitioners in choosing to label the right data, which when fed to a network, give the best improvement. Despite many AL methods claiming to be the SOTA in the field, there are many questions with respect to the experimental setup, and thus the lessons learned from them. In this work, we shed some light on the world of AL, devising a fair setup, and training multiple AL methods in different datasets. We trained over 3, 000 networks, showing the best-performing AL method, and learning in the process some insights that we hope can help other researchers. Similar studies in SSL [34] and metric learning [33] gave a massive boost to the research in their field. We hope our study can help practitioners choose the right acquisition function and engineering practices so that not everyone needs to reinvent the wheel.\n"
    },
    {
        "level": "##",
        "title": "References",
        "content": "\n[1] Hamed Habibi Aghdam, Abel Gonzalez-Garcia, Antonio M.\nL\u00b4opez, and Joost van de Weijer. Active learning for deep\ndetection neural networks. In *ICCV*, 2019. 7\n[2] William H. Beluch, Tim Genewein, Andreas N\u00a8urnberger,\nand Jan M. K\u00a8ohler. The power of ensembles for active learning in image classification. In *CVPR*, 2018. 1, 2, 4\n[3] David Berthelot, Nicholas Carlini, Ekin D. Cubuk, Alex Kurakin, Kihyuk Sohn, Han Zhang, and Colin Raffel. Remixmatch: Semi-supervised learning with distribution matching and augmentation anchoring. In *ICLR*. 3\n[4] David Berthelot, Nicholas Carlini, Ian J. Goodfellow, Nicolas Papernot, Avital Oliver, and Colin Raffel. Mixmatch: A holistic approach to semi-supervised learning. In *NeurIPS*,\n2019. 3, 8, 11\n[5] David Berthelot, Rebecca Roelofs, Kihyuk Sohn, Nicholas\nCarlini, and Alex Kurakin. Adamatch: A unified approach to semi-supervised learning and domain adaptation. *CoRR*,\nabs/2106.04732, 2021. 3\n[6] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian\nSchroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In ECCV, 2018. 1\n[7] Jiwoong Choi, Ismail Elezi, Hyuk-Jae Lee, Cl\u00b4ement Farabet, and Jose M. Alvarez. Active learning for deep object detection via probabilistic modeling. In *ICCV*, 2021. 7\n[8] Ismail Elezi, Zhiding Yu, Anima Anandkumar, Laura Leal-\nTaixe, and Jose M. Alvarez. Not all labels are equal: Rationalizing the labeling costs for training object detection. In CVPR, 2022. 3, 8, 11\n[9] Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian\napproximation:\nRepresenting model uncertainty in deep\nlearning. In *ICML*, 2016. 2\n[10] Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep\nbayesian active learning with image data. In International Conference on Machine Learning, pages 1183\u20131192, 2017. 2\n[11] Mingfei Gao, Zizhao Zhang, Guo Yu, Sercan \u00a8Omer Arik,\nLarry S. Davis, and Tomas Pfister. Consistency-based semisupervised active learning: Towards minimizing labeling cost. In *ECCV*, 2020. 1, 3, 8, 11\n[12] Gregory Griffin, Alex Holub, and Pietro Perona. Caltech-256\nobject category dataset. 2007. 4\n[13] Gurobi Optimization, LLC.\nGurobi Optimizer Reference\nManual, 2023. 3, 7\n[14] Elmar Haussmann, Michele Fenzi, Kashyap Chitta, Jan Ivanecky, Hanson Xu, Donna Roy, Akshita Mittel, Nicolas Koumchatzky, Clement Farabet, and Jose M Alvarez. Scalable active learning for object detection. In IV, 2020. 7\n[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition.\nIn *CVPR*,\n2016. 1\n[16] Neil Houlsby, Ferenc Huszar, Zoubin Ghahramani, and M\u00b4at\u00b4e\nLengyel. Bayesian active learning for classification and preference learning. *CoRR*, abs/1112.5745, 2011. 2\n[17] Juan Eugenio Iglesias, Ender Konukoglu, Albert Montillo,\nZhuowen Tu, and Antonio Criminisi. Combining generative\nand discriminative models for semantic segmentation of CT scans via active learning. In Information Processing in Medical Imaging, 2011. 2\n\n[18] Jisoo Jeong, Seungeui Lee, Jeesoo Kim, and Nojun Kwak.\nConsistency-based semi-supervised learning for object detection. In *NeurIPS*, 2019. 8, 11\n[19] Armand Joulin, Laurens van der Maaten, Allan Jabri, and\nNicolas Vasilache.\nLearning visual features from large\nweakly supervised data. In *ECCV*, 2016. 1\n[20] Chieh-Chi Kao, Teng-Yok Lee, Pradeep Sen, and Ming-Yu\nLiu. Localization-aware active learning for object detection. In *ACCV*, 2018. 7\n[21] Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In International Conference on Learning Representations, 2014. 2\n[22] Andreas Kirsch, Joost van Amersfoort, and Yarin Gal.\nBatchbald: Efficient and diverse batch acquisition for deep\nbayesian active learning. In Advances in Neural Information Processing Systems 32, pages 7024\u20137035, 2019. 2\n[23] Alex Krizhevsky. Learning multiple layers of features from\ntiny images. Technical report, 2009. 4\n[24] Samuli Laine and Timo Aila. Temporal ensembling for semisupervised learning. In International Conference on Learning Representations, 2017. 3\n[25] Dong-Hyun Lee.\nPseudo-label: The simple and efficient\nsemi-supervised learning method for deep neural networks. ICML Workshop on Challenges in Representation Learning, 2013. 3\n[26] David D. Lewis and Jason Catlett.\nHeterogeneous uncertainty sampling for supervised learning. In Machine Learning, Proceedings of the Eleventh International Conference, pages 148\u2013156, 1994. 2\n[27] David D. Lewis and William A. Gale. A sequential algorithm for training text classifiers.\nIn Proceedings of the\n17th Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, pages 3\u2013 12. ACM/Springer, 1994. 2\n[28] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and\nC. Lawrence Zitnick. Microsoft COCO: common objects in context. In *European Conference in Computer Vision*, pages 740\u2013755, 2014. 1\n[29] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian\nSzegedy, Scott Reed, Cheng-Yang Fu, and Alexander C Berg. Ssd: Single shot multibox detector. In European Conference on Computer Vision (ECCV), 2016. 7\n[30] Wenjie Luo, Alexander G. Schwing, and Raquel Urtasun.\nLatent structured active learning. In Advances in Neural Information Processing Systems, pages 728\u2013736, 2013. 2\n[31] Dhruv Mahajan, Ross B. Girshick, Vignesh Ramanathan,\nKaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pretraining.\nIn European Conference in Computer Vision, pages 185\u2013201, 2018. 1\n[32] Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and\nShin Ishii.\nVirtual adversarial training: A regularization\nmethod for supervised and semi-supervised learning. IEEE\nTrans. Pattern Anal. Mach. Intell., 41(8):1979\u20131993, 2019.\n\n1, 3\n\n[33] Kevin Musgrave, Serge J. Belongie, and Ser-Nam Lim. A\nmetric learning reality check. In *ECCV*, 2020. 1, 8\n[34] Avital Oliver, Augustus Odena, Colin Raffel, Ekin Dogus\nCubuk, and Ian J. Goodfellow. Realistic evaluation of deep semi-supervised learning algorithms. In Advances in Neural\nInformation Processing Systems, pages 3239\u20133250, 2018. 1, 3, 8\n[35] George Orwell. Animal farm. In *William Collins*, 1946. 1\n[36] Antti Rasmus, Mathias Berglund, Mikko Honkala, Harri\nValpola, and Tapani Raiko. Semi-supervised learning with ladder networks. In Advances in Neural Information Processing Systems, pages 3546\u20133554, 2015. 1\n[37] Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun.\nFaster R-CNN: towards real-time object detection with region proposal networks. In Advances in Neural Information Processing Systems, pages 91\u201399, 2015. 1\n[38] Dan Roth and Kevin Small. Margin-based active learning\nfor structured output spaces.\nIn European Conference on\nMachine Learning, pages 413\u2013424, 2006. 2\n[39] Soumya Roy, Asim Unmesh, and Vinay P. Namboodiri.\nDeep active learning for object detection. In *BMVC*, 2018. 7\n[40] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael S. Bernstein, Alexander C. Berg, and Fei-Fei Li. Imagenet large scale visual recognition challenge. *International Journal of Computer Vision*,\n115(3):211\u2013252, 2015. 1\n[41] Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set approach. In International Conference on Learning Representations, 2018. 2\n[42] Burr Settles. *Active Learning*. Synthesis Lectures on Artificial Intelligence and Machine Learning. Morgan & Claypool Publishers, 2012. 2\n[43] Burr Settles and Mark Craven. An analysis of active learning\nstrategies for sequence labeling tasks. In Empirical Methods in Natural Language Processing, pages 1070\u20131079, 2008. 2\n[44] H. Sebastian Seung, Manfred Opper, and Haim Sompolinsky. Query by committee. In Conference on Computational\nLearning Theory, pages 287\u2013294, 1992. 2\n[45] Claude E. Shannon. A mathematical theory of communication.\nMobile Computing and Communications Review,\n5(1):3\u201355, 2001. 2\n[46] Samarth Sinha, Sayna Ebrahimi, and Trevor Darrell. Variational adversarial active learning. *CoRR*, abs/1904.00370, 2019. 1, 2, 4\n[47] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao\nZhang, Han Zhang, Colin Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semisupervised learning with consistency and confidence.\nIn\nNeurIPS, 2020. 3\n[48] Antti Tarvainen and Harri Valpola. Mean teachers are better\nrole models: Weight-averaged consistency targets improve semi-supervised deep learning results. In Advances in Neural Information Processing Systems, pages 1195\u20131204, 2017. 3\n[49] Simon Tong and Daphne Koller. Support vector machine active learning with applications to text classification. Journal of Machine Learning Research, 2:45\u201366, 2001. 2\n[50] Toan Tran, Thanh-Toan Do, Ian D. Reid, and Gustavo\nCarneiro.\nBayesian generative active deep learning.\nIn\nICML, 2019. 2\n[51] Sudheendra Vijayanarasimhan and Kristen Grauman. Largescale live active learning: Training object detectors with crawled data and crowds. In Computer Vision and Pattern Recognition, pages 1449\u20131456, 2011. 2\n[52] Donggeun Yoo and In So Kweon. Learning loss for active\nlearning. In *Computer Vision and Pattern Recognition*, pages 93\u2013102. Computer Vision Foundation / IEEE, 2019. 1, 2, 3, 7\n[53] Tianning Yuan, Fang Wan, Mengying Fu, Jianzhuang Liu,\nSongcen Xu, Xiangyang Ji, and Qixiang Ye. Multiple instance active learning for object detection. In *CVPR*, 2021.\n7\n[54] Bowen Zhang, Yidong Wang, Wenxin Hou, Hao Wu, Jindong Wang, Manabu Okumura, and Takahiro Shinozaki. Flexmatch: Boosting semi-supervised learning with curriculum pseudo labeling. In *NeurIPS*, 2021. 3\nIn this supplementary material, we complement the results of the main paper. We give extra information about the consistency sections (Section 4.3.5 in the main paper), and we provide the mean and standard deviation for every experiment. We perform each experiment in a single NVIDIA V100 GPU.\n"
    },
    {
        "level": "##",
        "title": "5.1. Consistency Helps Active Learning 5.2. Image Classification",
        "content": "\nWe combine Active Learning with consistency-based Semi-Supervised Learning (AL-SSL) and see their combined effects. Because the results of SSL are already quite high with 1k \u2212 2k labeled images, we perform the experiments starting with 250 images, and double the number of labels in each cycle. For classification, we follow previous work and use MixMatch [4, 11] as the SSL algorithm of choice. We train the network for 1024 epochs, we do not make any changes to the algorithm, training procedure, or the backbone. During training, MixMatch makes two different augmentations in an image, and computes the loss function as the distance between the network's prediction We use the same loss function as inconsistency acquisition score.\n"
    },
    {
        "level": "##",
        "title": "5.2.1 Object Detection",
        "content": "\nWe present the results of the SSL in object detection. As SSL method, we use that of [18]. The method feed to the network an image, and its augmented version (for example, by performing a horizontal clip), and then computes as loss function the symmetric KL divergence between the predictions of the network for both views of the image. Similar to [8], we use the same loss function as acquisition function. We train the model for 120, 000 iterations using SGD\nwith momentum. and we do not make any changes to the algorithm or the training procedure.\n"
    },
    {
        "level": "##",
        "title": "5.3. Detailed Results",
        "content": "\nIn the paper, we provide plots for the main experiments due to the limited space. In Tables 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13 we summarize the exact numbers corresponding to Figures 1a, 1b, 1c, 1d, 2a, 2b, 3a, 3b, 3c, 4a, 4b and 4c of the main paper. We provide the mean and the standard deviation for each method and AL cycle. Each experiment has been run five times.\n\n| # labeled   |   Random |\n|-------------|----------|\n| 1k          |          |\n| 51.78       |          |\n| \u00b1           |          |\n| 0.6         |          |\n| 50.74       |          |\n| \u00b1           |          |\n| 1.5         |    50.74 |\n| \u00b1           |          |\n| 2.3         |    51.01 |\n| \u00b1           |          |\n| 1.1         |    50.34 |\n| \u00b1           |          |\n| 0.3         |    50.74 |\n| \u00b1           |          |\n| 0.6         |          |\n| 2k          |          |\n| 65.48       |          |\n| \u00b1           |          |\n| 0.7         |    66.58 |\n| \u00b1           |          |\n| 1.5         |          |\n| 66.90       |          |\n| \u00b1           |          |\n| 1.6         |          |\n| 62.71       |          |\n| \u00b1           |          |\n| 0.5         |    63.33 |\n| \u00b1           |          |\n| 0.8         |    62.82 |\n| \u00b1           |          |\n| 0.5         |          |\n| 3k          |    73.69 |\n| \u00b1           |          |\n| 0.9         |    76.09 |\n| \u00b1           |          |\n| 1.4         |          |\n| 76.14       |          |\n| \u00b1           |          |\n| 1.6         |          |\n| 72.42       |          |\n| \u00b1           |          |\n| 0.3         |    72.06 |\n| \u00b1           |          |\n| 0.6         |    71.6  |\n| \u00b1           |          |\n| 0.5         |          |\n| 4k          |    78.66 |\n| \u00b1           |          |\n| 1.3         |    80.81 |\n| \u00b1           |          |\n| 0.8         |          |\n| 81.34       |          |\n| \u00b1           |          |\n| 0.5         |          |\n| 78.92       |          |\n| \u00b1           |          |\n| 0.5         |    78.26 |\n| \u00b1           |          |\n| 0.3         |    76.27 |\n| \u00b1           |          |\n| 0.4         |          |\n| 5k          |          |\n| 81.05       |          |\n| \u00b1           |          |\n| 1.0         |    82.52 |\n| \u00b1           |          |\n| 0.4         |          |\n| 83.59       |          |\n| \u00b1           |          |\n| 0.6         |          |\n| 80.88       |          |\n| \u00b1           |          |\n| 0.5         |    82.65 |\n| \u00b1           |          |\n| 0.6         |    80.53 |\n| \u00b1           |          |\n| 0.2         |          |\n| 6k          |    83.52 |\n| \u00b1           |          |\n| 0.7         |    85.78 |\n| \u00b1           |          |\n| 0.3         |          |\n| 86.26       |          |\n| \u00b1           |          |\n| 0.7         |          |\n| 84.62       |          |\n| \u00b1           |          |\n| 0.3         |    86.23 |\n| \u00b1           |          |\n| 0.3         |    83.77 |\n| \u00b1           |          |\n| 0.2         |          |\n| 7k          |    85.01 |\n| \u00b1           |          |\n| 1.0         |    87.36 |\n| \u00b1           |          |\n| 0.4         |          |\n| 87.95       |          |\n| \u00b1           |          |\n| 0.3         |          |\n| 86.32       |          |\n| \u00b1           |          |\n| 0.1         |    87.79 |\n| \u00b1           |          |\n| 0.2         |    85.43 |\n| \u00b1           |          |\n| 0.4         |          |\n| 8k          |    86.1  |\n| \u00b1           |          |\n| 0.7         |    88.55 |\n| \u00b1           |          |\n| 0.3         |          |\n| 89.19       |          |\n| \u00b1           |          |\n| 0.2         |          |\n| 87.92       |          |\n| \u00b1           |          |\n| 0.2         |    88.75 |\n| \u00b1           |          |\n| 0.2         |    86.49 |\n| \u00b1           |          |\n| 0.2         |          |\n| 9k          |    87.15 |\n| \u00b1           |          |\n| 0.7         |    89.42 |\n| \u00b1           |          |\n| 0.4         |          |\n| 90.16       |          |\n| \u00b1           |          |\n| 0.2         |          |\n| 88.87       |          |\n| \u00b1           |          |\n| 0.2         |    90.11 |\n| \u00b1           |          |\n| 0.1         |    87.53 |\n| \u00b1           |          |\n| 0.2         |          |\n| 10k         |          |\n| 87.63       |          |\n| \u00b1           |          |\n| 0.5         |    90.08 |\n| \u00b1           |          |\n| 0.2         |          |\n| 90.89       |          |\n| \u00b1           |          |\n| 0.2         |          |\n| 89.72       |          |\n| \u00b1           |          |\n| 0.2         |    90.5  |\n| \u00b1           |          |\n| 0.1         |    87.85 |\n| \u00b1           |          |\n| 0.1         |          |\n| 11k         |    88.51 |\n| \u00b1           |          |\n| 0.6         |    90.97 |\n| \u00b1           |          |\n| 0.4         |          |\n| 91.65       |          |\n| \u00b1           |          |\n| 0.3         |          |\n| 90.81       |          |\n| \u00b1           |          |\n| 0.1         |    91.53 |\n| \u00b1           |          |\n| 0.1         |    89.19 |\n| \u00b1           |          |\n| 0.2         |          |\n| 12k         |    89.13 |\n| \u00b1           |          |\n| 0.6         |    91.61 |\n| \u00b1           |          |\n| 0.4         |          |\n| 92.27       |          |\n| \u00b1           |          |\n| 0.1         |          |\n| 91.41       |          |\n| \u00b1           |          |\n| 0.2         |    92.04 |\n| \u00b1           |          |\n| 0.3         |    89.4  |\n| \u00b1           |          |\n| 0.2         |          |\n| 13k         |    89.66 |\n| \u00b1           |          |\n| 0.5         |    92    |\n| \u00b1           |          |\n| 0.2         |          |\n| 92.76       |          |\n| \u00b1           |          |\n| 0.2         |          |\n| 91.75       |          |\n| \u00b1           |          |\n| 0.2         |    92.21 |\n| \u00b1           |          |\n| 0.2         |    89.41 |\n| \u00b1           |          |\n| 0.1         |          |\n| 14k         |    89.98 |\n| \u00b1           |          |\n| 0.6         |    92.36 |\n| \u00b1           |          |\n| 0.2         |          |\n| 93.18       |          |\n| \u00b1           |          |\n| 0.2         |          |\n| 92.03       |          |\n| \u00b1           |          |\n| 0.2         |    92.91 |\n| \u00b1           |          |\n| 0.3         |    90.24 |\n| \u00b1           |          |\n| 0.2         |          |\n| 15k         |    90.37 |\n| \u00b1           |          |\n| 0.5         |    92.63 |\n| \u00b1           |          |\n| 0.2         |          |\n| 93.35       |          |\n| \u00b1           |          |\n| 0.2         |          |\n| 92.40       |          |\n| \u00b1           |          |\n| 0.2         |    93.03 |\n| \u00b1           |          |\n| 0.2         |    90.17 |\n| \u00b1           |          |\n| 0.1         |          |\n| 16k         |    90.89 |\n| \u00b1           |          |\n| 0.4         |    93.06 |\n| \u00b1           |          |\n| 0.2         |          |\n| 93.67       |          |\n| \u00b1           |          |\n| 0.2         |          |\n| 92.89       |          |\n| \u00b1           |          |\n| 0.2         |    93.18 |\n| \u00b1           |          |\n| 0.1         |    90.65 |\n| \u00b1           |          |\n| 0.1         |          |\n| 17k         |          |\n| 91.06       |          |\n| \u00b1           |          |\n| 0.6         |    93.31 |\n| \u00b1           |          |\n| 0.2         |          |\n| 93.89       |          |\n| \u00b1           |          |\n| 0.1         |          |\n| 93.09       |          |\n| \u00b1           |          |\n| 0.2         |    93.22 |\n| \u00b1           |          |\n| 0.2         |    90.92 |\n| \u00b1           |          |\n| 0.2         |          |\n| 18k         |    91.39 |\n| \u00b1           |          |\n| 0.5         |    93.52 |\n| \u00b1           |          |\n| 0.2         |          |\n| 94.15       |          |\n| \u00b1           |          |\n| 0.1         |          |\n| 93.29       |          |\n| \u00b1           |          |\n| 0.1         |    93.56 |\n| \u00b1           |          |\n| 0.4         |    91.14 |\n| \u00b1           |          |\n| 0.2         |          |\n| 19k         |    91.6  |\n| \u00b1           |          |\n| 0.4         |    93.6  |\n| \u00b1           |          |\n| 0.2         |          |\n| 94.36       |          |\n| \u00b1           |          |\n| 0.2         |          |\n| 93.45       |          |\n| \u00b1           |          |\n| 0.1         |    93.55 |\n| \u00b1           |          |\n| 0.1         |    91.48 |\n| \u00b1           |          |\n| 0.1         |          |\n| 20k         |          |\n| 91.86       |          |\n| \u00b1           |          |\n| 0.4         |    93.67 |\n| \u00b1           |          |\n| 0.2         |          |\n| 94.40       |          |\n| \u00b1           |          |\n| 0.1         |          |\n| 93.85       |          |\n| \u00b1           |          |\n| 0.1         |    93.81 |\n| \u00b1           |          |\n| 0.2         |    91.77 |\n| \u00b1           |          |\n| 0.1         |          |\n| # labeled   |   Random |\n|-------------|----------|\n| 1k          |    13.28 |\n| \u00b1           |          |\n| 0.7         |    13.19 |\n| \u00b1           |          |\n| 0.7         |    13.42 |\n| \u00b1           |          |\n| 0.5         |    13.24 |\n| \u00b1           |          |\n| 0.7         |          |\n| 13.56       |          |\n| \u00b1           |          |\n| 0.8         |          |\n| 13.39       |          |\n| \u00b1           |          |\n| 0.6         |          |\n| 2k          |    20.43 |\n| \u00b1           |          |\n| 0.7         |    18.69 |\n| \u00b1           |          |\n| 0.7         |    19.87 |\n| \u00b1           |          |\n| 0.4         |          |\n| 21.24       |          |\n| \u00b1           |          |\n| 0.8         |          |\n| 19.73       |          |\n| \u00b1           |          |\n| 0.9         |    20.32 |\n| \u00b1           |          |\n| 1.3         |          |\n| 3k          |    26.89 |\n| \u00b1           |          |\n| 1.0         |    24.37 |\n| \u00b1           |          |\n| 0.9         |    25.47 |\n| \u00b1           |          |\n| 0.8         |          |\n| 28.28       |          |\n| \u00b1           |          |\n| 1.3         |          |\n| 25.00       |          |\n| \u00b1           |          |\n| 0.9         |    25.72 |\n| \u00b1           |          |\n| 0.8         |          |\n| 4k          |          |\n| 32.88       |          |\n| \u00b1           |          |\n| 1.6         |    29.57 |\n| \u00b1           |          |\n| 1.3         |    31.24 |\n| \u00b1           |          |\n| 1.6         |          |\n| 33.87       |          |\n| \u00b1           |          |\n| 0.9         |          |\n| 30.28       |          |\n| \u00b1           |          |\n| 1.4         |    30.38 |\n| \u00b1           |          |\n| 1.1         |          |\n| 5k          |    36.99 |\n| \u00b1           |          |\n| 1.4         |    33.09 |\n| \u00b1           |          |\n| 1.0         |    34.66 |\n| \u00b1           |          |\n| 2.1         |          |\n| 38.36       |          |\n| \u00b1           |          |\n| 1.2         |          |\n| 34.79       |          |\n| \u00b1           |          |\n| 0.9         |    35.66 |\n| \u00b1           |          |\n| 1.7         |          |\n| 6k          |          |\n| 43.45       |          |\n| \u00b1           |          |\n| 1.1         |    39.9  |\n| \u00b1           |          |\n| 0.7         |    43    |\n| \u00b1           |          |\n| 1.6         |          |\n| 45.70       |          |\n| \u00b1           |          |\n| 1.3         |          |\n| 41.30       |          |\n| \u00b1           |          |\n| 1.3         |    41.73 |\n| \u00b1           |          |\n| 1.2         |          |\n| 7k          |    47.08 |\n| \u00b1           |          |\n| 1.3         |    45.03 |\n| \u00b1           |          |\n| 1.0         |    47.13 |\n| \u00b1           |          |\n| 1.2         |          |\n| 49.06       |          |\n| \u00b1           |          |\n| 1.1         |          |\n| 44.29       |          |\n| \u00b1           |          |\n| 1.1         |    46.81 |\n| \u00b1           |          |\n| 1.0         |          |\n| 8k          |    50.41 |\n| \u00b1           |          |\n| 1.3         |    48.65 |\n| \u00b1           |          |\n| 0.7         |    50.08 |\n| \u00b1           |          |\n| 1.1         |          |\n| 51.51       |          |\n| \u00b1           |          |\n| 0.8         |          |\n| 48.01       |          |\n| \u00b1           |          |\n| 1.7         |    49.58 |\n| \u00b1           |          |\n| 0.9         |          |\n| 9k          |    52.8  |\n| \u00b1           |          |\n| 1.1         |    51.74 |\n| \u00b1           |          |\n| 0.7         |    52.85 |\n| \u00b1           |          |\n| 0.7         |          |\n| 53.83       |          |\n| \u00b1           |          |\n| 0.7         |          |\n| 50.97       |          |\n| \u00b1           |          |\n| 0.9         |    51.32 |\n| \u00b1           |          |\n| 0.9         |          |\n| 10k         |    55.09 |\n| \u00b1           |          |\n| 0.8         |    54.05 |\n| \u00b1           |          |\n| 0.5         |    55.68 |\n| \u00b1           |          |\n| 0.7         |          |\n| 55.90       |          |\n| \u00b1           |          |\n| 0.9         |          |\n| 54.03       |          |\n| \u00b1           |          |\n| 0.7         |    54.79 |\n| \u00b1           |          |\n| 0.5         |          |\n| 11k         |    56.89 |\n| \u00b1           |          |\n| 0.9         |    56.49 |\n| \u00b1           |          |\n| 0.6         |    57.66 |\n| \u00b1           |          |\n| 0.8         |          |\n| 58.25       |          |\n| \u00b1           |          |\n| 0.4         |          |\n| 56.41       |          |\n| \u00b1           |          |\n| 0.6         |    56.95 |\n| \u00b1           |          |\n| 0.8         |          |\n| 12k         |    58.58 |\n| \u00b1           |          |\n| 0.8         |    58.51 |\n| \u00b1           |          |\n| 0.6         |    59.67 |\n| \u00b1           |          |\n| 0.5         |          |\n| 59.93       |          |\n| \u00b1           |          |\n| 0.4         |          |\n| 58.61       |          |\n| \u00b1           |          |\n| 0.5         |    58.56 |\n| \u00b1           |          |\n| 0.4         |          |\n| 13k         |    60.05 |\n| \u00b1           |          |\n| 0.5         |    60.29 |\n| \u00b1           |          |\n| 0.5         |          |\n| 61.21       |          |\n| \u00b1           |          |\n| 0.4         |          |\n| 61.09       |          |\n| \u00b1           |          |\n| 0.4         |    59.58 |\n| \u00b1           |          |\n| 0.4         |    59.89 |\n| \u00b1           |          |\n| 0.6         |          |\n| 14k         |    61.3  |\n| \u00b1           |          |\n| 0.6         |    61.88 |\n| \u00b1           |          |\n| 0.6         |          |\n| 62.80       |          |\n| \u00b1           |          |\n| 0.5         |          |\n| 62.33       |          |\n| \u00b1           |          |\n| 0.3         |    60.38 |\n| \u00b1           |          |\n| 0.6         |    60.97 |\n| \u00b1           |          |\n| 0.5         |          |\n| 15k         |    62.35 |\n| \u00b1           |          |\n| 0.7         |    63.39 |\n| \u00b1           |          |\n| 0.5         |          |\n| 64.35       |          |\n| \u00b1           |          |\n| 0.5         |          |\n| 63.56       |          |\n| \u00b1           |          |\n| 0.7         |    61.56 |\n| \u00b1           |          |\n| 0.5         |    62.25 |\n| \u00b1           |          |\n| 0.4         |          |\n| 16k         |    63.35 |\n| \u00b1           |          |\n| 0.6         |    64.32 |\n| \u00b1           |          |\n| 0.4         |          |\n| 65.42       |          |\n| \u00b1           |          |\n| 0.1         |          |\n| 64.58       |          |\n| \u00b1           |          |\n| 0.4         |    62.67 |\n| \u00b1           |          |\n| 0.3         |    63.26 |\n| \u00b1           |          |\n| 0.5         |          |\n| 17k         |    64.22 |\n| \u00b1           |          |\n| 0.5         |    65.48 |\n| \u00b1           |          |\n| 0.6         |          |\n| 66.96       |          |\n| \u00b1           |          |\n| 0.0         |          |\n| 65.42       |          |\n| \u00b1           |          |\n| 0.3         |    63.72 |\n| \u00b1           |          |\n| 0.3         |    64.32 |\n| \u00b1           |          |\n| 0.4         |          |\n| 18k         |    65.21 |\n| \u00b1           |          |\n| 0.6         |    66.51 |\n| \u00b1           |          |\n| 0.5         |          |\n| 67.58       |          |\n| \u00b1           |          |\n| 0.3         |          |\n| 66.36       |          |\n| \u00b1           |          |\n| 0.5         |    64.83 |\n| \u00b1           |          |\n| 0.4         |    65.11 |\n| \u00b1           |          |\n| 0.4         |          |\n| 19k         |    65.92 |\n| \u00b1           |          |\n| 0.6         |    67.46 |\n| \u00b1           |          |\n| 0.4         |          |\n| 68.41       |          |\n| \u00b1           |          |\n| 0.0         |          |\n| 67.11       |          |\n| \u00b1           |          |\n| 0.5         |    65.52 |\n| \u00b1           |          |\n| 0.3         |    66.07 |\n| \u00b1           |          |\n| 0.3         |          |\n| 20k         |    66.69 |\n| \u00b1           |          |\n| 0.5         |    68.18 |\n| \u00b1           |          |\n| 0.4         |          |\n| 69.92       |          |\n| \u00b1           |          |\n| 0.8         |          |\n| 68.12       |          |\n| \u00b1           |          |\n| 0.4         |    66.6  |\n| \u00b1           |          |\n| 0.4         |    66.5  |\n| \u00b1           |          |\n| 0.4         |          |\n| # labeled   |   Random |\n|-------------|----------|\n| 1k          |    21.35 |\n| \u00b1           |          |\n| 1.6         |          |\n| 21.75       |          |\n| \u00b1           |          |\n| 1.7         |          |\n| 20.98       |          |\n| \u00b1           |          |\n| 1.4         |    20.17 |\n| \u00b1           |          |\n| 1.5         |    21.52 |\n| \u00b1           |          |\n| 1.6         |    21.74 |\n| \u00b1           |          |\n| 1.5         |          |\n| 2k          |    35.66 |\n| \u00b1           |          |\n| 1.9         |    35.76 |\n| \u00b1           |          |\n| 1.7         |          |\n| 41.04       |          |\n| \u00b1           |          |\n| 2.3         |          |\n| 38.02       |          |\n| \u00b1           |          |\n| 2.3         |    35.75 |\n| \u00b1           |          |\n| 2.4         |    35.98 |\n| \u00b1           |          |\n| 2.0         |          |\n| 3k          |          |\n| 47.21       |          |\n| \u00b1           |          |\n| 1.6         |    49.79 |\n| \u00b1           |          |\n| 1.6         |          |\n| 56.49       |          |\n| \u00b1           |          |\n| 0.9         |          |\n| 53.09       |          |\n| \u00b1           |          |\n| 2.0         |    48.43 |\n| \u00b1           |          |\n| 1.6         |    48.87 |\n| \u00b1           |          |\n| 1.5         |          |\n| 4k          |    56.32 |\n| \u00b1           |          |\n| 1.3         |    58.95 |\n| \u00b1           |          |\n| 1.4         |          |\n| 65.10       |          |\n| \u00b1           |          |\n| 1.3         |          |\n| 62.18       |          |\n| \u00b1           |          |\n| 1.4         |    57.36 |\n| \u00b1           |          |\n| 1.5         |    57.64 |\n| \u00b1           |          |\n| 1.3         |          |\n| 5k          |    62.66 |\n| \u00b1           |          |\n| 0.8         |    64.88 |\n| \u00b1           |          |\n| 1.2         |          |\n| 69.99       |          |\n| \u00b1           |          |\n| 1.0         |          |\n| 66.42       |          |\n| \u00b1           |          |\n| 0.8         |    63.34 |\n| \u00b1           |          |\n| 0.9         |    63.46 |\n| \u00b1           |          |\n| 0.9         |          |\n| # labeled   |   Random |\n|-------------|----------|\n| 1k          |     7.06 |\n| \u00b1           |          |\n| 0.5         |     6.58 |\n| \u00b1           |          |\n| 0.4         |     6.69 |\n| \u00b1           |          |\n| 0.8         |          |\n| 7.29        |          |\n| \u00b1           |          |\n| 0.7         |          |\n| 6.93        |          |\n| \u00b1           |          |\n| 0.5         |     7.06 |\n| \u00b1           |          |\n| 0.7         |          |\n| 2k          |    11.7  |\n| \u00b1           |          |\n| 0.5         |    11.12 |\n| \u00b1           |          |\n| 0.8         |    11.31 |\n| \u00b1           |          |\n| 1.0         |    11.62 |\n| \u00b1           |          |\n| 0.7         |    12.07 |\n| \u00b1           |          |\n| 0.7         |          |\n| 11.86       |          |\n| \u00b1           |          |\n| 0.9         |          |\n| 3k          |          |\n| 16.80       |          |\n| \u00b1           |          |\n| 0.6         |          |\n| 16.07       |          |\n| \u00b1           |          |\n| 1.3         |    16.44 |\n| \u00b1           |          |\n| 1.0         |    15.95 |\n| \u00b1           |          |\n| 0.8         |    16.78 |\n| \u00b1           |          |\n| 0.7         |    16.53 |\n| \u00b1           |          |\n| 0.8         |          |\n| 4k          |          |\n| 22.11       |          |\n| \u00b1           |          |\n| 0.9         |    20.6  |\n| \u00b1           |          |\n| 0.8         |    20.79 |\n| \u00b1           |          |\n| 1.1         |    21.12 |\n| \u00b1           |          |\n| 1.3         |          |\n| 21.68       |          |\n| \u00b1           |          |\n| 1.3         |          |\n| 21.60       |          |\n| \u00b1           |          |\n| 0.8         |          |\n| 5k          |    26.05 |\n| \u00b1           |          |\n| 1.2         |    24.89 |\n| \u00b1           |          |\n| 1.0         |          |\n| 26.45       |          |\n| \u00b1           |          |\n| 0.8         |          |\n| 25.19       |          |\n| \u00b1           |          |\n| 1.6         |    25.58 |\n| \u00b1           |          |\n| 1.5         |    25.56 |\n| \u00b1           |          |\n| 0.7         |          |\n| 6k          |    30.8  |\n| \u00b1           |          |\n| 0.7         |    30.23 |\n| \u00b1           |          |\n| 1.2         |          |\n| 31.27       |          |\n| \u00b1           |          |\n| 1.4         |          |\n| 30.75       |          |\n| \u00b1           |          |\n| 0.9         |    30.76 |\n| \u00b1           |          |\n| 1.2         |    30.76 |\n| \u00b1           |          |\n| 0.4         |          |\n| 7k          |    35.47 |\n| \u00b1           |          |\n| 0.9         |    34.56 |\n| \u00b1           |          |\n| 0.8         |          |\n| 35.71       |          |\n| \u00b1           |          |\n| 0.5         |          |\n| 34.94       |          |\n| \u00b1           |          |\n| 1.1         |    34.92 |\n| \u00b1           |          |\n| 0.9         |    35.02 |\n| \u00b1           |          |\n| 0.4         |          |\n| 8k          |    39.04 |\n| \u00b1           |          |\n| 0.8         |    38.4  |\n| \u00b1           |          |\n| 0.8         |          |\n| 40.99       |          |\n| \u00b1           |          |\n| 0.7         |          |\n| 38.88       |          |\n| \u00b1           |          |\n| 0.8         |    39.42 |\n| \u00b1           |          |\n| 0.9         |    39.18 |\n| \u00b1           |          |\n| 0.4         |          |\n| 9k          |    42.32 |\n| \u00b1           |          |\n| 0.7         |    41.81 |\n| \u00b1           |          |\n| 1.1         |          |\n| 44.30       |          |\n| \u00b1           |          |\n| 1.6         |          |\n| 42.65       |          |\n| \u00b1           |          |\n| 1.2         |    43.39 |\n| \u00b1           |          |\n| 0.9         |    42.95 |\n| \u00b1           |          |\n| 0.4         |          |\n| 10k         |    45.73 |\n| \u00b1           |          |\n| 0.5         |    45    |\n| \u00b1           |          |\n| 0.9         |          |\n| 47.95       |          |\n| \u00b1           |          |\n| 0.6         |          |\n| 47.05       |          |\n| \u00b1           |          |\n| 0.6         |    47.28 |\n| \u00b1           |          |\n| 1.3         |    46.9  |\n| \u00b1           |          |\n| 0.6         |          |\n| #labeled   |   500 |\n|------------|-------|\n| 500        | 41.5  |\n| \u00b1          |       |\n| 3.3        |       |\n| 1k         | 50.22 |\n| \u00b1          |       |\n| 2.1        |       |\n| 50.74      |       |\n| \u00b1          |       |\n| 2.3        |       |\n| 1.5k       |       |\n| 57.67      |       |\n| \u00b1          |       |\n| 1.9        |       |\n| 2k         | 63.28 |\n| \u00b1          |       |\n| 1.5        |       |\n| 66.90      |       |\n| \u00b1          |       |\n| 1.6        |       |\n| 65.78      |       |\n| \u00b1          |       |\n| 1.8        |       |\n| 2.5k       | 68.68 |\n| \u00b1          |       |\n| 1.6        |       |\n| 3k         | 72.75 |\n| \u00b1          |       |\n| 1.5        |       |\n| 76.14      |       |\n| \u00b1          |       |\n| 1.6        |       |\n| 3.5k       |       |\n| 77.24      |       |\n| \u00b1          |       |\n| 1.0        |       |\n| 4k         | 80.32 |\n| \u00b1          |       |\n| 0.5        |       |\n| 81.3       |       |\n| \u00b1          |       |\n| 0.5        |       |\n| 79.60      |       |\n| \u00b1          |       |\n| 0.6        |       |\n| 4.5k       | 82.19 |\n| \u00b1          |       |\n| 0.5        |       |\n| 5k         | 83.39 |\n| \u00b1          |       |\n| 0.5        |       |\n| 83.59      |       |\n| \u00b1          |       |\n| 0.5        |       |\n| 5.5k       |       |\n| 85.04      |       |\n| \u00b1          |       |\n| 0.5        |       |\n| 6k         |       |\n| 85.88      |       |\n| \u00b1          |       |\n| 0.4        |       |\n| 86.26      |       |\n| \u00b1          |       |\n| 0.7        |       |\n| 85.70      |       |\n| \u00b1          |       |\n| 0.5        |       |\n| 6.5k       |       |\n| 87.24      |       |\n| \u00b1          |       |\n| 0.3        |       |\n| 7k         | 87.65 |\n| \u00b1          |       |\n| 0.2        |       |\n| 87.95      |       |\n| \u00b1          |       |\n| 0.2        |       |\n| 7.5k       |       |\n| 88.74      |       |\n| \u00b1          |       |\n| 0.2        |       |\n| 8k         | 89.05 |\n| \u00b1          |       |\n| 0.2        |       |\n| 89.19      |       |\n| \u00b1          |       |\n| 0.2        |       |\n| 88.53      |       |\n| \u00b1          |       |\n| 0.2        |       |\n| 8.5k       | 89.52 |\n| \u00b1          |       |\n| 0.2        |       |\n| 9k         |       |\n| 89.92      |       |\n| \u00b1          |       |\n| 0.2        |       |\n| 90.16      |       |\n| \u00b1          |       |\n| 0.2        |       |\n| 9.5k       |       |\n| 90.26      |       |\n| \u00b1          |       |\n| 0.2        |       |\n| 10k        | 90.45 |\n| \u00b1          |       |\n| 0.1        |       |\n| 90.89      |       |\n| \u00b1          |       |\n| 0.2        |       |\n| 90.42      |       |\n| \u00b1          |       |\n| 0.2        |       |\n| 10.5k      |       |\n| 90.15      |       |\n| \u00b1          |       |\n| 0.1        |       |\n| 11k        | 91.27 |\n| \u00b1          |       |\n| 0.1        |       |\n| 91.65      |       |\n| \u00b1          |       |\n| 0.2        |       |\n| 11.5k      |       |\n| 91.95      |       |\n| \u00b1          |       |\n| 0.1        |       |\n| 12k        | 92.09 |\n| \u00b1          |       |\n| 0.1        |       |\n| 92.27      |       |\n| \u00b1          |       |\n| 0.1        |       |\n| 91.91      |       |\n| \u00b1          |       |\n| 0.1        |       |\n| 12.5k      |       |\n| 92.35      |       |\n| \u00b1          |       |\n| 0.1        |       |\n| 13k        | 92.5  |\n| \u00b1          |       |\n| 0.1        |       |\n| 92.76      |       |\n| \u00b1          |       |\n| 0.2        |       |\n| 13.5k      | 92.78 |\n| \u00b1          |       |\n| 0.1        |       |\n| 14k        |       |\n| 92.65      |       |\n| \u00b1          |       |\n| 0.1        |       |\n| 93.18      |       |\n| \u00b1          |       |\n| 0.1        |       |\n| 92.76      |       |\n| \u00b1          |       |\n| 0.1        |       |\n| 14.5k      | 93.11 |\n| \u00b1          |       |\n| 0.1        |       |\n| 15k        |       |\n| 93.22      |       |\n| \u00b1          |       |\n| 0.1        |       |\n| 93.35      |       |\n| \u00b1          |       |\n| 0.2        |       |\n| 15.5k      | 93.25 |\n| \u00b1          |       |\n| 0.1        |       |\n| 16k        | 93.42 |\n| \u00b1          |       |\n| 0.1        |       |\n| 93.67      |       |\n| \u00b1          |       |\n| 0.1        |       |\n| 93.39      |       |\n| \u00b1          |       |\n| 0.1        |       |\n| 16.5k      | 93.42 |\n| \u00b1          |       |\n| 0.1        |       |\n| 17k        | 93.45 |\n| \u00b1          |       |\n| 0.1        |       |\n| 93.89      |       |\n| \u00b1          |       |\n| 0.1        |       |\n| 17.5k      | 93.52 |\n| \u00b1          |       |\n| 0.0        |       |\n| 18k        | 93.82 |\n| \u00b1          |       |\n| 0.0        |       |\n| 94.15      |       |\n| \u00b1          |       |\n| 0.0        |       |\n| 93.61      |       |\n| \u00b1          |       |\n| 0.0        |       |\n| 18.5k      | 93.76 |\n| \u00b1          |       |\n| 0.0        |       |\n| 19k        | 93.82 |\n| \u00b1          |       |\n| 0.0        |       |\n| 94.36      |       |\n| \u00b1          |       |\n| 0.1        |       |\n| 19.5k      | 93.89 |\n| \u00b1          |       |\n| 0.0        |       |\n| 20k        | 93.98 |\n| \u00b1          |       |\n| 0.0        |       |\n| 94.40      |       |\n| \u00b1          |       |\n| 0.0        |       |\n| 93.89      |       |\n| \u00b1          |       |\n| 0.0        |       |\n| #labeled   |   Scratch |\n|------------|-----------|\n| 1k         |           |\n| 50.74      |           |\n| \u00b1          |           |\n| 0.3        |           |\n| 50.66      |           |\n| \u00b1          |           |\n| 0.9        |           |\n| 2k         |           |\n| 66.90      |           |\n| \u00b1          |           |\n| 1.6        |           |\n| 63.87      |           |\n| \u00b1          |           |\n| 0.4        |           |\n| 3k         |           |\n| 76.14      |           |\n| \u00b1          |           |\n| 1.6        |           |\n| 72.33      |           |\n| \u00b1          |           |\n| 0.7        |           |\n| 4k         |           |\n| 81.34      |           |\n| \u00b1          |           |\n| 0.5        |           |\n| 79.27      |           |\n| \u00b1          |           |\n| 0.7        |           |\n| 5k         |           |\n| 83.59      |           |\n| \u00b1          |           |\n| 0.5        |           |\n| 82.82      |           |\n| \u00b1          |           |\n| 0.3        |           |\n| 6k         |           |\n| 86.26      |           |\n| \u00b1          |           |\n| 0.7        |           |\n| 85.68      |           |\n| \u00b1          |           |\n| 0.2        |           |\n| 7k         |           |\n| 87.95      |           |\n| \u00b1          |           |\n| 0.2        |           |\n| 87.46      |           |\n| \u00b1          |           |\n| 0.1        |           |\n| 8k         |           |\n| 89.19      |           |\n| \u00b1          |           |\n| 0.2        |           |\n| 88.90      |           |\n| \u00b1          |           |\n| 0.2        |           |\n| 9k         |           |\n| 90.16      |           |\n| \u00b1          |           |\n| 0.2        |           |\n| 89.76      |           |\n| \u00b1          |           |\n| 0.3        |           |\n| 10k        |           |\n| 90.89      |           |\n| \u00b1          |           |\n| 0.2        |           |\n| 90.85      |           |\n| \u00b1          |           |\n| 0.1        |           |\n| 11k        |           |\n| 91.65      |           |\n| \u00b1          |           |\n| 0.2        |           |\n| 91.43      |           |\n| \u00b1          |           |\n| 0.2        |           |\n| 12k        |           |\n| 92.27      |           |\n| \u00b1          |           |\n| 0.1        |           |\n| 92.06      |           |\n| \u00b1          |           |\n| 0.1        |           |\n| 13k        |           |\n| 92.76      |           |\n| \u00b1          |           |\n| 0.2        |           |\n| 92.5       |           |\n| \u00b1          |           |\n| 0.1        |           |\n| 14k        |           |\n| 93.18      |           |\n| \u00b1          |           |\n| 0.1        |           |\n| 93.10      |           |\n| \u00b1          |           |\n| 0.1        |           |\n| 15k        |           |\n| 93.35      |           |\n| \u00b1          |           |\n| 0.2        |           |\n| 93.26      |           |\n| \u00b1          |           |\n| 0.2        |           |\n| 16k        |     93.67 |\n| \u00b1          |           |\n| 0.1        |           |\n| 93.71      |           |\n| \u00b1          |           |\n| 0.1        |           |\n| 17k        |           |\n| 93.89      |           |\n| \u00b1          |           |\n| 0.1        |           |\n| 93.77      |           |\n| \u00b1          |           |\n| 0.1        |           |\n| 18k        |           |\n| 94.15      |           |\n| \u00b1          |           |\n| 0.0        |           |\n| 93.98      |           |\n| \u00b1          |           |\n| 0.1        |           |\n| 19k        |           |\n| 94.36      |           |\n| \u00b1          |           |\n| 0.1        |           |\n| 94.33      |           |\n| \u00b1          |           |\n| 0.2        |           |\n| 20k        |           |\n| 94.40      |           |\n| \u00b1          |           |\n| 0.0        |     94.4  |\n| \u00b1          |           |\n| 0.1        |           |\n| #labeled   |   Random |\n|------------|----------|\n| 1k         |          |\n| 51.79      |          |\n| \u00b1          |          |\n| 0.7        |          |\n| 50.74      |          |\n| \u00b1          |          |\n| 2.3        |    51.3  |\n| \u00b1          |          |\n| 2.0        |          |\n| 2k         |    65.49 |\n| \u00b1          |          |\n| 0.7        |          |\n| 66.90      |          |\n| \u00b1          |          |\n| 1.6        |          |\n| 65.43      |          |\n| \u00b1          |          |\n| 1.0        |          |\n| 3k         |    73.7  |\n| \u00b1          |          |\n| 0.9        |          |\n| 76.15      |          |\n| \u00b1          |          |\n| 0.5        |          |\n| 75.86      |          |\n| \u00b1          |          |\n| 1.6        |          |\n| 4k         |          |\n| 78.66      |          |\n| \u00b1          |          |\n| 1.3        |          |\n| 81.35      |          |\n| \u00b1          |          |\n| 0.5        |          |\n| 79.29      |          |\n| \u00b1          |          |\n| 0.6        |          |\n| 5k         |    81.06 |\n| \u00b1          |          |\n| 1.0        |          |\n| 83.59      |          |\n| \u00b1          |          |\n| 0.6        |          |\n| 82.87      |          |\n| \u00b1          |          |\n| 0.2        |          |\n| 6k         |          |\n| 83.52      |          |\n| \u00b1          |          |\n| 0.7        |          |\n| 86.26      |          |\n| \u00b1          |          |\n| 0.7        |          |\n| 86.20      |          |\n| \u00b1          |          |\n| 0.4        |          |\n| 7k         |    85.02 |\n| \u00b1          |          |\n| 1.0        |          |\n| 87.95      |          |\n| \u00b1          |          |\n| 0.3        |          |\n| 87.83      |          |\n| \u00b1          |          |\n| 0.3        |          |\n| 8k         |    86.11 |\n| \u00b1          |          |\n| 0.7        |          |\n| 89.20      |          |\n| \u00b1          |          |\n| 0.2        |          |\n| 89.17      |          |\n| \u00b1          |          |\n| 0.2        |          |\n| 9k         |    87.16 |\n| \u00b1          |          |\n| 0.7        |          |\n| 90.16      |          |\n| \u00b1          |          |\n| 0.2        |          |\n| 90.20      |          |\n| \u00b1          |          |\n| 0.2        |          |\n| 10k        |    87.64 |\n| \u00b1          |          |\n| 0.5        |          |\n| 90.90      |          |\n| \u00b1          |          |\n| 0.2        |          |\n| 90.75      |          |\n| \u00b1          |          |\n| 0.2        |          |\n| 11k        |    88.51 |\n| \u00b1          |          |\n| 0.6        |          |\n| 91.66      |          |\n| \u00b1          |          |\n| 0.3        |          |\n| 91.65      |          |\n| \u00b1          |          |\n| 0.1        |          |\n| 12k        |    89.17 |\n| \u00b1          |          |\n| 0.6        |          |\n| 92.28      |          |\n| \u00b1          |          |\n| 0.1        |          |\n| 93.38      |          |\n| \u00b1          |          |\n| 0.2        |          |\n| 13k        |    89.67 |\n| \u00b1          |          |\n| 0.5        |          |\n| 92.77      |          |\n| \u00b1          |          |\n| 0.2        |          |\n| 92.78      |          |\n| \u00b1          |          |\n| 0.4        |          |\n| 14k        |    89.98 |\n| \u00b1          |          |\n| 0.6        |          |\n| 93.18      |          |\n| \u00b1          |          |\n| 0.2        |          |\n| 93.15      |          |\n| \u00b1          |          |\n| 0.1        |          |\n| 15k        |    90.37 |\n| \u00b1          |          |\n| 0.5        |          |\n| 93.36      |          |\n| \u00b1          |          |\n| 0.2        |          |\n| 93.32      |          |\n| \u00b1          |          |\n| 0.3        |          |\n| 16k        |    90.9  |\n| \u00b1          |          |\n| 0.4        |          |\n| 93.67      |          |\n| \u00b1          |          |\n| 0.2        |          |\n| 93.67      |          |\n| \u00b1          |          |\n| 0.2        |          |\n| 17k        |    91.06 |\n| \u00b1          |          |\n| 0.6        |          |\n| 93.89      |          |\n| \u00b1          |          |\n| 0.1        |          |\n| 93.97      |          |\n| \u00b1          |          |\n| 0.1        |          |\n| 18k        |    91.39 |\n| \u00b1          |          |\n| 0.5        |          |\n| 94.16      |          |\n| \u00b1          |          |\n| 0.1        |          |\n| 94.15      |          |\n| \u00b1          |          |\n| 0.0        |          |\n| 19k        |    91.6  |\n| \u00b1          |          |\n| 0.4        |          |\n| 94.37      |          |\n| \u00b1          |          |\n| 0.2        |          |\n| 94.28      |          |\n| \u00b1          |          |\n| 0.1        |          |\n| 20k        |    91.86 |\n| \u00b1          |          |\n| 0.4        |          |\n| 94.40      |          |\n| \u00b1          |          |\n| 0.1        |          |\n| 94.31      |          |\n| \u00b1          |          |\n| 0.1        |          |\n| #labeled   |   Random |\n|------------|----------|\n| 1k         |    49.98 |\n| \u00b1          |          |\n| 1.2        |          |\n| 49.98      |          |\n| \u00b1          |          |\n| 1.8        |          |\n| 49.98      |          |\n| \u00b1          |          |\n| 1.4        |          |\n| 2k         |          |\n| 65.10      |          |\n| \u00b1          |          |\n| 1.0        |          |\n| 65.70      |          |\n| \u00b1          |          |\n| 1.4        |          |\n| 58.72      |          |\n| \u00b1          |          |\n| 1.0        |          |\n| 3k         |    70.99 |\n| \u00b1          |          |\n| 1.0        |          |\n| 74.00      |          |\n| \u00b1          |          |\n| 0.8        |          |\n| 66.45      |          |\n| \u00b1          |          |\n| 0.7        |          |\n| 4k         |    78.2  |\n| \u00b1          |          |\n| 1.0        |          |\n| 80.59      |          |\n| \u00b1          |          |\n| 0.8        |          |\n| 72.93      |          |\n| \u00b1          |          |\n| 0.9        |          |\n| 5k         |          |\n| 79.19      |          |\n| \u00b1          |          |\n| 0.8        |          |\n| 82.79      |          |\n| \u00b1          |          |\n| 0.7        |          |\n| 75.63      |          |\n| \u00b1          |          |\n| 0.7        |          |\n| 6k         |    82.92 |\n| \u00b1          |          |\n| 0.7        |          |\n| 84.96      |          |\n| \u00b1          |          |\n| 0.7        |          |\n| 79.98      |          |\n| \u00b1          |          |\n| 0.6        |          |\n| 7k         |    84.88 |\n| \u00b1          |          |\n| 0.8        |          |\n| 87.31      |          |\n| \u00b1          |          |\n| 0.5        |          |\n| 81.32      |          |\n| \u00b1          |          |\n| 0.6        |          |\n| 8k         |    85.84 |\n| \u00b1          |          |\n| 0.6        |          |\n| 89.07      |          |\n| \u00b1          |          |\n| 0.4        |          |\n| 83.61      |          |\n| \u00b1          |          |\n| 0.5        |          |\n| 9k         |    87.08 |\n| \u00b1          |          |\n| 0.5        |          |\n| 89.66      |          |\n| \u00b1          |          |\n| 0.4        |          |\n| 85.04      |          |\n| \u00b1          |          |\n| 0.4        |          |\n| 10k        |          |\n| 87.28      |          |\n| \u00b1          |          |\n| 0.4        |          |\n| 90.36      |          |\n| \u00b1          |          |\n| 0.3        |          |\n| 86.04      |          |\n| \u00b1          |          |\n| 0.4        |          |\n| 11k        |    88.48 |\n| \u00b1          |          |\n| 0.5        |          |\n| 90.87      |          |\n| \u00b1          |          |\n| 0.4        |          |\n| 86.79      |          |\n| \u00b1          |          |\n| 0.4        |          |\n| 12k        |    88.58 |\n| \u00b1          |          |\n| 0.4        |          |\n| 91.78      |          |\n| \u00b1          |          |\n| 0.2        |          |\n| 87.83      |          |\n| \u00b1          |          |\n| 0.4        |          |\n| 13k        |    89.26 |\n| \u00b1          |          |\n| 0.4        |          |\n| 92.08      |          |\n| \u00b1          |          |\n| 0.3        |          |\n| 88.01      |          |\n| \u00b1          |          |\n| 0.4        |          |\n| 14k        |    89.85 |\n| \u00b1          |          |\n| 0.4        |          |\n| 92.7       |          |\n| \u00b1          |          |\n| 0.3        |          |\n| 89.23      |          |\n| \u00b1          |          |\n| 0.4        |          |\n| 15k        |    89.87 |\n| \u00b1          |          |\n| 0.4        |          |\n| 92.9       |          |\n| \u00b1          |          |\n| 0.3        |          |\n| 89.27      |          |\n| \u00b1          |          |\n| 0.4        |          |\n| 16k        |    90.17 |\n| \u00b1          |          |\n| 0.4        |          |\n| 93.36      |          |\n| \u00b1          |          |\n| 0.3        |          |\n| 90.35      |          |\n| \u00b1          |          |\n| 0.3        |          |\n| 17k        |          |\n| 90.69      |          |\n| \u00b1          |          |\n| 0.4        |          |\n| 93.56      |          |\n| \u00b1          |          |\n| 0.3        |          |\n| 90.47      |          |\n| \u00b1          |          |\n| 0.3        |          |\n| 18k        |    91.04 |\n| \u00b1          |          |\n| 0.3        |          |\n| 93.82      |          |\n| \u00b1          |          |\n| 0.2        |          |\n| 90.98      |          |\n| \u00b1          |          |\n| 0.3        |          |\n| 19k        |    90.99 |\n| \u00b1          |          |\n| 0.3        |          |\n| 93.94      |          |\n| \u00b1          |          |\n| 0.2        |          |\n| 91.63      |          |\n| \u00b1          |          |\n| 0.3        |          |\n| 20k        |          |\n| 91.28      |          |\n| \u00b1          |          |\n| 0.3        |          |\n| 93.95      |          |\n| \u00b1          |          |\n| 0.2        |          |\n| 91.53      |          |\n| \u00b1          |          |\n| 0.3        |          |\n| #labeled   |   Random |\n|------------|----------|\n| 1k         |    49.82 |\n| \u00b1          |          |\n| 1.1        |          |\n| 49.82      |          |\n| \u00b1          |          |\n| 1.2        |          |\n| 49.82      |          |\n| \u00b1          |          |\n| 1.3        |          |\n| 2k         |    64.43 |\n| \u00b1          |          |\n| 0.8        |          |\n| 65.18      |          |\n| \u00b1          |          |\n| 0.9        |          |\n| 51.89      |          |\n| \u00b1          |          |\n| 1.0        |          |\n| 3k         |    72.65 |\n| \u00b1          |          |\n| 0.7        |          |\n| 75.21      |          |\n| \u00b1          |          |\n| 0.7        |          |\n| 59.43      |          |\n| \u00b1          |          |\n| 0.7        |          |\n| 4k         |          |\n| 77.6       |          |\n| \u00b1          |          |\n| 0.7        |          |\n| 77.93      |          |\n| \u00b1          |          |\n| 0.7        |          |\n| 56.86      |          |\n| \u00b1          |          |\n| 0.7        |          |\n| 5k         |    81.03 |\n| \u00b1          |          |\n| 0.6        |          |\n| 82.76      |          |\n| \u00b1          |          |\n| 0.6        |          |\n| 55.78      |          |\n| \u00b1          |          |\n| 0.6        |          |\n| 6k         |          |\n| 83.00      |          |\n| \u00b1          |          |\n| 0.6        |          |\n| 85.30      |          |\n| \u00b1          |          |\n| 0.6        |          |\n| 62.61      |          |\n| \u00b1          |          |\n| 0.6        |          |\n| 7k         |    84.62 |\n| \u00b1          |          |\n| 0.5        |          |\n| 87.13      |          |\n| \u00b1          |          |\n| 0.5        |          |\n| 65.42      |          |\n| \u00b1          |          |\n| 0.5        |          |\n| 8k         |    85.72 |\n| \u00b1          |          |\n| 0.4        |          |\n| 88.33      |          |\n| \u00b1          |          |\n| 0.4        |          |\n| 67.20      |          |\n| \u00b1          |          |\n| 0.4        |          |\n| 9k         |    86.42 |\n| \u00b1          |          |\n| 0.4        |          |\n| 89.57      |          |\n| \u00b1          |          |\n| 0.4        |          |\n| 73.00      |          |\n| \u00b1          |          |\n| 0.4        |          |\n| 10k        |    87.15 |\n| \u00b1          |          |\n| 0.3        |          |\n| 90.76      |          |\n| \u00b1          |          |\n| 0.3        |          |\n| 74.99      |          |\n| \u00b1          |          |\n| 0.3        |          |\n| 11k        |    88.66 |\n| \u00b1          |          |\n| 0.3        |          |\n| 91.12      |          |\n| \u00b1          |          |\n| 0.3        |          |\n| 75.89      |          |\n| \u00b1          |          |\n| 0.3        |          |\n| 12k        |    88.7  |\n| \u00b1          |          |\n| 0.3        |          |\n| 91.99      |          |\n| \u00b1          |          |\n| 0.3        |          |\n| 79.26      |          |\n| \u00b1          |          |\n| 0.3        |          |\n| 13k        |    89    |\n| \u00b1          |          |\n| 0.3        |          |\n| 92.36      |          |\n| \u00b1          |          |\n| 0.3        |          |\n| 71.82      |          |\n| \u00b1          |          |\n| 0.3        |          |\n| 14k        |    89.55 |\n| \u00b1          |          |\n| 0.3        |          |\n| 92.68      |          |\n| \u00b1          |          |\n| 0.3        |          |\n| 80.90      |          |\n| \u00b1          |          |\n| 0.3        |          |\n| 15k        |    89.74 |\n| \u00b1          |          |\n| 0.3        |          |\n| 92.78      |          |\n| \u00b1          |          |\n| 0.3        |          |\n| 82.25      |          |\n| \u00b1          |          |\n| 0.3        |          |\n| 16k        |    90.15 |\n| \u00b1          |          |\n| 0.2        |          |\n| 92.94      |          |\n| \u00b1          |          |\n| 0.2        |          |\n| 82.36      |          |\n| \u00b1          |          |\n| 0.2        |          |\n| 17k        |    90.24 |\n| \u00b1          |          |\n| 0.3        |          |\n| 93.02      |          |\n| \u00b1          |          |\n| 0.3        |          |\n| 83.78      |          |\n| \u00b1          |          |\n| 0.3        |          |\n| 18k        |    91.13 |\n| \u00b1          |          |\n| 0.2        |          |\n| 93.67      |          |\n| \u00b1          |          |\n| 0.2        |          |\n| 83.60      |          |\n| \u00b1          |          |\n| 0.2        |          |\n| 19k        |    90.91 |\n| \u00b1          |          |\n| 0.2        |          |\n| 93.75      |          |\n| \u00b1          |          |\n| 0.2        |          |\n| 83.64      |          |\n| \u00b1          |          |\n| 0.2        |          |\n| 20k        |    91.69 |\n| \u00b1          |          |\n| 0.2        |          |\n| 93.92      |          |\n| \u00b1          |          |\n| 0.2        |          |\n| 83.99      |          |\n| \u00b1          |          |\n| 0.2        |          |\n| #labeled   |   Random |\n|------------|----------|\n| 2k         |    60.95 |\n| \u00b1          |          |\n| 0.2        |    61.23 |\n| \u00b1          |          |\n| 0.6        |    60.95 |\n| \u00b1          |          |\n| 0.6        |          |\n| 62.36      |          |\n| \u00b1          |          |\n| 0.5        |          |\n| 61.23      |          |\n| \u00b1          |          |\n| 0.4        |    60.89 |\n| \u00b1          |          |\n| 0.5        |          |\n| 3k         |    64.18 |\n| \u00b1          |          |\n| 0.2        |    64.57 |\n| \u00b1          |          |\n| 0.4        |    64.91 |\n| \u00b1          |          |\n| 0.5        |          |\n| 65.90      |          |\n| \u00b1          |          |\n| 0.5        |          |\n| 64.64      |          |\n| \u00b1          |          |\n| 0.3        |    64.24 |\n| \u00b1          |          |\n| 0.3        |          |\n| 4k         |          |\n| 66.39      |          |\n| \u00b1          |          |\n| 0.2        |    66.94 |\n| \u00b1          |          |\n| 0.2        |    66.9  |\n| \u00b1          |          |\n| 0.3        |          |\n| 67.63      |          |\n| \u00b1          |          |\n| 0.2        |          |\n| 66.92      |          |\n| \u00b1          |          |\n| 0.2        |    67.12 |\n| \u00b1          |          |\n| 0.2        |          |\n| 5k         |    67.46 |\n| \u00b1          |          |\n| 0.3        |    68.7  |\n| \u00b1          |          |\n| 0.2        |          |\n| 69.05      |          |\n| \u00b1          |          |\n| 0.2        |          |\n| 68.88      |          |\n| \u00b1          |          |\n| 0.2        |    68.12 |\n| \u00b1          |          |\n| 0.2        |    68.04 |\n| \u00b1          |          |\n| 0.2        |          |\n| 6k         |    68.58 |\n| \u00b1          |          |\n| 0.4        |    69.82 |\n| \u00b1          |          |\n| 0.3        |          |\n| 70.35      |          |\n| \u00b1          |          |\n| 0.2        |          |\n| 69.74      |          |\n| \u00b1          |          |\n| 0.2        |    68.95 |\n| \u00b1          |          |\n| 0.2        |    68.87 |\n| \u00b1          |          |\n| 0.2        |          |\n| 7k         |    69.17 |\n| \u00b1          |          |\n| 0.2        |    70.48 |\n| \u00b1          |          |\n| 0.2        |          |\n| 70.49      |          |\n| \u00b1          |          |\n| 0.2        |          |\n| 70.16      |          |\n| \u00b1          |          |\n| 0.2        |    69.43 |\n| \u00b1          |          |\n| 0.1        |    69.26 |\n| \u00b1          |          |\n| 0.2        |          |\n| #labeled   |   Random |\n|------------|----------|\n| 2k         |    63.12 |\n| \u00b1          |          |\n| 0.2        |          |\n| 63.18      |          |\n| \u00b1          |          |\n| 0.2        |          |\n| 63.15      |          |\n| \u00b1          |          |\n| 0.2        |          |\n| 3k         |    67.08 |\n| \u00b1          |          |\n| 0.1        |          |\n| 67.44      |          |\n| \u00b1          |          |\n| 0.1        |          |\n| 67.42      |          |\n| \u00b1          |          |\n| 0.1        |          |\n| 4k         |          |\n| 69.44      |          |\n| \u00b1          |          |\n| 0.1        |    70.22 |\n| \u00b1          |          |\n| 0.1        |          |\n| 70.42      |          |\n| \u00b1          |          |\n| 0.1        |          |\n| 5k         |    71.13 |\n| \u00b1          |          |\n| 0.2        |          |\n| 72.28      |          |\n| \u00b1          |          |\n| 0.1        |          |\n| 72.33      |          |\n| \u00b1          |          |\n| 0.1        |          |\n| 6k         |    72.18 |\n| \u00b1          |          |\n| 0.1        |          |\n| 73.56      |          |\n| \u00b1          |          |\n| 0.1        |          |\n| 73.28      |          |\n| \u00b1          |          |\n| 0.2        |          |\n| 7k         |          |\n| 73.10      |          |\n| \u00b1          |          |\n| 0.1        |    74.81 |\n| \u00b1          |          |\n| 0.1        |          |\n| 74.85      |          |\n| \u00b1          |          |\n| 0.1        |          |\n| #labeled   |   Random |\n|------------|----------|\n| 250        |    87.58 |\n| \u00b1          |          |\n| 0.2        |    88.85 |\n| \u00b1          |          |\n| 0.2        |          |\n| 89.97      |          |\n| \u00b1          |          |\n| 0.3        |          |\n| 500        |    90.37 |\n| \u00b1          |          |\n| 0.1        |    90.89 |\n| \u00b1          |          |\n| 0.2        |          |\n| 91.57      |          |\n| \u00b1          |          |\n| 0.2        |          |\n| 1k         |    91.76 |\n| \u00b1          |          |\n| 0.1        |    92.47 |\n| \u00b1          |          |\n| 0.2        |          |\n| 92.87      |          |\n| \u00b1          |          |\n| 0.1        |          |\n| 2k         |    92.78 |\n| \u00b1          |          |\n| 0.1        |    93.27 |\n| \u00b1          |          |\n| 0.2        |          |\n| 93.74      |          |\n| \u00b1          |          |\n| 0.2        |          |\n| 4k         |    93.38 |\n| \u00b1          |          |\n| 0.1        |    93.97 |\n| \u00b1          |          |\n| 0.1        |          |\n| 94.23      |          |\n| \u00b1          |          |\n| 0.1        |          |\n"
    }
]