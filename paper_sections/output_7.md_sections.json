[
    {
        "level": "#",
        "title": "Simba: Simplified Mamba-Based Architecture For Vision And Multivariate Time Series",
        "content": "\nBadri N. Patro1 and Vijay S, Agneeswaran1\nMicrosoft\n{badripatro,vagneeswaran}@microsoft.com Abstract. Transformers have widely adopted attention networks for sequence mixing and MLPs for channel mixing, playing a pivotal role in achieving breakthroughs across domains. However, recent literature highlights issues with attention networks, including low inductive bias and quadratic complexity concerning input sequence length. State Space Models (SSMs) like S4 and others (Hippo, Global Convolutions, liquid S4, LRU, Mega, and Mamba), have emerged to address the above issues to help handle longer sequence lengths. Mamba, while being the state-of-the-art SSM, has a stability issue when scaled to large networks for computer vision datasets. We propose SiMBA, a new architecture that introduces Einstein FFT (EinFFT) for channel modeling by specific eigenvalue computations and uses the Mamba block for sequence modeling. Extensive performance studies across image and time-series benchmarks demonstrate that SiMBA outperforms existing SSMs, bridging the performance gap with state-of-the-art transformers. Notably, SiMBA establishes itself as the new state-of-the-art SSM on ImageNet and transfer learning benchmarks such as Stanford Car and Flower as well as task learning benchmarks as well as seven time series benchmark datasets. The project page is available on this website https://github.com/badripatro/Simba Keywords: Transformer \u00b7 Mamba \u00b7 Spectral Channel Mixing \u00b7 State Space Model\n"
    },
    {
        "level": "##",
        "title": "1 Introduction",
        "content": "\nThe evolution of language models in the technological landscape is transitioning from Large Language Models (LLMs) to the paradigm of Small Language Models (SLMs)\ninspired by cutting-edge SLM architectures like Mistral [22], Phi [28], Orca [38], among others. At the heart of both LLMs and SLMs lies the power of transformers, where the layers are not only scaled but also exhibit scaling in both the token and channel modeling.\n\nThis has made transformers the building blocks of LLMs and SLMs.\n\nTransformers operate through two fundamental mixing directions: sequential modeling, involving the interaction of one token with another within the input sequence, and channel modeling, facilitating interactions within the channel dimension or across input features. Traditionally, multi-headed self-attention (MHSA) was employed by transformers for sequence modeling, but its computational complexity of \ud835\udc42(\ud835\udc412) posed inefficiencies and performance challenges, particularly for longer sequences. To address this limitation, and recognize the need for handling extended input sequences in domains like genomics or protein folding, a novel approach emerged with the introduction of the Structured State Space model (S4) [15]. This model leverages state-space-based sequential modeling, offering enhanced efficiency and performance for processing longer input sequences.\n\nHowever, S4 has been less effective in catering to modeling information-dense data, particularly in domains such as computer vision, and faces challenges in discrete scenarios like genomic data. In response to these limitations, several other SSMs, including Hippo [14], H3 [9], Hyena [48], RetNet [52], RWKV [47] and S4nd [39] and Gated State Spaces [37], have been proposed in the literature to address the shortcomings of S4, specifically focusing on enhancing its ability to handle long-range dependencies. Additionally, global convolutions-based [11] state-space models have been introduced as an alternative solution to mitigate issues related to long-range dependencies. Further expansions in the S4 model family include Liquid S4 models [18] and a variant known as Mega [36], which interprets S4 through an exponential moving average approach. These advancements collectively contribute to a more versatile and effective suite of models for diverse applications. Mamba [13] is a selective state space sequence modeling technique proposed recently to address difficulties typical state space models had in handling long sequences efficiently. The typical state space models have trouble propagating or forgetting information in long sequences. Mamba handles this difficulty, by incorporating the current token in the state space, achieving in-context learning. Mamba in general addresses the two concerns of transformers, namely its lack of inductive bias by using CNN and its quadratic complexity by using input-specific state space models.\n\nThe current instantiation of Mamba has stability issues i.e. the training loss is not converging while scaling to large-sized networks on the ImageNet dataset. It is not clear as to why Mamba has this instability while scaling to large networks. This leads to the problem of vanishing/exploding gradients commonly observed in Mamba in general. Existing literature, such as Oppenheim and Verghese's work [43], establishes that linear state space models exhibit stability when all eigenvalues of matrix A are negative real numbers. This motivates the need for a stable Mamba architecture, as presented in this paper, and specifically, we use Fourier Transforms followed by a learnable layer with non-linearity. SiMBA also introduces residual connections with dropouts, which help in handling solving the instability issues in Mamba, as illustrated in Figure-1. This strategic application of learnable Fourier Transforms aims to manipulate the eigenvalues, ensuring they are negative real numbers. Thus, The proposed channel modeling technique, named EinFFT, is a distinctive contribution to the field. This is a unique contribution, as SSMs in the literature have not addressed channel modeling explicitly.\n\n| Method               | Seqence Mixing     | Channel Mixing   | Models                        |\n|----------------------|--------------------|------------------|-------------------------------|\n| Convolution          | ConvNet            | MLP              | AlexNet, VGG, ResNet, RegNetY |\n| Transformer          | Attention          | MLP              | ViT, Deit, T2T, TnT           |\n| MLP-Mixer            | MLP                | MLP              | Mlp-Mixer, GMLP, DynamicMLP   |\n| Spectral-Mixer       | FFT/Wavelet        | MLP              | GFNet, AFNO,WaveMix           |\n| State Space          | SSM                | -                | S4, Hyena, Hippo, H3, Mamba   |\n| Conv-Transformer     | Attention+ ConvNet | MLP              | Swin, CvT, CMT, CSwin         |\n| Spectral-Transformer | FFT +Attention     | MLP              | SiMBA, SVT, WaveViT           |\n| SiMBA                | Mamba              | EinFFT           | SiMBA                         |\n\nOur architectural investigation explores diverse state space models, including S4\n[15], Hippo [14], Hyena [48], and Mamba [13], alongside attention models such as DeIT [57], ViT [7], and Swin [33], as well as spectral models like GFNet [50], and AFNO [16] for sequence modeling. Additionally, we explore various channel modeling alternatives, including MLP, and Monarch Mixer, and introduce a novel technique called EinFFT. Through extensive experimentation, we have identified the most efficient and streamlined state space architecture to date, named SiMBA. This novel architecture incorporates Mamba for sequence modeling and introduces EinFFT as a new channel modeling technique. SiMBA effectively addresses the instability issues observed in Mamba when scaling to large networks. The architectural alternatives explored for large-scale sequence modeling are depicted in Figure-1. Table-1 provides an overview of large vision models used for image recognition tasks, categorizing them based on their sequence mixing and channel mixing techniques. It highlights a diverse range of models, including those based on convolutional models, transformers models, MLP-mixers, spectralmixers models, and state space methods. Additionally, it introduces hybrid models combining convolution with transformers or spectral approaches. Lastly, it presents SiMBA, a novel model utilizing Mamba for sequence mixing and EinFFT for channel mixing.\n\nThe contribution of this paper is as follows:\n\n- EinFFT: A new technique for channel modeling known as EinFFT is proposed,\nwhich solves the stability issue in Mamba. This uses Fourier transforms with nonlinearity to model eigenvalues as negative real numbers, which solves instability\n[43]. We validate this technique on two data modalities time series and ImageNet\ndataset.\n- SiMBA: We propose an optimized Mamba architecture for computer vision tasks,\nknown as SiMBA which uses EinFFT for channel modeling and Mamba for token\nmixing to handle inductive bias and computational complexity. We also show the\n4\nB. N. Patro et al.\nimportance of different architectural elements of SiMBA with ablation studies including residual connections and dropouts.\n\n- Performance Gap: It must be noted that SiMBA is the first SSM to close the performance gap with state-of-the-art attention-based transformers on the ImageNet dataset and six standard time series datasets. We show with extensive performance analysis how SiMBA achieves state-of-art performance compared to V-Mamba and Vision Mamba in the vision domain. We have shown the generalization capability of SiMBA architecture in other domains like time series to handle long sequences.\n- We show the performance of SiMBA on transfer learning datasets like CIFAR, Stanford Car, and Flower. We also show that SiMBA achieves comparable performance even in other tasks such as instance segmentation with the MS COCO dataset.\n"
    },
    {
        "level": "##",
        "title": "2 Related Work",
        "content": "\nAt the heart of SiMBA's transformer-based architecture lies a combination of multiheaded self-attention and a multi-layer perceptron (MLP). When comparing SiMBA with attention-based transformers like ViT [7], DeIT [57], Swin [34], CSwin [6], and CVT [64], it becomes evident that ViT, although pioneering attention-based transformers in computer vision, faced challenges due to its quadratic attention complexity (\ud835\udc42(\ud835\udc412)).\n\nSubsequent models like DeIT, Swin, TvT, CVT, and CSwin aimed at refining transformer performance in computer vision and specific NLP tasks. Alternatively, MLP Mixer architectures such as MLP Mixers [55] and DynamicMLP [62] sought to replace attention networks with MLP to address these challenges. Efforts to mitigate attention complexity led to the development of transformers like GFNet [50], AFNO [16], FNet [25], and FourierFormer [40], incorporating Fourier transforms to reduce complexity to \ud835\udc42(\ud835\udc41 log(\ud835\udc41)). Despite this reduction, these models demonstrated a performance gap compared to state-of-the-art attention-based transformers. The emergence of transformers like SVT [46], WaveViT [69], and SpectFormer [45], combining initial spectral layers with deeper attention mechanisms, surpassed the performance of both attention-based and MLP-based transformers. This comprehensive comparative analysis highlights SiMBA's distinctive position in the evolving landscape of transformer architectures.\n\nAttention-based transformers encounter limitations in modeling long input sequences, especially when dependencies extend beyond the attention window size. This constraint proves crucial in applications such as high-resolution imagery analysis and genomics. S4\npioneered state space models to address this issue by reducing complexity to \ud835\udc42(\ud835\udc41 log(\ud835\udc41)), enabling the handling of long-range dependencies in tasks like the long-range arena path-X [54]. Subsequent efforts, including Hippo and Long Convolutions [10], aimed to enhance state space models' efficiency but demonstrated a performance gap compared to state-of-the-art transformers. Hyena [48], H3 [9], and related models further improved state space models' effectiveness but faced challenges due to a lack of consideration for the current input. Mamba attempted to bridge this gap by parameterizing the current input in the state space model. However, state-space models often encounter instability issues due to their sensitivity to recurrent dynamics.\n\nVision Mamba [76] and V-Mamba [32] adapted the Mamba architecture for computer vision tasks, utilizing bi-directional and visual state space models. However, the performance study section reveals a performance gap between Vision Mamba, V-Mamba, and state-of-the-art transformer models like SpectFormer [45], SVT [46], WaveViT [69], and Volo [72]. SiMBA addresses this gap by incorporating Mamba for token mixing, replacing attention networks, and leveraging Einstein FFT (EinFFT) for channel mixing. SiMBA introduces the Einstein blending method for channel mixing, offering a novel approach without the constraints of requiring perfect square dimensions for sequence length N and channel dimensions. Furthermore, SiMBA adopts the pyramid version of the transformer architecture, providing a significant performance boost compared to vanilla state space models. While many state space models reduce complexity to \ud835\udc42(\ud835\udc41 log(\ud835\udc41)), they often fall short of achieving the performance levels seen in stateof-the-art attention-based transformers.\n"
    },
    {
        "level": "##",
        "title": "3 Method",
        "content": "\nIn this study, we introduce EinFFT, a novel approach for frequency-domain channel mixing utilizing Einstein Matrix multiplication. EinFFT is specifically designed for complex number representations of frequency components, enabling the effective capture of key patterns in Image patch data with a global view and energy compaction. It must be noted that EinFFT is also applicable for other sequence data modalities like time series or speech or even text data. We have validated EinFFT-based SiMBA for image and time series benchmark datasets.\n"
    },
    {
        "level": "##",
        "title": "3.1 Channel Mixing: Einstein Fft (Einfft)",
        "content": "\nThe Channel Mixing component of SiMBA (EinFFT) encompasses three main components: Spectral Transformation, Spectral Gating Network using Einstein Matrix multiplication, and Inverse Spectral Transformation as depicted in Figure 1.\n\nSpectral Transformation For a given input function \ud835\udc1f(\ud835\udc65) and its corresponding frequency domain conversion function \ue232(\ud835\udc58), by using Discrete Fourier Transform (DFT).\n\nLet \ue232 denote the Fourier transform of a function \ud835\udc53(\ud835\udc65) and \ue232\u22121 its inverse then\n\n\ue232(\ud835\udc58) = \u222b\ud835\udc37\n       \ud835\udc53(\ud835\udc65)\ud835\udc52\u2212\ud835\udc572\ud835\udf0b\ud835\udc58\ud835\udc65d\ud835\udc65 = \u222b\ud835\udc37\n                    \ud835\udc53(\ud835\udc65) cos(2\ud835\udf0b\ud835\udc58\ud835\udc65)d\ud835\udc65 + \ud835\udc57 \u222b\ud835\udc37\n                                    \ud835\udc53(\ud835\udc65) sin(2\ud835\udf0b\ud835\udc58\ud835\udc65)d\ud835\udc65\n                                                 (1)\n\nHere, \ud835\udc58 is the frequency variable, \ud835\udc65 is the spatial variable, and \ud835\udc57 is the imaginary unit.\n\nThe real part of \ue232 is denoted as \ud835\udc45\ud835\udc52(\ue232), and the imaginary part as \ud835\udc3c\ud835\udc5a(\ue232). The complete conversion is expressed as \ue232 = \ud835\udc45\ud835\udc52(\ue232) + *\ud835\udc57\ud835\udc3c\ud835\udc5a*(\ue232). Fourier transform is employed to decompose the input signal into its constituent frequencies. This allows the identification of periodic or aperiodic patterns which are crucial for image recognition or detection tasks.\n\nConvolution Theorem 1 The convolution theorem states that the Fourier transform of the convolution of two functions in the spatial domain is equal to the pointwise product of their Fourier transforms. Mathematically, if \ud835\udc53(\ud835\udc65) and \ud835\udc54(\ud835\udc65) are two functions in the\n\n6\nB. N. Patro et al.\nspatial domain, and $F(u)$ and $G(u)$ are their respective Fourier transforms, then the convolution theorem can be expressed as:_\n\n$$F\\{f\\,*\\,g\\}(u)=F(u)\\cdot G(u)$$\n\n_where:$F$ denotes the Fourier transform operator, $*$ denotes the convolution operator._\n\u221e\n\u221e\nRayleigh's Theorem 1 Rayleigh's Theorem states that the total energy (or power) of a signal in the spatial domain is equal to the total energy in its frequency domain representation. Rayleigh's Theorem for continuous signals, and is used to express the equivalence of energy between an image patch in the spatial domain (|\ud835\udc20(\ud835\udc65)|2) and its representation in the frequency domain (|\ue233(\ud835\udc53)|2) as defined as:\n\u222b\n\u2212\u221e\n|\ud835\udc20(\ud835\udc65)|2 \ud835\udc51\ud835\udc65 = \u222b\n\u2212\u221e\n|\ue233(\ud835\udc53)|2 \ud835\udc51\ud835\udc53\nHere, \ue233(\ud835\udc53) = \u222b \u221e\n\u2212\u221e \ud835\udc20(\ud835\udc65)\ud835\udc52\u2212\ud835\udc572\ud835\udf0b\ud835\udc53\ud835\udc65d\ud835\udc65, where \ud835\udc65 represents the channel dimension, and \ud835\udc53\ndenotes the frequency dimension. Rayleigh's Theorem describes the conservation of energy between the spatial domain and the frequency domain. It indicates that the integral of the squared magnitude of the signal in the time domain is equal to the integral of the squared magnitude of its Fourier transform in the frequency domain.\n\nThe implications of this theorem, emphasize that if the majority of the energy of an input image is concentrated in a limited number of frequency components, then an accurate representation of the image can be achieved by focusing on those specific components. This is a key insight from the theorem, emphasizing the concentration of energy in certain frequency components and the potential benefits of selectively considering those components for more efficient and informative representations in the frequency domain.\n\nFrequency-domain Channel MixingFor a given input $\\mathbf{X}\\in\\mathbb{R}^{N\\times D}$ and its corresponding frequency domain conversion $\\mathcal{X}\\in\\mathbb{R}^{N\\times D}$, by using Fourier transform. The convolution theorem states that the Fourier transform of the convolutions on $\\mathbf{X}$ with its kernel in the spatial domain can be equivalently represented as the product operations of their frequency-domain representations. This equivalence is expressed as follows:\n\n$$\\mathcal{F}(\\mathbf{X}*W+B)=\\mathcal{F}(\\mathbf{X})\\cdot\\mathcal{F}(W)+\\mathcal{F}(B)=\\mathcal{X}\\mathcal{W}+B\\tag{2}$$\n\nwhere $*$ denotes circular convolution, $\\mathcal{W}$ and $B$ represent the complex number weight and bias in the frequency domain, while $W$ and $B$ denote the weight and bias in the spatial domain, and $\\mathcal{F}$ signifies the Discrete Fourier Transform (DFT). The output DFT operation is a complex number value $\\mathcal{X}\\in\\mathbb{R}^{N\\times D}$, multiplied with a complex number weight matrix $\\mathcal{W}\\in\\mathbb{R}^{D\\times D}$ and added with a complex number bias $B\\in\\mathbb{R}^{D}$ using the Spectral Gating Network (SGD). To reduce complexity we perform Einstein Matrix Multiplication (EMM) to have an efficient block diagonal matrix. The SGN is expressed by the following formulation:\n\n$$h^{\\ell}=\\sigma(h^{\\ell-1}\\mathcal{W}^{\\ell}+B^{\\ell}),h^{0}=\\mathcal{X}\\tag{3}$$\nHere, \u210e\ud835\udcc1 \u2208 \u211d\ud835\udc41\u00d7\ud835\udc37 represents the final output, \ud835\udcc1 denotes the \ud835\udcc1-th layer, and \ud835\udf0e is the activation function. Considering both \ue244 and \ue243 as complex numbers, we extend Equation (3) by employing the multiplication rule for complex numbers. The extended formulation is as follows:\n\n(4) \ud835\udc45\ud835\udc52(\u210e)\ud835\udcc1 = *EMM*(\ud835\udc45\ud835\udc52(\u210e\ud835\udcc1\u22121)\ue243\ud835\udcc1 \ud835\udc5f ) \u2212 *EMM*(\ud835\udc3c\ud835\udc5a(\u210e\ud835\udcc1\u22121)\ue243\ud835\udcc1 \ud835\udc56 ) + \ue22e\ud835\udcc1 \ud835\udc5f \ud835\udc3c\ud835\udc5a(\u210e)\ud835\udcc1 = *EMM*(\ud835\udc45\ud835\udc52(\u210e\ud835\udcc1\u22121)\ue243\ud835\udcc1 \ud835\udc56 ) + *EMM*(\ud835\udc3c\ud835\udc5a(\u210e\ud835\udcc1\u22121)\ue243\ud835\udcc1 \ud835\udc5f ) + \ue22e\ud835\udcc1 \ud835\udc56 \u210e\ud835\udcc1 = \ud835\udf0e(\ud835\udc45\ud835\udc52(\u210e)\ud835\udcc1) + \ud835\udc57\ud835\udf0e(\ud835\udc3c\ud835\udc5a(\u210e)\ud835\udcc1)\n\n   Here, \ue243\ud835\udcc1 = \ue243\ud835\udcc1\n                 \ud835\udc5f +\ud835\udc57\ue243\ud835\udcc1\n                        \ud835\udc56 and \ue22e\ud835\udcc1 = \ue22e\ud835\udcc1\n                                     \ud835\udc5f +\ud835\udc57\ue22e\ud835\udcc1\n                                          \ud835\udc56 represent the real and imaginary parts,\nrespectively. The implications of this theorem, emphasize that if the energy of an image\npatch is concentrated in a limited number of frequency components, then with the help\nof Rayleigh's Theorem a representation focusing on those specific frequency compo-\nnents can accurately capture the signal. this can be modeled with the help of a learnable\nspectral gating mechanism. We also apply a non-linear activation function to handle\nstability in the sequence modeling block (Mamba) by allowing a specific Eigenvalue\nthat is required for convergence. We then apply EMM between input \ud835\udc08 and the weight\n\ud835\udc4a \u2208 \u211d\ud835\udc36\ud835\udc4f\u00d7\ud835\udc36\ud835\udc51\u00d7\ud835\udc36\ud835\udc51 along the channel dimensions, where the channels are arranged in\nnumber of blocks and number of subchannels in each block to make it block diagonal\nmatrix ( \ud835\udc08 from \u211d\ud835\udc41\u00d7\ud835\udc36 to \u211d\ud835\udc41\u00d7\ud835\udc36\ud835\udc4f\u00d7\ud835\udc36\ud835\udc51, where \ud835\udc36 = \ud835\udc36\ud835\udc4f \u00d7 \ud835\udc36\ud835\udc51, and \ud835\udc4f << \ud835\udc51). This results in\na blended feature tensor \ud835\udc4c \u2208 \u211d\ud835\udc41\u00d7\ud835\udc36\ud835\udc4f\u00d7\ud835\udc36\ud835\udc51 as defined where \ud835\udc41 = \ud835\udc3b \u00d7 \ud835\udc4a . The formula\nfor EMM is:\n                      \ud835\udc18\ud835\udc41\u00d7\ud835\udc36\ud835\udc4f\u00d7\ud835\udc36\ud835\udc51 = \ud835\udc08\ud835\udc41\u00d7\ud835\udc36\ud835\udc4f\u00d7\ud835\udc36\ud835\udc51 \u29c6 \ud835\udc16\ud835\udc36\ud835\udc4f\u00d7\ud835\udc36\ud835\udc51\u00d7\ud835\udc36\ud835\udc51\n\nWhere \u29c6 represents an Einstein matrix multiplication\n  The channel mixing in the frequency domain, denoted as EinFFT, is performed by\nseparately computing the real and imaginary parts of frequency components. Subse-\nquently, these parts are stacked to form a complex number, with the final result obtained\nby concatenating the real and imaginary parts. This methodology enhances the extrac-\ntion of patterns while ensuring efficient utilization of computational resources.\n\nInverse Spectral Transformation : After learning in the frequency domain, \ue232 can be\nconverted back into the time domain using the inverse conversion formulation:\n\n$$f(x)=\\int_{D}F(k)e^{j2\\pi kx}\\mathrm{d}f=\\int_{D}(Re(F(k))+jIm(F(k))e^{j2\\pi kx}\\mathrm{d}f\\tag{5}$$\n\nThe frequency spectrum, expressed as a combination of cos and sin waves, aids in discerning prominent frequencies and periodic patterns in time series signals. The terms FFT and IFFT are used for brevity. The Spectral Transformation stage leverages the Fourier transform for enhanced signal analysis, capturing important periodic patterns crucial for the Image classification task.\n\nEinFFT Architecture for SiMBA Our method, EinFFT, leverages frequency-domain channel mixing through the application of Einstein Matrix multiplication on complex\n\n8\nB. N. Patro et al.\nnumber representations. This enables the extraction of intricate data patterns with enhanced global visibility and energy concentration.\n\nConsidering channel dependencies is crucial for accurate class prediction as it allows the model to capture interactions and correlations between different variables. The frequency channel learner facilitates communication between different channels, and learning channel dependencies by sharing the same weights across \ud835\udc3f timestamps. For the \ud835\udc59-th timestamp \ud835\udc17\u2236,(\ud835\udc59)\n\ud835\udc50\n\u2208 \u211d\ud835\udc41\u00d7\ud835\udc51, the frequency channel learner operates as follows:\n\n$$\\begin{split}\\mathcal{X}^{:,(I)}&=\\mathcal{X}_{R}^{:,(I)}+j\\mathcal{X}_{I}^{:,(I)}=\\text{FFT}(\\mathbf{X}_{c}^{:,(I)})\\\\ Re(h^{e})+jIm(h^{e})=\\text{EMM}(\\mathcal{X}^{:,(I)},\\mathcal{W}^{c},B^{e})\\text{as in eq-4}\\\\ \\mathcal{Y}^{:,(I)}&=\\mathcal{Y}_{R}^{:,(I)}+j\\mathcal{Y}_{I}^{:,(I)}=\\sigma(Re(h^{e})^{f})+j\\sigma(Im(h)^{e})\\\\ \\mathcal{Z}^{:,(I)}&=\\mathcal{Z}_{R}^{:,(I)}+j\\mathcal{Z}_{I}^{:,(I)}=\\text{EMM}(\\mathcal{Y}^{:,(I)},\\mathcal{W}^{c},B^{e})\\text{as in eq-4}\\\\ &\\mathbf{Z}_{c}^{:,(I)}=\\text{IFTFT}(\\mathcal{Z}_{R}^{:,(I)}+j\\mathcal{Z}_{I}^{:,(I)})\\end{split}\\tag{6}$$\n\nHere, $\\mathcal{X}^{:,(I)}\\in\\mathbb{R}^{\\frac{n}{2}\\times d}$ represents the frequency components of $\\mathbf{X}_{c}^{:,(I)}$. The operations FFT and IFFT are performed along the channel dimension. EinfFT refers to the frequency-domain channel Mixing introduced, taking $\\mathcal{W}^{c}$ and $B^{e}$ as the complex num\n\nber weight matrix and biases, respectively. The output \ue246\u2236,(\ud835\udc59) \u2208 \u211d\n                                                                 \ud835\udc41\n                                                                  2 \u00d7\ud835\udc51 of the EinFFT\nis then converted back to the time domain as \ud835\udc19\u2236,(\ud835\udc59) \u2208 \u211d\ud835\udc41\u00d7\ud835\udc51. Finally, the outputs \ud835\udc19\u2236,(\ud835\udc59)\n\nacross \ud835\udc3f timestamps are ensembled to produce the overall output \ud835\udc19\ud835\udc61 \u2208 \u211d\ud835\udc41\u00d7\ud835\udc3f\u00d7\ud835\udc51.\n"
    },
    {
        "level": "##",
        "title": "3.2 Sequence Modeling: Mamba Based Ssm",
        "content": "\nTo model a large sequence we use state space models instead of Multi-headed selfattention due to its complexity. The state space model [13, 15] is commonly known as a linear time-invariant system that map the input stimulation \ud835\udc65(\ud835\udc61) \u2208 \ue23e\ud835\udc3f to a response \ud835\udc66(\ud835\udc61) through a hidden space \u210e(\ud835\udc61) \u2208 \ue23e\ud835\udc41. Structured state space sequence models (S4)\nare a recent class of sequence models for deep learning that are broadly related to RNNs, CNNs, and classical state space models. Mathematically, The Continuous-time Latent State spaces can be modeled as linear ordinary differential equations that use evolution parameter \ud835\udc34 \u2208 \ue23e\ud835\udc41\u00d7\ud835\udc41 and projection parameter \ud835\udc35 \u2208 \ue23e\ud835\udc41\u00d71 and \ud835\udc36 \u2208 \ue23e\ud835\udc41\u00d71 as follows:\n\n$x^{\\prime}(t)=Ax(t)+Bu(t)$\n\n$y(t)=Cx(t)+Du(t)$\nDiscrete-time SSM:\n. The discrete form of SSM uses a time-scale parameter \u0394 to transform continuous parameters A, B, and C to discrete parameters \u0304\ud835\udc34, \u0304\ud835\udc35 and \u0304\ud835\udc36 using fixed formula \u0304\ud835\udc34 = \ud835\udc53\ud835\udc34(\u0394, \ud835\udc34), \u0304\ud835\udc35 = \ud835\udc53\ud835\udc35(\u0394*, \ud835\udc34, \ud835\udc35*). The pair \ud835\udc53\ud835\udc34, \ud835\udc53\ud835\udc35 is the discretization rule that uses a zero-order hold (ZOH) for this transformation. The equations are as follows.\n\n$$x_{k}=\\overline{\\mathbf{A}}x_{k-1}+\\overline{\\mathbf{B}}u_{k}\\overline{\\mathbf{A}}=(\\mathbf{I}-\\Delta/2\\cdot\\mathbf{A})^{-1}(\\mathbf{I}+\\Delta/2\\cdot\\mathbf{A})\\tag{8}$$ $$y_{k}=\\overline{\\mathbf{C}}x_{k}\\overline{\\mathbf{B}}=(\\mathbf{I}-\\Delta/2\\cdot\\mathbf{A})^{-1}\\Delta\\mathbf{B}\\overline{\\mathbf{C}}=\\mathbf{C}.$$\nConvolutional Kernel Representation The discretized form of recurrent SSM in equation-8 is not practically trainable due to its sequential nature. To get efficient representation, we model continuous convolution as discrete convolution as it is a linear time-invariant system. For simplicity let the initial state be \ud835\udc65\u22121 = 0. Then unrolling (8)\nexplicitly yields:\n\n$$x_{0}=\\overline{B}u_{0}x_{1}=\\overline{A}\\overline{B}u_{0}+\\overline{B}u_{1}x_{2}=\\overline{A}^{2}\\overline{B}u_{0}+\\overline{A}\\overline{B}u_{1}+\\overline{B}u_{2}\\ldots$$ $$y_{0}=\\overline{C}\\overline{B}u_{0}y_{1}=\\overline{C}\\overline{A}\\overline{B}u_{0}+\\overline{C}\\overline{B}u_{1}y_{2}=\\overline{C}\\overline{A}^{2}\\overline{B}u_{0}+\\overline{C}\\overline{A}\\overline{B}u_{1}+\\overline{C}\\overline{B}u_{2}\\ldots$$\nThis can be vectorized into a convolution (9) with an explicit formula for the convolution kernel (10).\n\n\ud835\udc66\ud835\udc58 = \ud835\udc6a\ud835\udc68 \ud835\udc58\ud835\udc69\ud835\udc620 + \ud835\udc6a\ud835\udc68 \ud835\udc58\u22121\ud835\udc69\ud835\udc621 + \u22ef + **\ud835\udc6a\ud835\udc68\ud835\udc69**\ud835\udc62\ud835\udc58\u22121 + \ud835\udc6a\ud835\udc69\ud835\udc62\ud835\udc58 (9) \ud835\udc66 = \ud835\udc72 \u2217 \ud835\udc62. \ud835\udc56\u2208[\ud835\udc3f] = (\ud835\udc6a\ud835\udc69, \ud835\udc6a\ud835\udc68\ud835\udc69, \u2026 , \ud835\udc6a\ud835\udc68 \ud835\udc3f\u22121\ud835\udc69). (10) \ud835\udc72 \u2208 \u211d\ud835\udc3f \u2236= \ue237\ud835\udc3f(\ud835\udc68, \ud835\udc69, \ud835\udc6a) \u2236= ( \ud835\udc6a\ud835\udc68 \ud835\udc56\ud835\udc69 )\n\ud835\udc72 in equation (9) can be represented as a single (non-circular) convolution which can be computed very efficiently with FFTs.However, computing \ud835\udc72 in (10) is non-trivial and is modelled as a \ud835\udc72 the **SSM convolution kernel** or filter.\n\nSpecifically, we model input sequence using the state-of-the-art state space model Mamba [13]. Mamba identifies a critical weakness in existing models: their inability to perform content-based reasoning. To address this, Mamba introduces selective state spaces (SSMs) that allow the model to selectively propagate or forget information along the sequence length dimension based on the current token. While we apply the Mamba block for the vision task, we face the problem of stability issues (loss convergence issue) compared to other models like S4 or Hippo. We are providing one type of solution for the instability issue by preserving only negative eigenvalue. To perform this we need an extra module which we call the channel mixing, which was missing in the mamba block. We combine the channel mixing module with the Sequence mixing module and make a Simplified Mamba Based Architecture (SiMBA) as shown in the figure-1. The input token sequence \ud835\udc17\ud835\ude95\u22121 is first normalized by the normalization layer. Next, we linearly project the normalized sequence to the \ud835\udc31 and \ud835\udc33 with dimension size \ud835\udc38. Then, we process the \ud835\udc31 from the forward and backward directions. For each direction, we first apply the 1-D convolution to the \ud835\udc31 and get the \ud835\udc31\u2032\n\ud835\udc5c. We then linearly project the \ud835\udc31\u2032\n\ud835\udc5c to the \ud835\udc01\ud835\udc5c, \ud835\udc02\ud835\udc5c,\n\ud835\udeab\ud835\udc5c, respectively. The \ud835\udeab\ud835\udc5c is then used to transform the \ud835\udc00\ud835\udc5c, \ud835\udc01\ud835\udc5c, respectively.\n\n\ud835\udc12\ud835\udc22\ud835\udc0c\ud835\udc01\ud835\udc00 uses Mamba block for sequence modeling, \ud835\ude7b denotes the number of layers, and **\ud835\udc0d\ud835\udc28\ud835\udc2b\ud835\udc26** stands for the normalization layer and EinFFT for channel modeling. The process involves applying the Mamba block to the previous time step \ud835\udc17\ud835\ude95\u22121, followed by a residual connection and dropout (\ud835\udc03\ud835\udc0f). The resulting tensor is then normalized (**\ud835\udc0d\ud835\udc28\ud835\udc2b\ud835\udc26**) to obtain the final sequence vector. The subsequent operation involves applying EinFFT, which is the proposed frequency-domain channel mixing operation. Finally, the tensor undergoes another dropout and is added to the previous state \ud835\udc17\ud835\udc59.\n\n10\nB. N. Patro et al.\n\nSiMBA illustrates an important trade-off between performance and scalability. Mamba by itself may have stability issues for large networks. Mamba combined with MLP for channel mixing bridges the performance gap for small-scale networks, but may have the same stability issues for large networks. Mamba combined with EinFFT solves stability issues for both small-scale and large networks. There may still be a performance gap with state-of-the-art transformers for large networks, which we shall address in future work.\n"
    },
    {
        "level": "##",
        "title": "4 Experiment",
        "content": "\nWe conducted a comprehensive evaluation of SiMBA on key computer vision tasks, including image recognition, and instance segmentation as well as on other data modalities such as time series. Our assessments for SiMBA model on standard datasets involved the following: a) Training and evaluating ImageNet1K [5] from scratch for the image recognition task. b) Ablation analysis comparing SiMBA with various state space architectures for sequence modeling and various architectures for channel modeling. c) Evaluation of SiMBA on other data modalities such as time series datasets - namely the multi-variate time series benchmark [66]. d) Transfer learning on CIFAR-10 [24], CIFAR-100 [24], Stanford Cars [23], and Flowers-102 [42] for image recognition. e) Fine-tuning SiMBA for downstream instance segmentation tasks. All experiments were conducted on a hierarchical transformer architecture, currently a state-of-the-art model, with an image size of 224 \u00d7 224 \u00d7 3.\n"
    },
    {
        "level": "##",
        "title": "4.1 Sota Comparison On Imagenet 1K",
        "content": "\nWe conducted performance evaluations on the ImageNet 1K dataset, comprising 1.2 million training images and 50,000 validation images distributed across 1000 categories. Our results, presented in Table 2, categorize architectures based on their size: small (<5 GFlops), base (5-10 GFlops), and large (>10 GFlops). In the small category, SiMBA(MLP) demonstrates remarkable performance with an 84.0% top-1 accuracy, surpassing prominent convolutional networks like ResNet-101 and ResNet-152, as well as leading transformers such as EffNet, ViT, Swin, and DeIT. Our comparison extends to state space models (SSMs) like S4ND, HyenaViT [48], VMambaa [32], and Vim [76], where SiMBA\nnot only outperforms them significantly but also maintains comparable complexity. Specifically, SiMBA-S(EinFFT) and SiMBA-S(MLP) stand out among other small models, including SSMs, small convolutional networks, and transformers, achieving accuracies of 82.7% and 84.0%, respectively. Furthermore, it surpasses competitor transformers like Wave-ViT-S, Max-ViT-T, and iFormer-S with lower GFlops and parameters while being on par with SpectFormer. Notably, SiMBA-S outperforms SSMs like Vim-T (76.1%)\nand V-Mamba-T (82.2%). We compare among our variants such as SiMBA(EinFFT), SiMBA(Monarch), and SiMBA(MLP), where Mamba is used for sequence modeling, and EinFFT, Monarch Mixing, and standard MLP are used for channel modeling, respectively. Although there is a slight difference in performance between SiMBA with EinFFT as a channel mixer compared to using MLP as a channel mixer, the MLP version demonstrates superior performance in terms of top-1 accuracy. However, it is noted\n\n| Method                  | Image Size #Param. FLOPs Top-1 acc.   |\n|-------------------------|---------------------------------------|\n| Convnets                |                                       |\n| ResNet-101              |                                       |\n| 224                     |                                       |\n| 2                       |                                       |\n| 45M                     | -                                     |\n| RegNetY-8G [            | 49                                    |\n| 2                       |                                       |\n| 39M                     | 8.0G                                  |\n| ResNet-152              |                                       |\n| 224                     |                                       |\n| 2                       |                                       |\n| 60M                     | -                                     |\n| RegNetY-16G [           | 49                                    |\n| 2                       |                                       |\n| 84M                     | 16.0G                                 |\n| Transformers            |                                       |\n| DeiT-S [                | 57                                    |\n| 2                       |                                       |\n| 22M                     | 4.6G                                  |\n| Swin-T [                | 33                                    |\n| 2                       |                                       |\n| 29M                     | 4.5G                                  |\n| EffNet-B4 [             | 27                                    |\n| 2                       |                                       |\n| 19M                     | 4.2G                                  |\n| WaveViT-H-S             |                                       |\n| \u22c6                       |                                       |\n|                         |                                       |\n| [                       | 69                                    |\n| 2                       |                                       |\n| 22.7M                   | 4.1G                                  |\n| SVT-H-S                 |                                       |\n| \u22c6                       |                                       |\n|                         |                                       |\n| [                       | 46                                    |\n| 2                       |                                       |\n| 22M                     | 3.9G                                  |\n| EffNet-B5 [             | 27                                    |\n| 2                       |                                       |\n| 30M                     | 9.9G                                  |\n| Swin-S [                | 33                                    |\n| 2                       |                                       |\n| 50M                     | 8.7G                                  |\n| CMT-B [                 | 17                                    |\n| 2                       |                                       |\n| 45M                     | 9.3G                                  |\n| MaxViT-S [              | 58                                    |\n| 2                       |                                       |\n| 69M                     | 11.7G                                 |\n| iFormer-B [             | 51                                    |\n| 2                       |                                       |\n| 48M                     | 9.4G                                  |\n| Wave-ViT-B              |                                       |\n| \u22c6                       |                                       |\n|                         |                                       |\n| [                       | 69                                    |\n| 2                       |                                       |\n| 33M                     | 7.2G                                  |\n| SVT-H-B                 |                                       |\n| \u22c6                       |                                       |\n|                         |                                       |\n| [                       | 46                                    |\n| 2                       |                                       |\n| 32.8M                   | 6.3G                                  |\n| ViT-b + Monarch [       | 8                                     |\n| 2                       |                                       |\n| 33M                     | -                                     |\n| M2-ViT-b [              | 8                                     |\n| 2                       |                                       |\n| 45M                     | -                                     |\n| DeiT-B [                | 57                                    |\n| 2                       |                                       |\n| 86M                     | 17.5G                                 |\n| Swin-B [                | 33                                    |\n| 2                       |                                       |\n| 88M                     | 15.4G                                 |\n| M2-Swin-B [             | 8                                     |\n| 224                     |                                       |\n| 2                       |                                       |\n| 50M                     | -                                     |\n| EffNet-B6 [             | 27                                    |\n| 2                       |                                       |\n| 43M                     | 19.0G                                 |\n| MaxViT-B [              | 58                                    |\n| 2                       |                                       |\n| 120M                    | 23.4G                                 |\n| VOLO-D2                 |                                       |\n| \u22c6                       |                                       |\n|                         |                                       |\n| [                       | 72                                    |\n| 2                       |                                       |\n| 58M                     | 14.1G                                 |\n| VOLO-D3                 |                                       |\n| \u22c6                       |                                       |\n|                         |                                       |\n| [                       | 72                                    |\n| 2                       |                                       |\n| 86M                     | 20.6G                                 |\n| Wave-ViT-L              |                                       |\n| \u22c6                       |                                       |\n|                         |                                       |\n| [                       | 69                                    |\n| 2                       |                                       |\n| 57M                     | 14.8G                                 |\n| SVT-H-L                 |                                       |\n| \u22c6                       |                                       |\n|                         |                                       |\n| [                       | 46                                    |\n| 2                       |                                       |\n| 54.0M                   | 12.7G                                 |\n| SSMs                    |                                       |\n| Vim-Ti [                | 76                                    |\n| 224                     |                                       |\n| 2                       |                                       |\n| 7M                      | -                                     |\n| VMamba-T [              | 32                                    |\n| 2                       |                                       |\n| 22M                     | 5.6G                                  |\n| SiMBA-S(Monarch) (Ours) | 224                                   |\n| 2                       |                                       |\n| 18.5M                   | 3.6G                                  |\n| SiMBA-S(EinFFT) (Ours)  | 224                                   |\n| 2                       |                                       |\n| 15.3M                   | 2.4G                                  |\n| SiMBA-S(MLP) (Ours)     | 224                                   |\n| 2                       |                                       |\n| 26.5M                   | 5.0G                                  |\n| Vim-S [                 | 76                                    |\n| 224                     |                                       |\n| 2                       |                                       |\n| 26M                     | -                                     |\n| VMamba-S [              | 32                                    |\n| 2                       |                                       |\n| 44M                     | 11.2G                                 |\n| SiMBA-B(Monarch) (Ours) | 224                                   |\n| 2                       |                                       |\n| 26.9M                   | 5.5G                                  |\n| SiMBA-B(EinFFT) (Ours)  | 224                                   |\n| 2                       |                                       |\n| 22.8M                   | 4.2G                                  |\n| SiMBA-B(MLP) (Ours)     | 224                                   |\n| 2                       |                                       |\n| 40.0M                   | 9.0G                                  |\n| HyenaViT-B [            | 8                                     |\n| 2                       |                                       |\n| 88M                     | -                                     |\n| S4ND-ViT-B [            | 39                                    |\n| 2                       |                                       |\n| 89M                     | -                                     |\n| VMamba-B [              | 32                                    |\n| 2                       |                                       |\n| 75M                     | 18.0G                                 |\n| SiMBA-L(Monarch) (Ours) | 224                                   |\n| 2                       |                                       |\n| 42M                     | 8.7G                                  |\n| SiMBA-L(EinFFT) (Ours)  | 224                                   |\n| 2                       |                                       |\n| 36.6M                   | 7.6G                                  |\n| SiMBA-L(MLP)            |                                       |\n| \u2020                       |                                       |\n|                         |                                       |\n| (Ours)                  | 224                                   |\n| 2                       |                                       |\n| 66.6M                   | 16.3G                                 |\n\n(5\u2264GFLOPs<10), and Large (10\u2264GFLOPs<30). \u2020 indicates that instability in the training SSM.\n\nthat the MLP version is less efficient in terms of parameters and FLOPS. The SiMBA\n(MLP) variant outperforms other SSM models in terms of top-1 accuracy, parameters, and FLOPS.\n"
    },
    {
        "level": "##",
        "title": "12 B. N. Patro Et Al.",
        "content": "\nModels\nSiMBA\nTimesNet\nCrossformer\nPatchTST\nETSFormer\nDLinear\nFEDFormer Autoformer Pyraformer\nMTGNN\nMetric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE\n96\n0.324 0.360 0.338 0.375 0.349 0.395 0.339 0.377 0.375 0.398 0.345 0.372 0.379 0.419 0.505 0.475 0.543 0.510 0.379 0.446\nETTm1\n192 **0.363 0.382** 0.374 0.387 0.405 0.411 0.376 0.392 0.408 0.410 0.380 0.389 0.426 0.441 0.553 0.496 0.557 0.537 0.470 0.428 336 **0.395 0.405** 0.410 0.411 0.432 0.431 0.408 0.417 0.435 0.428 0.413 0.413 0.445 0.459 0.621 0.537 0.754 0.655 0.473 0.430 720 **0.451 0.437** 0.478 0.450 0.487 0.463 0.499 0.461 0.499 0.462 0.474 0.453 0.543 0.490 0.671 0.561 0.908 0.724 0.553 0.479\n96\n0.177 0.263 0.187 0.267 0.208 0.292 0.192 0.273 0.189 0.280 0.193 0.292 0.203 0.287 0.255 0.339 0.435 0.507 0.203 0.299\nETTm2\n192 **0.245 0.306** 0.249 0.309 0.263 0.332 0.252 0.314 0.253 0.319 0.284 0.362 0.269 0.328 0.281 0.340 0.730 0.673 0.265 0.328 336 **0.304 0.343** 0.321 0.351 0.337 0.369 0.318 0.357 0.314 0.357 0.369 0.427 0.325 0.366 0.339 0.372 1.201 0.845 0.365 0.374 720 **0.400 0.399** 0.408 0.403 0.429 0.430 0.413 0.416 0.414 0.413 0.554 0.522 0.421 0.415 0.433 0.432 3.625 1.451 0.461 0.459\n96\n0.379 0.395 0.384 0.402 0.384 0.428 0.385 0.408 0.494 0.479 0.386 0.400 0.376 0.419 0.449 0.459 0.664 0.612 0.515 0.517\nETTh1\n192 0.432 **0.424** 0.436 0.429 0.438 0.452 **0.431** 0.432 0.538 0.504 0.437 0.432 0.420 0.448 0.500 0.482 0.790 0.681 0.553 0.522 336 **0.473 0.443** 0.491 0.469 0.495 0.483 0.485 0.462 0.574 0.521 0.481 0.459 0.459 0.465 0.521 0.496 0.891 0.738 0.612 0.577 720 **0.483 0.469** 0.521 0.500 0.522 0.501 0.497 0.483 0.562 0.535 0.519 0.516 0.506 0.507 0.514 0.512 0.963 0.782 0.609 0.597\n96\n0.290 0.339 0.340 0.374 0.347 0.391 0.343 0.376 0.340 0.391 0.333 0.387 0.358 0.397 0.346 0.388 0.645 0.597 0.354 0.454\nETTh2\n192 **0.373 0.390** 0.402 0.414 0.419 0.427 0.405 0.417 0.430 0.439 0.477 0.476 0.429 0.439 0.456 0.452 0.788 0.683 0.457 0.464 336 **0.376 0.406** 0.452 0.452 0.449 0.465 0.448 0.453 0.485 0.479 0.594 0.541 0.496 0.487 0.482 0.486 0.907 0.747 0.515 0.540 720 **0.407 0.431** 0.462 0.468 0.479 0.505 0.464 0.483 0.500 0.497 0.831 0.657 0.463 0.474 0.515 0.511 0.963 0.783 0.532 0.576\n96\n0.165 **0.253** 0.168 0.272 0.185 0.288 **0.159** 0.268 0.187 0.304 0.197 0.282 0.193 0.308 0.201 0.317 0.386 0.449 0.217 0.318\nElectricity\n192 **0.173 0.262** 0.184 0.289 0.201 0.295 0.177 0.278 0.199 0.315 0.196 0.285 0.201 0.315 0.222 0.334 0.378 0.443 0.238 0.352\n336 **0.188 0.277** 0.198 0.300 0.211 0.312 0.195 0.296 0.212 0.329 0.209 0.301 0.214 0.329 0.231 0.338 0.376 0.443 0.260 0.348 720 **0.214 0.305** 0.220 0.320 0.223 0.335 0.215 0.317 0.233 0.345 0.245 0.333 0.246 0.355 0.254 0.361 0.376 0.445 0.290 0.369\n96\n0.468 0.268 0.593 0.321 0.591 0.329 0.583 0.319 0.607 0.392 0.650 0.396 0.587 0.366 0.613 0.388 0.867 0.468 0.660 0.437\nTraffic\n192 **0.413 0.317** 0.617 0.336 0.607 0.345 0.591 0.331 0.621 0.399 0.598 0.370 0.604 0.373 0.616 0.382 0.869 0.467 0.649 0.438 336 **0.529 0.284** 0.629 0.336 0.613 0.339 0.599 0.332 0.622 0.396 0.605 0.373 0.621 0.383 0.622 0.337 0.881 0.469 0.653 0.472 720 **0.564 0.297** 0.640 0.350 0.620 0.348 0.601 0.341 0.632 0.396 0.645 0.394 0.626 0.382 0.660 0.408 0.896 0.473 0.639 0.437\n96\n0.176 **0.219** 0.172 0.220 0.191 0.251 **0.171** 0.230 0.197 0.281 0.196 0.255 0.217 0.296 0.266 0.336 0.622 0.556 0.230 0.329\nWeather\n192 0.222 **0.260 0.219** 0.261 0.219 0.279 0.219 0.271 0.237 0.312 0.237 0.296 0.276 0.336 0.307 0.367 0.739 0.624 0.263 0.322 336 **0.275 0.297** 0.280 0.306 0.287 0.332 0.277 0.321 0.298 0.353 0.283 0.335 0.339 0.380 0.359 0.395 1.004 0.753 0.354 0.396 720 **0.350 0.349** 0.365 0.359 0.368 0.378 0.365 0.367 0.352 0.288 0.345 0.381 0.403 0.428 0.419 0.428 1.420 0.934 0.409 0.371\n\nbold and the second best is underlined.\n\nMoving to the base size, SiMBA-B demonstrates superior performance compared to other SSMs such as Hyena-ViT-B (78.5%), Vim-S (80.5%), VMamba-S (83.5%), and S4ND-ViT-B (80.4%), while maintaining similar parameters and GFlops. SiMBA- B achieves an impressive accuracy of 84.7%, which is on par with transformer-based models like Wave-ViT-B (84.8%), iFormer-B (84.6%), and Max-ViT-S (84.5%), but with notably fewer parameters and GFlops. Notably, the SiMBA (MLP) variant outperforms other SSM models and the current state-of-the-art model in terms of top-1 accuracy, while also reducing the number of parameters and FLOPS, thus marking a significant advancement in state-of-the-art performance.\n\nIn the large category, SiMBA-L achieves an 83.9% top-1 accuracy, surpassing VMamba-\nB (83.2%) with comparable computational complexity and parameters. However, a performance gap remains compared to state-of-the-art transformers such as SVT-H-L (85.7%).\n\nFuture work aims to address this gap by further scaling SiMBA. However, SiMBA-L\n(EinFFT) outperforms other SSMs in this range. Notably, the SiMBA(MLP) variant exhibits instability issues in large network sizes, prompting us to solely report results from the SiMBA(EinFFT) variant for large network sizes. SiMBA(EinFFT) mitigates stability concerns at large network sizes and demonstrates greater efficiency in terms of FLOPS\nand parameters, outperforming all other SSMs, while with certain trade-offs.\n"
    },
    {
        "level": "##",
        "title": "4.2 Sota For Multi-Variate Time Series Forecasting",
        "content": "\nWe conducted a comprehensive evaluation of our State Space model, SiMBA, on seven benchmark standard datasets widely used for Multivariate Time Series Forecasting, including Electricity, Weather, Traffic, and four ETT datasets (ETTh1, ETTh2, ETTm1, and ETTm2), as presented in Table 3. Our evaluation compares SiMBA with various state-of-the-art models, including Transformer-based methods like PatchTST [41], CrossFormer [74], FEDFormer [75], ETSFormer [63], PyraFormer [31], and AutoFormer [3]. Additionally, CNN-based methods such as TimeNet [65], graph-based methods like MTGNN [67], and MLP-based models like DLinear [73] are included in the comparison.\n\nSiMBA demonstrates superior performance across multiple evaluation metrics, including Mean Squared Error (MSE) and Mean Absolute Error (MAE), outperforming the state-of-the-art models. These results underscore the versatility and effectiveness of the SiMBA architecture in handling diverse time series forecasting tasks and modalities, solidifying its position as a leading model in the field. While presenting our results, it's important to note that due to space limitations in Table 3, we couldn't include some recent methods in the Time series domain, such as FourierGNN [70], CrossGNN [21], TiDE [4], SciNet [30], and FreTS [71]. For a fair comparison, we utilized the code from PatchTST [41], and the results were based on a lookup window of size 96 for all datasets.\n\n|                             |       |      | Model         | Param Top-1% Top-5% Description(Seq-mix, Channel-mix)   |\n|-----------------------------|-------|------|---------------|---------------------------------------------------------|\n| ResNet-152                  | 60M   | 78.6 | 94.3          | ConvNet, MLP                                            |\n| ViT-b                       | 87M   | 78.5 | 93.6          | Attention, MLP                                          |\n| s4 only                     |       |      |               |                                                         |\n| \u2020                           |       |      |               |                                                         |\n| 13.2M                       | 58.9  | 82.5 | S4, NA        |                                                         |\n| Hippo only                  |       |      |               |                                                         |\n| \u2020                           |       |      |               |                                                         |\n| 16.4M                       | 63.2  | 89.2 | Hippo, NA     |                                                         |\n| Mamba only                  |       |      |               |                                                         |\n| \u2020                           |       |      |               |                                                         |\n| 15.3M                       | 39.1  | 67.1 | Mamba, NA     |                                                         |\n| s4+MLP                      | 23.5M | 75.9 | 93.6          | S4, MLP                                                 |\n| Hippo+MLP                   | 24.8M | 77.9 | 94.0          | Hippo, MLP                                              |\n| SiMBA( Mamba +Monach) 18.5M | 81.1  | 95.5 | Mamba, Monach |                                                         |\n| SiMBA(Mamba +EinFFT) 15.3M  | 81.7  | 95.9 | Mamba, EinFFT |                                                         |\n| SiMBA( Mamba +MLP)          | 26.6M | 84   | 96.7          | Mamba, MLP                                              |\n"
    },
    {
        "level": "##",
        "title": "4.3 Ablation Analysis Of Model Simba",
        "content": "\nIn our ablation analysis on the ImageNet-1k dataset for small-sized models (Table 4), we explored various sequence modeling configurations, focusing on SiMBA and comparing it with other state space models (SSMs) like S4 and Hyena. Notably, Mamba alone faced stability issues and was non-trainable for the ImageNet dataset for largescale networks, prompting us to integrate it with different channel mixing components. The results highlight that both S4 and Hyena when utilized as standalone models without channel mixing, exhibit a performance gap compared to state-of-the-art vision transformers like SVT, WaveViT, Volo, and MaxViT. To address this, our SiMBA architecture combines Mamba as the sequence modeling component with different channel mixing strategies. In the ablation study for channel mixing, we explored three components: MLP, Monarch Mixing (M2), and our EinFFT. The findings in Table 4 indicate\n\nMask R-CNN 1\u00d7 schedule\nBackbone\nAPb\nAPb\n50\nAPb\n75\nAPm\nAPm\n50\nAPm\n75"
    },
    {
        "level": "##",
        "title": "4.4 Task Learning: Object Detection",
        "content": "\nIn our experiments on the MS COCO 2017 dataset, a widely used benchmark for object detection and instance segmentation, consisting of approximately 118,000 training images and 5,000 validation images, we employed two widely-used detection frameworks: RetinaNet [29] and Mask R-CNN [19]. Model performance was assessed using Average Precision (AP). SiMBA-s, pre-trained on the ImageNet-1K dataset, served as the backbone architecture, and Xavier initialization was applied to additional layers. The results, detailed in Table 5, showcase SiMBA's competitive performance in comparison to RetinaNet [29] and Mask R-CNN [19]. Notably, SiMBA surpasses the latest models, including LITv2 [44], RegionViT, and PVT [60] transformer models, outperforming ResNet in\n\n| Method       |   size | mIoU (SS)   | mIoU (MS)   |\n|--------------|--------|-------------|-------------|\n| ResNet-101   |        |             |             |\n| 512          |        |             |             |\n| 2            |        |             |             |\n| 42.9         |   44   | 85M         | 1030G       |\n| DeiT-B + MLN |        |             |             |\n| 512          |        |             |             |\n| 2            |        |             |             |\n| 45.5         |   47.2 | 144M        | 2007G       |\n| Swin-S       |        |             |             |\n| 512          |        |             |             |\n| 2            |        |             |             |\n| 47.6         |   49.5 | 81M         | 1039G       |\n| ConvNeXt-S   |        |             |             |\n| 512          |        |             |             |\n| 2            |        |             |             |\n| 48.7         |   49.6 | 82M         | 1027G       |\n| SiMBA-S      |        |             |             |\n| 512          |        |             |             |\n| 2            |        |             |             |\n| 49.0         |   49.6 | 62M         | 1040G       |\n\nterms of AP. SiMBA exhibits superior performance over vanilla ViT models and hierarchical transformer models, achieving the highest AP. Furthermore, we extend SiMBA's capabilities to semantic segmentation on the ADE20K dataset using UperNet [68], with results summarized in Table 7. The outcomes underscore SiMBA's versatility and effectiveness across diverse computer vision tasks.\n"
    },
    {
        "level": "##",
        "title": "4.5 Transfer Learning Comparison",
        "content": "\nTo evaluate the efficacy of the SiMBA architecture and learned representations, we conducted assessments on various transfer learning benchmark datasets, including CIFAR- 10 [24], CIFAR-100 [24], Stanford Cars [23], and Flowers-102 [42]. The performance of SiMBA, pre-trained on ImageNet-1K and fine-tuned for image classification on these datasets, was compared against ResMLP models and state-of-the-art models such as GFNet [50]. Table 6 highlights the competitive performance of SiMBA on downstream datasets, outperforming ResMLP models and demonstrating comparable results to GFNet [50]. Additionally, SiMBA's performance was competitive against popular models like ViT-b, HyenaViT-b, and ViT-b-Monarch on ImageNet-1k. Notably, SiMBA achieved superior results compared to ResNet-152, despite having fewer parameters, emphasizing its effectiveness in transfer learning scenarios.\n"
    },
    {
        "level": "##",
        "title": "5 Conclusion",
        "content": "\nThis paper has proposed a new channel modeling technique EinFFT which solves the key stability problem in Mamba. We have also proposed SiMBA, a new architecture that leverages EinFFT for channel modeling and Mamba for sequence modeling. SiMBA allows for the exploration of various alternatives for sequence modeling like S4, long conv, Hyena, H3, RWKV, and even newer state space models. Importantly, SiMBA allows the exploration of alternatives for channel modeling like M2, and EinFFT as well as other possible spectral techniques. SiMBA also bridges the performance gap that most state space models have with state-of-art transformers on both vision and time series datasets.\n\nWe plan to explore a few alternatives within the SiMBA framework such as long conv for sequence modeling with M2 or EinFFT for channel modeling.\n\n16\nB. N. Patro et al.\n"
    },
    {
        "level": "##",
        "title": "References",
        "content": "\n1. Cai, Z., Vasconcelos, N.: Cascade r-cnn: Delving into high quality object detection. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 6154\u20136162 (2018) 20, 23, 24\n2. Chen, K., Wang, J., Pang, J., Cao, Y., Xiong, Y., Li, X., Sun, S., Feng, W., Liu, Z., Xu,\nJ., et al.: Mmdetection: Open mmlab detection toolbox and benchmark. arXiv preprint arXiv:1906.07155 (2019) 23, 24\n3. Chen, M., Peng, H., Fu, J., Ling, H.: Autoformer: Searching transformers for visual recognition. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 12270\u201312280 (2021) 13\n4. Das, A., Kong, W., Leach, A., Mathur, S.K., Sen, R., Yu, R.: Long-term forecasting with tide:\nTime-series dense encoder. Transactions on Machine Learning Research (2023) 13\n5. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale hierarchical image database. In: 2009 IEEE conference on computer vision and pattern recognition. pp. 248\u2013255. Ieee (2009) 10, 11\n6. Dong, X., Bao, J., Chen, D., Zhang, W., Yu, N., Yuan, L., Chen, D., Guo, B.: Cswin transformer: A general vision transformer backbone with cross-shaped windows. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 12124\u201312134 (2022) 4\n7. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is worth 16x16 words: Transformers for image recognition at scale. In: International Conference on Learning Representations (2020) 3, 4, 14, 23\n8. Fu, D., Arora, S., Grogan, J., Johnson, I., Eyuboglu, E.S., Thomas, A., Spector, B., Poli,\nM., Rudra, A., R\u00e9, C.: Monarch mixer: A simple sub-quadratic gemm-based architecture.\nAdvances in Neural Information Processing Systems 36 (2024) 11\n9. Fu, D.Y., Dao, T., Saab, K.K., Thomas, A.W., Rudra, A., R\u00e9, C.: Hungry hungry hippos: Towards language modeling with state space models. arXiv preprint arXiv:2212.14052 (2022) 2, 4\n10. Fu, D.Y., Epstein, E.L., Nguyen, E., Thomas, A.W., Zhang, M., Dao, T., Rudra, A., Re, C.:\nSimple hardware-efficient long convolutions for sequence modeling. In: ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models (2023) 4\n11. Fu, D.Y., Kumbong, H., Nguyen, E., R\u00e9, C.: Flashfftconv: Efficient convolutions for long\nsequences with tensor cores. arXiv preprint arXiv:2311.05908 (2023) 2\n12. Glorot, X., Bengio, Y.: Understanding the difficulty of training deep feedforward neural networks. In: Proceedings of the thirteenth international conference on artificial intelligence and statistics. pp. 249\u2013256. JMLR Workshop and Conference Proceedings (2010) 24\n13. Gu, A., Dao, T.: Mamba: Linear-time sequence modeling with selective state spaces. arXiv\npreprint arXiv:2312.00752 (2023) 2, 3, 8, 9\n14. Gu, A., Dao, T., Ermon, S., Rudra, A., R\u00e9, C.: Hippo: Recurrent memory with optimal polynomial projections. In: Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., Lin, H. (eds.) Advances in Neural Information Processing Systems. vol. 33, pp. 1474\u20131487. Curran Associates, Inc. (2020), https://proceedings.neurips.cc/paper_files/paper/2020/ file/102f0bb6efb3a6128a3c750dd16729be-Paper.pdf 2, 3\n15. Gu, A., Goel, K., Re, C.: Efficiently modeling long sequences with structured state spaces.\nIn: International Conference on Learning Representations (2021) 2, 3, 8\n16. Guibas, J., Mardani, M., Li, Z., Tao, A., Anandkumar, A., Catanzaro, B.: Efficient token\nmixing for transformers via adaptive fourier neural operators. In: International Conference on Learning Representations (2022) 3, 4\n17. Guo, J., Han, K., Wu, H., Tang, Y., Chen, X., Wang, Y., Xu, C.: Cmt: Convolutional neural\nnetworks meet vision transformers. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 12175\u201312185 (2022) 11\n18. Hasani, R., Lechner, M., Amini, A., Rus, D., Grosu, R.: Liquid time-constant networks.\nIn: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 35, pp. 7657\u20137666 (2021) 2\n19. He, K., Gkioxari, G., Doll\u00e1r, P., Girshick, R.: Mask r-cnn. In: Proceedings of the IEEE international conference on computer vision. pp. 2961\u20132969 (2017) 14, 24\n20. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 770\u2013778 (2016) 14, 23\n21. Huang, Q., Shen, L., Zhang, R., Ding, S., Wang, B., Zhou, Z., Wang, Y.: Crossgnn: Confronting noisy multivariate time series via cross interaction refinement. In: Thirty-seventh Conference on Neural Information Processing Systems (2023) 13\n22. Jiang, A.Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D.S., de las Casas, D., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., Lavaud, L.R., Lachaux, M.A., Stock, P., Scao, T.L., Lavril, T., Wang, T., Lacroix, T., Sayed, W.E.: Mistral 7b (2023) 1\n23. Krause, J., Stark, M., Deng, J., Fei-Fei, L.: 3d object representations for fine-grained categorization. In: Proceedings of the IEEE international conference on computer vision workshops. pp. 554\u2013561 (2013) 10, 15, 23, 24\n24. Krizhevsky, A., et al.: Learning multiple layers of features from tiny images (2009) 10, 15,\n23, 24\n25. Lee-Thorp, J., Ainslie, J., Eckstein, I., Ontanon, S.: Fnet: Mixing tokens with fourier transforms. arXiv preprint arXiv:2105.03824 (2021) 4\n26. Li, X., Wang, W., Wu, L., Chen, S., Hu, X., Li, J., Tang, J., Yang, J.: Generalized focal loss:\nLearning qualified and distributed bounding boxes for dense object detection. Advances in\nNeural Information Processing Systems 33, 21002\u201321012 (2020) 20, 23, 24\n27. Li, Y., Yuan, G., Wen, Y., Hu, E., Evangelidis, G., Tulyakov, S., Wang, Y., Ren, J.: Efficientformer: Vision transformers at mobilenet speed. arXiv preprint arXiv:2206.01191 (2022) 11\n28. Li, Y., Bubeck, S., Eldan, R., Giorno, A.D., Gunasekar, S., Lee, Y.T.: Textbooks are all you\nneed ii: phi-1.5 technical report (2023) 1\n29. Lin, T.Y., Goyal, P., Girshick, R., He, K., Doll\u00e1r, P.: Focal loss for dense object detection. In:\nProceedings of the IEEE international conference on computer vision. pp. 2980\u20132988 (2017) 14, 24\n30. Liu, M., Zeng, A., Chen, M., Xu, Z., Lai, Q., Ma, L., Xu, Q.: Scinet: Time series modeling\nand forecasting with sample convolution and interaction. Advances in Neural Information\nProcessing Systems 35, 5816\u20135828 (2022) 13\n31. Liu, S., Yu, H., Liao, C., Li, J., Lin, W., Liu, A.X., Dustdar, S.: Pyraformer: Low-complexity\npyramidal attention for long-range time series modeling and forecasting. In: International conference on learning representations (2021) 13\n32. Liu, Y., Tian, Y., Zhao, Y., Yu, H., Xie, L., Wang, Y., Ye, Q., Liu, Y.: Vmamba: Visual state\nspace model. arXiv preprint arXiv:2401.10166 (2024) 4, 10, 11\n33. Liu, Z., Hu, H., Lin, Y., Yao, Z., Xie, Z., Wei, Y., Ning, J., Cao, Y., Zhang, Z., Dong, L., et al.:\nSwin transformer v2: Scaling up capacity and resolution. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 12009\u201312019 (2022) 3, 11\n34. Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer:\nHierarchical vision transformer using shifted windows. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 10012\u201310022 (2021) 4, 23, 24\n35. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. In: International Conference on Learning Representations (2018) 24\n"
    },
    {
        "level": "##",
        "title": "18 B. N. Patro Et Al.",
        "content": "\n36. Ma, X., Zhou, C., Kong, X., He, J., Gui, L., Neubig, G., May, J., Zettlemoyer, L.: Mega: Moving average equipped gated attention. In: The Eleventh International Conference on Learning Representations (2022) 2\n37. Mehta, H., Gupta, A., Cutkosky, A., Neyshabur, B.: Long range language modeling via gated\nstate spaces. In: The Eleventh International Conference on Learning Representations (2022) 2\n38. Mukherjee, S., Mitra, A., Jawahar, G., Agarwal, S., Palangi, H., Awadallah, A.: Orca: Progressive learning from complex explanation traces of gpt-4 (2023) 1\n39. Nguyen, E., Goel, K., Gu, A., Downs, G., Shah, P., Dao, T., Baccus, S., R\u00e9, C.: S4nd: Modeling images and videos as multidimensional signals with state spaces. Advances in neural\ninformation processing systems 35, 2846\u20132861 (2022) 2, 11\n40. Nguyen, T.M., Pham, M., Nguyen, T.M., Nguyen, K., Osher, S., Ho, N.: Fourierformer:\nTransformer meets generalized fourier integral theorem. In: Advances in Neural Information Processing Systems (2022) 4\n41. Nie, Y., Nguyen, N.H., Sinthong, P., Kalagnanam, J.: A time series is worth 64 words: Longterm forecasting with transformers. In: The Eleventh International Conference on Learning Representations (2022) 13\n42. Nilsback, M.E., Zisserman, A.: Automated flower classification over a large number of\nclasses. In: 2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing. pp. 722\u2013729. IEEE (2008) 10, 15, 23, 24\n43. Oppenheim, A.V., Verghese, G.: Signals, Systems and Inference. Pearson, USA (2010) 2, 3 44. Pan, Z., Cai, J., Zhuang, B.: Fast vision transformers with hilo attention. In: Advances in\nNeural Information Processing Systems (2022) 14\n45. Patro, B.N., Namboodiri, V.P., Agneeswaran, V.S.: Spectformer: Frequency and attention is\nwhat you need in a vision transformer. arXiv preprint arXiv:2304.06446 (2023) 4, 5\n46. Patro, B.N., Agneeswaran, V.S.: Scattering vision transformer: Spectral mixing matters. In:\nThirty-seventh Conference on Neural Information Processing Systems (2023) 4, 5, 11\n47. Peng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcadinho, S., Cao, H., Cheng, X., Chung,\nM., Grella, M., GV, K.K., et al.: Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048 (2023) 2\n48. Poli, M., Massaroli, S., Nguyen, E., Fu, D.Y., Dao, T., Baccus, S., Bengio, Y., Ermon, S., Re,\nC.: Hyena hierarchy: Towards larger convolutional language models (2023) 2, 3, 4, 10\n49. Radosavovic, I., Kosaraju, R.P., Girshick, R., He, K., Doll\u00e1r, P.: Designing network design\nspaces. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 10428\u201310436 (2020) 11\n50. Rao, Y., Zhao, W., Zhu, Z., Lu, J., Zhou, J.: Global filter networks for image classification.\nAdvances in Neural Information Processing Systems 34, 980\u2013993 (2021) 3, 4, 14, 15, 23\n51. Si, C., Yu, W., Zhou, P., Zhou, Y., Wang, X., YAN, S.: Inception transformer. In: Advances\nin Neural Information Processing Systems (2022) 11\n52. Sun, Y., Dong, L., Huang, S., Ma, S., Xia, Y., Xue, J., Wang, J., Wei, F.: Retentive network:\nA successor to transformer for large language models. ICLR (2023) 2\n53. Tan, M., Le, Q.: Efficientnet: Rethinking model scaling for convolutional neural networks.\nIn: International conference on machine learning. pp. 6105\u20136114. PMLR (2019) 14, 23\n54. Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham, P., Rao, J., Yang, L., Ruder, S.,\nMetzler, D.: International conference on learning representations (iclr) (2021) 4\n55. Tolstikhin, I.O., Houlsby, N., Kolesnikov, A., Beyer, L., Zhai, X., Unterthiner, T., Yung, J.,\nSteiner, A., Keysers, D., Uszkoreit, J., et al.: Mlp-mixer: An all-mlp architecture for vision.\nAdvances in neural information processing systems 34, 24261\u201324272 (2021) 4\n56. Touvron, H., Bojanowski, P., Caron, M., Cord, M., El-Nouby, A., Grave, E., Izacard, G.,\nJoulin, A., Synnaeve, G., Verbeek, J., et al.: Resmlp: Feedforward networks for image clas-\nsification with data-efficient training. IEEE Transactions on Pattern Analysis and Machine Intelligence (2022) 14, 23\n\n57. Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., J\u00e9gou, H.: Training dataefficient image transformers & distillation through attention. In: International Conference on Machine Learning. pp. 10347\u201310357. PMLR (2021) 3, 4, 11, 14, 23\n58. Tu, Z., Talebi, H., Zhang, H., Yang, F., Milanfar, P., Bovik, A., Li, Y.: Maxvit: Multi-axis\nvision transformer. In: Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XXIV. pp. 459\u2013479. Springer (2022) 11\n59. Wang, P., Wang, X., Luo, H., Zhou, J., Zhou, Z., Wang, F., Li, H., Jin, R.: Scaled relu matters for training vision transformers. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 36, pp. 2495\u20132503 (2022) 11\n60. Wang, W., Xie, E., Li, X., Fan, D.P., Song, K., Liang, D., Lu, T., Luo, P., Shao, L.: Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 568\u2013578 (2021) 14\n61. Wang, W., Xie, E., Li, X., Fan, D.P., Song, K., Liang, D., Lu, T., Luo, P., Shao, L.: Pvt\nv2: Improved baselines with pyramid vision transformer. Computational Visual Media 8(3),\n415\u2013424 (2022) 23\n62. Wang, Z., Jiang, W., Zhu, Y.M., Yuan, L., Song, Y., Liu, W.: Dynamixer: a vision mlp architecture with dynamic mixing. In: International Conference on Machine Learning. pp. 22691\u2013 22701. PMLR (2022) 4\n63. Woo, G., Liu, C., Sahoo, D., Kumar, A., Hoi, S.: Etsformer: Exponential smoothing transformers for time-series forecasting. ICLR (2022) 13\n64. Wu, H., Xiao, B., Codella, N., Liu, M., Dai, X., Yuan, L., Zhang, L.: Cvt: Introducing convolutions to vision transformers. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 22\u201331 (2021) 4\n65. Wu, H., Hu, T., Liu, Y., Zhou, H., Wang, J., Long, M.: Timesnet: Temporal 2d-variation modeling for general time series analysis. In: The Eleventh International Conference on Learning Representations (2022) 13\n66. Wu, H., Xu, J., Wang, J., Long, M.: Autoformer: Decomposition transformers with Auto-\nCorrelation for long-term series forecasting. In: Advances in Neural Information Processing Systems (2021) 10\n67. Wu, Z., Pan, S., Long, G., Jiang, J., Chang, X., Zhang, C.: Connecting the dots: Multivariate time series forecasting with graph neural networks. In: Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining. pp. 753\u2013763 (2020) 13\n68. Xiao, T., Liu, Y., Zhou, B., Jiang, Y., Sun, J.: Unified perceptual parsing for scene understanding. In: Proceedings of the European conference on computer vision (ECCV). pp. 418\u2013434 (2018) 15\n69. Yao, T., Pan, Y., Li, Y., Ngo, C.W., Mei, T.: Wave-vit: Unifying wavelet and transformers for\nvisual representation learning. In: Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XXV. pp. 328\u2013345. Springer (2022) 4, 5, 11\n70. Yi, K., Zhang, Q., Fan, W., He, H., Hu, L., Wang, P., An, N., Cao, L., Niu, Z.: Fouriergnn:\nRethinking multivariate time series forecasting from a pure graph perspective. In: Thirtyseventh Conference on Neural Information Processing Systems (2023) 13\n71. Yi, K., Zhang, Q., Fan, W., Wang, S., Wang, P., He, H., An, N., Lian, D., Cao, L., Niu, Z.:\nFrequency-domain mlps are more effective learners in time series forecasting. Advances in\nNeural Information Processing Systems 36 (2024) 13\n72. Yuan, L., Hou, Q., Jiang, Z., Feng, J., Yan, S.: Volo: Vision outlooker for visual recognition.\nIEEE Transactions on Pattern Analysis and Machine Intelligence (2022) 5, 11\n20\nB. N. Patro et al.\n73. Zeng, A., Chen, M., Zhang, L., Xu, Q.: Are transformers effective for time series forecasting?\nIn: Proceedings of the AAAI conference on artificial intelligence. vol. 37, pp. 11121\u201311128 (2023) 13\n74. Zhang, Y., Yan, J.: Crossformer: Transformer utilizing cross-dimension dependency for multivariate time series forecasting. In: The Eleventh International Conference on Learning Representations (2022) 13\n75. Zhou, T., Ma, Z., Wen, Q., Wang, X., Sun, L., Jin, R.: Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting. In: International Conference on Machine Learning. pp. 27268\u201327286. PMLR (2022) 13\n76. Zhu, L., Liao, B., Zhang, Q., Wang, X., Liu, W., Wang, X.: Vision mamba: Efficient visual representation learning with bidirectional state space model. arXiv preprint arXiv:2401.09417 (2024) 4, 10, 11, 22\n"
    },
    {
        "level": "##",
        "title": "A Appendix",
        "content": "\nThis section includes comprehensive details regarding training configurations for both transfer learning and task learning tasks. Specifically, Table-10 provides a breakdown of the dataset information utilized for transformer learning, offering insights into the datasets employed in the training process. For a thorough understanding of the SiMBA algorithm, Algorithm-32 is presented, providing a complete architectural view of the SiMBA model. This algorithm outlines the key operations and processes involved in the SiMBA architecture, facilitating a deeper understanding of its inner workings. Furthermore, detailed implementation insights into the EinFFT channel mixing method are provided, shedding light on the mechanism utilized for frequency-domain channel mixing within the SiMBA model. This implementation detail offers clarity on how the frequency-domain information is leveraged to enhance the model's performance in capturing intricate relationships within sequential data. This comparison aids in understanding the different approaches employed by various vision models in handling sequential data and exploiting inter-channel relationships. Moreover, the document reports the object detection performance of two prominent models, GFL [26] and Cascade Mask R- CNN [1], on the MS COCO val2017 dataset. The performance metrics presented in Table- 9 demonstrate a notable improvement in performance, showcasing the effectiveness of the proposed SiMBA model.\n"
    },
    {
        "level": "##",
        "title": "B Einfft Implementation",
        "content": "\nWe proposed EinFFT module introduces a novel approach to model channels more efficiently, with complex-valued weights and advanced signal processing techniques.\n\nThe architecture leverages Fast Fourier Transform (FFT) operations, which transform the physical domain to the Frequency Domain. The module's distinctive formulation employs complex-valued weights initialized with a scale factor, promoting enhanced representation learning. The incorporation of soft thresholding introduces sparsity constraints, facilitating noise reduction and feature selection. The complex-valued weights contribute to a more expressive model, enriching the representational space. The modular and extensible implementation of these operations in the code presents a valuable tool for researchers and practitioners in sequence modeling, offering a unique perspective on capturing intricate dependencies within sequential data.\n\nMathematically, the core computations of EinFFT can be represented as follows.\n\nLet \ud835\udc65 denote the input tensor of shape \ud835\udc35 \u00d7 \ud835\udc41 \u00d7 \ud835\udc36, where \ud835\udc35 is the batch size, \ud835\udc41 is the sequence length, and \ud835\udc36 is the number of channels. The FFT operation is denoted as FFT(\ud835\udc65, dim = 1, norm =\u2032 *\ud835\udc5c\ud835\udc5f\ud835\udc61\u210e\ud835\udc5c*\u2032). Complex-valued weights \ud835\udc4a1, \ud835\udc4a2, \ud835\udc351, \ud835\udc352 are utilized in a series of operations involving complex multiplication, rectified linear units (ReLU), and soft thresholding:\nB. N. Patro et al.\n\n\ud835\udc4b = FFT(\ud835\udc65)\n\ud835\udc4breal1 = max(Re(\ud835\udc4b) \u22c5 \ud835\udc4a1,real \u2212 Im(\ud835\udc4b) \u22c5 \ud835\udc4a1,imag + \ud835\udc351,real, 0)\n\ud835\udc4bimag1 = max(Re(\ud835\udc4b) \u22c5 \ud835\udc4a1,imag + Im(\ud835\udc4b) \u22c5 \ud835\udc4a1,real + \ud835\udc351,imag, 0)\n\ud835\udc4breal2 = Re(\ud835\udc4breal1 \u22c5 \ud835\udc4a2,real \u2212 \ud835\udc4bimag1 \u22c5 \ud835\udc4a2,imag + \ud835\udc352,real)\n\ud835\udc4bimag2 = Im(\ud835\udc4breal1 \u22c5 \ud835\udc4a2,imag + \ud835\udc4bimag1 \u22c5 \ud835\udc4a2,real + \ud835\udc352,imag)\n\ud835\udc4bshrink = softshrink(\ud835\udc4breal2 & \ud835\udc4bimag2, \ud835\udf06 = sparsity_threshold)\n\ud835\udc65ifft = IFFT(\ud835\udc4bshrink, dim = 1, norm =\u2032 *\ud835\udc5c\ud835\udc5f\ud835\udc61\u210e\ud835\udc5c*\u2032)\n\ud835\udc65reshaped = reshape(\ud835\udc65ifft, (*\ud835\udc35, \ud835\udc41, \ud835\udc36*))\nThis sequence of operations encapsulates the essence of the EinFFT module, providing a comprehensive mathematical description of its functionality.\n\nThe SiMBA Block Process, depicted in Algorithm 32 inspire from ViM [76], unfolds as follows: Initially, the input token sequence \ud835\udc17\ud835\ude95\u22121 undergoes normalization via a dedicated layer. Subsequently, a linear transformation is employed to project the normalized sequence onto \ud835\udc31 and \ud835\udc33, both of dimension size \ud835\udc43. Moving forward, the algorithm proceeds to process \ud835\udc31 in both forward and backward directions. Within each direction, a 1-D\nconvolution operation is applied to \ud835\udc31 to yield \ud835\udc31\ud835\udc5c. Subsequently, linear projections are applied to \ud835\udc31\ud835\udc5c, resulting in \ud835\udc01\ud835\udc5c, \ud835\udc02\ud835\udc5c, and \ud835\udeab\ud835\udc5c. Utilizing \ud835\udeab\ud835\udc5c, transformations are performed on\n\ud835\udc00\ud835\udc5c and \ud835\udc01\ud835\udc5c. Following this, the SSM mechanism computes \ud835\udc32*\ud835\udc53\ud835\udc5c\ud835\udc5f\ud835\udc64\ud835\udc4e\ud835\udc5f\ud835\udc51* and \ud835\udc32*\ud835\udc4f\ud835\udc4e\ud835\udc50\ud835\udc58\ud835\udc64\ud835\udc4e\ud835\udc5f\ud835\udc51*. Gating by \ud835\udc33 is applied to \ud835\udc32\ud835\udc5c, culminating in the generation of the output token sequence \ud835\udc17\ud835\ude95.\n\nThis output sequence is the aggregation of the gated \ud835\udc32\ud835\udc5c from both directions. In essence, the SiMBA Block Process encompasses a series of operations, including normalization, linear projections, convolution, and attention mechanisms, ultimately leading to the generation of the final token sequence \ud835\udc17\ud835\ude95.\n"
    },
    {
        "level": "##",
        "title": "C Experiment C.1 Dataset And Training Setup For Image Classification Task",
        "content": "\nWe describe the training process of the image recognition task using the ImageNet1K\nbenchmark dataset, which includes 1.28 million training images and 50K validation images belonging to 1,000 categories. The vision backbones are trained from scratch using data augmentation techniques like RandAug, CutOut, and Token Labeling objectives with MixToken. The performance of the trained backbones is evaluated using both top- 1 and top-5 accuracies on the validation set. The optimization process involves using the AdamW optimizer with a momentum of 0.9, 10 epochs of linear warm-up, and 310\nepochs of cosine decay learning rate scheduler. The batch size is set to 128 and is distributed on 8 A100 GPUs. The learning rate and weight decay are fixed at 0.00001 and\n0.05, respectively.\n\n| ImageNet-1k                                          | CIFAR-10   |\n|------------------------------------------------------|------------|\n| Optimizer                                            | AdamW      |\n| Optimizer momentum                                   |            |\n| \ud835\udefd                                                    |            |\n| 1                                                    |            |\n| , \ud835\udefd                                                  |            |\n| 2                                                    |            |\n| = 0                                                  |            |\n| .                                                    |            |\n| 9                                                    |            |\n| ,                                                    |            |\n| 0                                                    |            |\n| .                                                    |            |\n| 999                                                  |            |\n| Learning rate schedule Cosine decay w/ linear warmup |            |\n| Dropout rate                                         | 0.2        |\n| Label smoothing                                      | 0.1        |\n| Image size                                           | 224 x 224  |\n| Base learning rate                                   | 1e-3       |\n| Batch size                                           | 128        |\n| Training epochs                                      | 300        |\n| Warmup epochs                                        | 10         |\n| Stochastic depth rate                                | 0.1        |\n| Weight decay                                         | 0.05       |\n| Backbone       | Method   |\n|----------------|----------|\n| \ud835\udc34\ud835\udc43             |          |\n|                | \ud835\udc4f        |\n|                |          |\n| \ud835\udc34\ud835\udc43             |          |\n|                | \ud835\udc4f        |\n| 50             |          |\n|                | \ud835\udc34\ud835\udc43       |\n| \ud835\udc4f              |          |\n| 75             |          |\n| ResNet50 [     | 20       |\n| GFL [          | 26       |\n| 44.5 63.0 48.3 |          |\n| Swin-T [       | 34       |\n| PVTv2-B2 [     | 61       |\n| 50.2 69.4 54.7 |          |\n| SiMBA-S (Ours) |          |\n| 50.3 69.0 55.1 |          |\n| ResNet50 [     | 20       |\n| Cascade        |          |\n| Mask           |          |\n| R-CNN          |          |\n| [              | 1        |\n| 46.3 64.3 50.5 |          |\n| Swin-T [       | 34       |\n| PVTv2-B2 [     | 61       |\n| 51.1 69.8 55.3 |          |\n| SiMBA-S (Ours) |          |\n| 51.4 70.1 56.0 |          |\n"
    },
    {
        "level": "##",
        "title": "C.2 Training Setup For Transfer Learning",
        "content": "\nTo test the effectiveness of our architecture and learned representation, we evaluated vanilla SiMBA on commonly used transfer learning benchmark datasets, including CIFAR- 10 [24], CIFAR100 [24], Oxford-IIIT-Flower [42] and Standford Cars [23]. Our approach followed the methodology of previous studies [7,50,53,56,57], where we initialized the model with ImageNet pre-trained weights and fine-tuned it on the new datasets.\n\nIn Table-6 of the main paper, we have presented a comparison of the transfer learning performance of our basic and best models with state-of-the-art CNNs and vision transformers. The transfer learning setup employs a batch size of 64, a learning rate (lr) of\n0.0001, a weight-decay of 1e-4, a clip-grad of 1, and warmup epochs of 5. We have utilized a pre-trained model trained on the Imagenet-1K dataset, which we have fine-tuned on the transfer learning dataset specified in table-10 for 1000 epochs.\n\n24\nB. N. Patro et al.\n"
    },
    {
        "level": "##",
        "title": "C.3 Task Learning: Object Detection",
        "content": "\nTraining setup: In this section, we examine the pre-trained SiMBA-H-small behavior on COCO dataset for two downstream tasks that localize objects ranging from boundingbox level to pixel level, i.e., object detection and instance segmentation. Two mainstream detectors, i.e., RetinaNet [29] and Mask R-CNN [19] as shown in table-8 of the main paper, and two state-of-the-art detectors i.e., GFL [26], and Cascade Mask R-CNN [1]\nin mmdetection [2] in this supplementary doc. We are employed for each downstream task, and we replace the CNN backbones in each detector with our SiMBA-H-small for evaluation. Specifically, each vision backbone is first pre-trained over ImageNet1K, and the newly added layers are initialized with Xavier [12]. Next, we follow the standard setups in [34] to train all models on the COCO train2017 (\u223c118K images). Here the batch size is set as 16, and AdamW [35] is utilized for optimization (weight decay: 0.05, initial learning rate: 0.0001, betas=(0.9, 0.999)). We used learning rate (lr) configuration with step lr policy, linear warmup at every 500 iterations with warmup ration 0.001. All models are finally evaluated on the COCO val2017 (5K images). For state-of-the-art models like GFL [26], and Cascade Mask R-CNN [1], we utilize 3 \u00d7 schedule (i.e., 36\nepochs) with the multi-scale strategy for training, whereas for RetinaNet [29] and Mask R-CNN [19] we utilize 1 \u00d7 schedule (i.e., 12 epochs).\n\n| Dataset     | CIFAR-10   | CIFAR-100   | Flowers-102   | Stanford Cars   |\n|-------------|------------|-------------|---------------|-----------------|\n| Train Size  | 50,000     | 50,000      | 8,144         | 2,040           |\n| Test Size   | 10,000     | 10,000      | 8,041         | 6,149           |\n| #Categories |            |             |               |                 |\n| 10          | 100        | 196         | 102           |                 |\n"
    },
    {
        "level": "##",
        "title": "C.4 Simba Architecture Details",
        "content": "\nThe architectural details of the simplified SiMBA-based context can be expressed as follows:\n\n$$\\begin{split}\\mathbf{X}_{l}&=\\mathbf{DP}(\\mathbf{M}\\mathbf{m}\\mathbf{b}(\\mathbf{X}_{1-1}))+\\mathbf{X}_{1-1},\\\\ \\mathbf{X}_{l}&=\\mathbf{DP}(\\mathbf{EinFFT}(\\mathbf{Norm}(\\mathbf{X}_{1})))+\\mathbf{X}_{l},\\end{split}\\tag{11}$$\n\nHere, $\\mathbf{SiMBA}$ represents the proposed Mamba block, L denotes the number of layers, and $\\mathbf{Norm}$ stands for the normalization layer. The process involves applying the Mamba block to the previous time step $\\mathbf{X}_{1-1}$, followed by a residual connection and dropout $(\\mathbf{DP})$. The resulting tensor is then normalized $(\\mathbf{Norm})$ and by applying the EinFFT operation subsequently, which is the proposed frequency-domain channel mixing operation. Finally, the tensor undergoes another dropout and is added to the previous state $\\mathbf{X}_{l}$. This overall structure is iteratively applied for multiple layers (1).\n\nIn our proposed SiMBA model, we employ a series of operations to process sequential data effectively. Here's a breakdown of the key components and their interactions:\n\n- Mamba Block (**\ud835\udc0c\ud835\udc1a\ud835\udc26\ud835\udc1b\ud835\udc1a**): The Mamba block is the fundamental building block of\nour model, responsible for capturing temporal dependencies within the input time\nseries. By applying this block to the previous time step \ud835\udc17\ud835\ude95\u22121, we aim to extract\nrelevant features and patterns crucial for forecasting. The residual connection with dropout (\ud835\udc03\ud835\udc0f) ensures the retention of essential information from preceding steps, enhancing the model's ability to learn complex temporal dynamics.\n- Normalization (**\ud835\udc0d\ud835\udc28\ud835\udc2b\ud835\udc26**): Normalizing the tensor \ud835\udc170\n\ud835\ude7b enhances the stability and convergence of the model during training. This step helps mitigate issues related to gradient vanishing or explosion, promoting smoother learning dynamics.\n- Frequency-Domain Channel Mixing (**\ud835\udc04\ud835\udc22\ud835\udc27\ud835\udc05\ud835\udc05\ud835\udc13**): The operation **\ud835\udc04\ud835\udc22\ud835\udc27\ud835\udc05\ud835\udc05\ud835\udc13** represents\na novel approach to exploit frequency-domain information for capturing intricate relationships among different channels within the data. By transforming the normalized tensor into the frequency domain, our model can effectively capture periodic patterns and complex inter-channel dependencies, thereby improving its forecasting capabilities.\n- Dropout (\ud835\udc03\ud835\udc0f): Dropout is applied after the frequency-domain channel mixing to\nprevent overfitting and enhance the model's generalization ability. This regularization technique aids in robust feature learning by reducing the model's sensitivity to noise in the data.\n- Residual Connection and Iteration: The residual connection adds the result of the\ndropout operation to the previous state \ud835\udc17\ud835\udc59, facilitating iterative refinement of temporal representations across multiple layers (\ud835\ude95). This iterative process enables the\nmodel to progressively learn hierarchical features and capture increasingly complex temporal dynamics.\nAs for the hyperparameters governing our architecture, we define:\n- \ud835\ude7b: The number of Mamba blocks, determining the depth of the model. - \ud835\ude73: The hidden state dimension, representing the dimensionality of the model's hidden states. - \ud835\ude7f: The expanded state dimension, determining the dimensionality of the expanded states within the model. - \ud835\ude7d: The sequence length of the input data sequence.\n\nour SiMBA architecture leverages Mamba blocks, frequency-domain channel mixing, normalization, and dropout to effectively process sequential data and capture intricate temporal dependencies. The defined hyperparameters control the model's depth, hidden state dimension, expanded state dimension, and attention mechanism dimensionality, thereby shaping its overall characteristics and performance.\n"
    },
    {
        "level": "##",
        "title": "Algorithm 1 Simba Block Process",
        "content": "\n1: **Input:** Patch sequence \ud835\udc17\ud835\udc59\u22121 : (\ud835\ude71, \ud835\ude7d, \ud835\ude73)\n2: **Output:** Patch sequence \ud835\udc17\ud835\udc59 : (\ud835\ude71, \ud835\ude7d, \ud835\ude73)\n3: /* normalize the input sequence \ud835\udc17\u2032\n\ud835\udc59\u22121 */\n4: \ud835\udc17\u2032\n\ud835\udc59\u22121 : (\ud835\ude71, \ud835\ude7d, \ud835\ude73) \u2190 **\ud835\udc0d\ud835\udc28\ud835\udc2b\ud835\udc26**(\ud835\udc59\u22121)\n5: \ud835\udc31 : (\ud835\ude71, \ud835\ude7d, \ud835\ude7f) \u2190 **\ud835\udc0b\ud835\udc22\ud835\udc27\ud835\udc1e\ud835\udc1a\ud835\udc2b**\ud835\udc31(\ud835\udc17\u2032\n\ud835\udc59\u22121)\n6: \ud835\udc33 : (\ud835\ude71, \ud835\ude7d, \ud835\ude7f) \u2190 **\ud835\udc0b\ud835\udc22\ud835\udc27\ud835\udc1e\ud835\udc1a\ud835\udc2b**\ud835\udc33(\ud835\udc17\u2032\n\ud835\udc59\u22121)\n7: /* process with different direction */\n8: **for** \ud835\udc5c in {loop} do\n9:\n\ud835\udc31\u2032\n\ud835\udc5c : (\ud835\ude71, \ud835\ude7d, \ud835\ude7f) \u2190 \ud835\udc12\ud835\udc22\ud835\udc0b\ud835\udc14(**\ud835\udc02\ud835\udc28\ud835\udc27\ud835\udc2f\ud835\udfcf\ud835\udc1d**\ud835\udc5c(\ud835\udc31))\n10:\n\ud835\udc01\ud835\udc5c : (\ud835\ude71, \ud835\ude7d, \ud835\ude7a) \u2190 \ud835\udc0b\ud835\udc22\ud835\udc27\ud835\udc1e\ud835\udc1a\ud835\udc2b\ud835\udc01\n\ud835\udc5c (\ud835\udc31\u2032\n\ud835\udc5c)\n11:\n\ud835\udc02\ud835\udc5c : (\ud835\ude71, \ud835\ude7d, \ud835\ude7a) \u2190 \ud835\udc0b\ud835\udc22\ud835\udc27\ud835\udc1e\ud835\udc1a\ud835\udc2b\ud835\udc02\n\ud835\udc5c (\ud835\udc31\u2032\n\ud835\udc5c)\n12:\n/* softplus ensures positive \ud835\udeab\ud835\udc5c */\n13:\n\ud835\udeab\ud835\udc5c : (\ud835\ude71, \ud835\ude7d, \ud835\ude7f) \u2190 log(1 + exp(\ud835\udc0b\ud835\udc22\ud835\udc27\ud835\udc1e\ud835\udc1a\ud835\udc2b\ud835\udeab\n\ud835\udc5c (\ud835\udc31\u2032\n\ud835\udc5c) + \ud835\udc0f\ud835\udc1a\ud835\udc2b\ud835\udc1a\ud835\udc26\ud835\udc1e\ud835\udc2d\ud835\udc1e\ud835\udc2b\ud835\udeab\n\ud835\udc5c ))\n14:\n/* shape of \ud835\udc0f\ud835\udc1a\ud835\udc2b\ud835\udc1a\ud835\udc26\ud835\udc1e\ud835\udc2d\ud835\udc1e\ud835\udc2b\ud835\udc00\n\ud835\udc5c is (\ud835\ude7f, \ud835\ude7a) */\n15:\n\ud835\udc00\ud835\udc5c : (\ud835\ude71, \ud835\ude7d, \ud835\ude7f, \ud835\ude7a) \u2190 \ud835\udeab\ud835\udc5c\n\u2a02 \ud835\udc0f\ud835\udc1a\ud835\udc2b\ud835\udc1a\ud835\udc26\ud835\udc1e\ud835\udc2d\ud835\udc1e\ud835\udc2b\ud835\udc00\n\ud835\udc5c\n17:\n\ud835\udc32\ud835\udc5c : (\ud835\ude71, \ud835\ude7d, \ud835\ude7f) \u2190 **\ud835\udc12\ud835\udc12\ud835\udc0c**(\ud835\udc00\ud835\udc5c, \ud835\udc01\ud835\udc5c, \ud835\udc02\ud835\udc5c)(\ud835\udc31\u2032\n\ud835\udc5c)\n16:\n\ud835\udc01\ud835\udc5c : (\ud835\ude71, \ud835\ude7d, \ud835\ude7f, \ud835\ude7a) \u2190 \ud835\udeab\ud835\udc5c\n\u2a02 \ud835\udc01\ud835\udc5c\n18: end for\n19: /* get gated \ud835\udc32\ud835\udc5c */\n20: \ud835\udc32\ud835\udc5c : (\ud835\ude71, \ud835\ude7d, \ud835\ude7f) \u2190 \ud835\udc32\ud835\udc5c\n\u2a00 **\ud835\udc12\ud835\udc22\ud835\udc0b\ud835\udc14**(\ud835\udc33)\n21: /* residual connection */\n22: \ud835\udc17\ud835\udc59 : (\ud835\ude71, \ud835\ude7d, \ud835\ude73) \u2190 \ud835\udc03\ud835\udc0f(**\ud835\udc0b\ud835\udc22\ud835\udc27\ud835\udc1e\ud835\udc1a\ud835\udc2b**\ud835\udc17(\ud835\udc32\ud835\udc5c)) + \ud835\udc17\ud835\udc59\u22121\n23: **\ud835\udc11\ud835\udc1e\ud835\udc2c** : (\ud835\ude71, \ud835\ude7d, \ud835\ude73) \u2190 \ud835\udc17\ud835\udc59\n24: \ud835\udc17\ud835\udc59 : (\ud835\ude71, \ud835\ude7d, \ud835\ude73) \u2190 **\ud835\udc05\ud835\udc05\ud835\udc13**(\ud835\udc17\ud835\udc59) 25: \ud835\udc17\ud835\udc59 : (\ud835\ude71, \ud835\ude7d, \ud835\ude72\ud835\ude8b, \ud835\ude72\ud835\ude8d) \u2190 **\ud835\udc2b\ud835\udc1e\ud835\udc1a\ud835\udc2b\ud835\udc2b\ud835\udc1a\ud835\udc27\ud835\udc20\ud835\udc1e**(\ud835\udc17\ud835\udc59), \ud835\ude73 = \ud835\ude72\ud835\ude8b \u00d7 \ud835\ude72\ud835\ude8d\n26: \ud835\udc161, \ud835\udc162 : (\ud835\ude72\ud835\ude8b, \ud835\ude72\ud835\ude8d, \ud835\ude72\ud835\ude8d) \u2190 shape of complex value **\ud835\udc0f\ud835\udc1a\ud835\udc2b\ud835\udc1a\ud835\udc26\ud835\udc1e\ud835\udc2d\ud835\udc1e\ud835\udc2b**\ud835\udc161,\ud835\udc162\n\ud835\udc59\n27: \ud835\udc011, \ud835\udc012 : (\ud835\ude72\ud835\ude8b, \ud835\ude72\ud835\ude8d) \u2190 shape of complex value **\ud835\udc0f\ud835\udc1a\ud835\udc2b\ud835\udc1a\ud835\udc26\ud835\udc1e\ud835\udc2d\ud835\udc1e\ud835\udc2b**\ud835\udc011,\ud835\udc012\n\ud835\udc59\n28: \ud835\udc17\ud835\udc45\ud835\udc52\ud835\udc4e\ud835\udc59\n\ud835\udc59\n: (\ud835\ude71, \ud835\ude7d, \ud835\ude72\ud835\ude8b, \ud835\ude72\ud835\ude8d) \u2190 *\ud835\udc45\ud835\udc52\ud835\udc3f\ud835\udc48*(\ud835\udc04\ud835\udc0c\ud835\udc0c(\ud835\udc161, \ud835\udc01\ud835\udc45\ud835\udc52\ud835\udc4e\ud835\udc59\n1\n)(\ud835\udc31\ud835\udc45\ud835\udc52\ud835\udc4e\ud835\udc59\n\ud835\udc59\n))\n29: \ud835\udc17\ud835\udc3c\ud835\udc5a\ud835\udc54\n\ud835\udc59\n: (\ud835\ude71, \ud835\ude7d, \ud835\ude72\ud835\ude8b, \ud835\ude72\ud835\ude8d) \u2190 *\ud835\udc45\ud835\udc52\ud835\udc3f\ud835\udc48*(\ud835\udc04\ud835\udc0c\ud835\udc0c(\ud835\udc161, \ud835\udc01\ud835\udc3c\ud835\udc5a\ud835\udc54\n1\n)(\ud835\udc31\ud835\udc3c\ud835\udc5a\ud835\udc54\n\ud835\udc59\n))\n30: \ud835\udc17\ud835\udc59 : (\ud835\ude71, \ud835\ude7d, \ud835\ude72\ud835\ude8b, \ud835\ude72\ud835\ude8d) \u2190 **\ud835\udc08\ud835\udc05\ud835\udc05\ud835\udc13**(\ud835\udc17\ud835\udc45\ud835\udc52\ud835\udc4e\ud835\udc59\n\ud835\udc59\n+ \ud835\udc57\ud835\udc17\ud835\udc3c\ud835\udc5a\ud835\udc54\n\ud835\udc59\n)\n31: \ud835\udc17\ud835\udc59 : (\ud835\ude71, \ud835\ude7d, \ud835\ude73) \u2190 \ud835\udc17\ud835\udc59 + \ud835\udc11\ud835\udc1e\ud835\udc2c\n32: Return: \ud835\udc17\ud835\udc59"
    }
]