[
    {
        "level": "#",
        "title": "Deep Clustering Evaluation: How To Validate Internal Clustering Validation Measures",
        "content": "\nZeya Wang \u22171 and Chenglong Ye \u20201\n1Dr. Bing Zhang Department of Statistics, University of Kentucky\n"
    },
    {
        "level": "##",
        "title": "Abstract",
        "content": "\nDeep clustering, a method for partitioning complex, high-dimensional data using deep neural networks, presents unique evaluation challenges. Traditional clustering validation measures, designed for low-dimensional spaces, are problematic for deep clustering, which involves projecting data into lower-dimensional embeddings before partitioning. Two key issues are identified: 1) the curse of dimensionality when applying these measures to raw data, and 2) the unreliable comparison of clustering results across different embedding spaces stemming from variations in training procedures and parameter settings in different clustering models.\n\nThis paper addresses these challenges in evaluating clustering quality in deep learning.\n\nWe present a theoretical framework to highlight ineffectiveness arising from using internal validation measures on raw and embedded data and propose a systematic approach to applying clustering validity indices in deep clustering contexts. Experiments show that this framework aligns better with external validation measures, effectively reducing the misguidance from the improper use of clustering validity indices in deep learning.\n\nKeywords: Deep clustering, Internal validation measures, Clustering evaluation, ACE, Admissible space\n"
    },
    {
        "level": "##",
        "title": "1 Introduction",
        "content": "\nClustering, a core task in unsupervised learning, groups entities based on similarities, proving essential across various applications from image analysis to data segmentation (LeCun *et al.*,\n1998; JAIN *et al.*, 1999). With advancements in deep learning, particularly in image processing, deep networks have excelled in label prediction and feature extraction from unlabeled data.\n\nThis progress has spawned deep clustering methods (Yang *et al.*, 2016; Ghasedi Dizaji *et al.*,\n2017; Caron *et al.*, 2018), which enhance traditional clustering techniques' scalability to highdimensional data by using deep networks to project data into a lower-dimensional latent feature space (or named embedding space). This projection facilitates data partitioning in this more manageable space, supported by innovative clustering loss designs and network structures, leading to a proliferation of successful clustering methods in diverse fields.\n\nEvaluating clustering results in machine learning is essential for ensuring algorithmic quality and optimal partitioning. This evaluation typically involves two types (Liu *et al.*, 2010): internal measures (also known as validity index), which assess clustering quality based on the data and outcomes without external information, and *external measures*, which compare results to known labels or \"ground truth\". The usage of external measures is often limited as such ground truth is frequently unavailable. See more details in Section 2.2. Internal measures often falter for high-dimensional data due to the notorious curse of dimensionality, making their application based on the raw input data (the generated score from which is referred to as the *raw score* in this paper) impractical for the majority of deep clustering problems. In addition to the data partitioning results, deep clustering algorithms yield embedded data, constituting a \"paired output\" alongside the partitioning results. Due to the significantly reduced dimensionality of the embedded data, many works in the literature (Wang *et al.*, 2018, 2021; Huang *et al.*, 2021b,a; Ronen *et al.*, 2022; Hadipour *et al.*, 2022; Li *et al.*, 2023) utilize internal measures based on the paired embedded data as a validation criterion (referred to as the *paired score* in this paper). Figure 1 illustrates these two evaluation approaches. Despite the ability of embedded data to mitigate the curse of dimensionality, the application of the *paired score* for calculating and comparing different partitioning results is problematic. The embedding space, where this embedded data resides, is influenced by training parameters and processes. Internal measures are typically designed under the assumption that the evaluated data comes from the same feature space. Consequently, this variation in embedding spaces hampers the precise reflection of partitioning quality and compromises the reliability of comparing internal measure values for partitioning results based on their respective paired embedding spaces. For instance, one model might disperse embedded data points across clusters with more separation but slight errors at the boundaries, while another could distribute data across clusters more compactly without any errors in classification. Despite its less precise partitioning, the first model might receive a higher score from an internal measure like the silhouette score, which evaluates based on distances within and between clusters. The questionable reliance on the *paired score* in much of the existing literature, as mentioned earlier, highlights the need to appropriately validate internal measures for assessing deep clustering performances. This paper provides a theoretical understanding that such comparisons across different embedding spaces may fail due to the embedding space discrepancy. Ideally, we want to compare clustering results based on one ideal embedding space. However, in real practice, we lack knowledge about which space is ideally separable. To address this problem, we propose a simple yet effective logic and strategy to guide the usage of internal measures in deep clustering evaluation.\n\nIn summary, our major contributions include: Theoretical Justifications: We provide formal theoretical proofs showcasing that employing both 1) the high-dimensional raw data and 2) separate embedded data paired with individual partitioning results for computing clustering validity measures does not ensure the convergence of the comparative relationship between clustering results to the truth. We also establish theoretical properties for identifying admissible embedding spaces among all embedding spaces obtained with clustering results. These properties serve as a foundational framework for developing a strategy to select optimal spaces. To the best of our knowledge, we are the first to explore the significance of feature spaces for evaluating deep clustering.\n\nEvaluation Strategy: Based on the theoretical analysis, we introduce a strategy for identifying admissible embedding spaces during evaluation. By combining the calculated internal measure scores from the chosen embedding spaces, we enhance the robustness of the evaluation results.\n\nThrough extensive experiments and ablation studies, focusing on scenarios such as hyperparameter tuning, cluster number selection, and checkpoint selection, we demonstrate the effectiveness and importance of the proposed framework for evaluating deep clustering methods.\n"
    },
    {
        "level": "##",
        "title": "2 Preliminaries 2.1 Deep Clustering",
        "content": "\nLet X = {x1, *\u00b7 \u00b7 \u00b7* xn} denote a collection of unlabeled n observations, where xi is i.i.d. generated from some unknown distribution PX. A clustering problem can be defined as partitioning these\n"
    },
    {
        "level": "##",
        "title": "Input Data  \ud835\udcb3",
        "content": "\nobservations into latent groups or clusters. We denote the unknown labels corresponding to the observations as Y = {y1, \u00b7 \u00b7 \u00b7 *, y*n}, where each yi \u2208 {1, \u00b7 \u00b7 \u00b7 *, K*} and K represents the number of the groups. Clustering techniques find a good mapping (up to permutations) from X to {1*, ..., K*}, which we represent as \u03d5 : X \u2192 {1*, ..., K*}. The outcomes of \u03d5 form a partition \u03c1 = {C1, \u00b7 \u00b7 \u00b7 , CK}\nof the index set {1, \u00b7 \u00b7 \u00b7 *, n*}, where \u02c6yi := \u03d5(xi) = k if and only if i \u2208 Ck for any k = 1*, ..., K* and i = 1*, ..., n*. Deep clustering approaches transform the high-dimensional space X to a significantly lower-dimensional space Z through an encoder network, denoted as f, that maps each xi *\u2208 X* to zi *\u2208 Z*. The reduced-dimension data space Z is often referred to in the literature as embedding space. In practice, f(\u00b7) can be built using a convnet or transformer encoder. Subsequently, clustering is performed on the lower-dimensional data Z := {z1, *\u00b7 \u00b7 \u00b7* zn} to generate labels Y . In this context, we employ g(\u00b7) : Z \u2192 Y to represent the mapping from Z to Y . Then the clustering algorithm \u03d5 can be expressed as a composition function \u03d5(\u00b7) = g(f(\u00b7)). Generally, existing deep clustering methods can be categorized into two classes: autoencoder-based and clustering deep neural network-based approaches (Min *et al.*, 2018). Please refer to Appendix A.1 for an in-depth literature review and additional details on various deep clustering methods.\n"
    },
    {
        "level": "##",
        "title": "2.2 Clustering Evaluation",
        "content": "\nExternal measures In clustering, partitions are autonomously learned without supervised labels, hindering a direct comparison with the actual partition on holdout sets, as commonly practiced in supervised learning. If true partition labels are available, external validation measures, which assess the similarity between estimated partition labels and true cluster labels, are employed.\n\nTwo widely used metrics for this purpose are normalized mutual information (NMI) and clustering accuracy (ACC) (see Appendix A.3 for definitions). External measures are primarily used for benchmarking, but their applicability is limited in many clustering evaluation settings due to the requirement for true labels. Despite its limited usage, considering it as a similarity measure with truth, in this paper, we will treat it as the \"truth\" measure in our analysis. Internal measures Internal measures, known as validity indices, are developed to evaluate clustering quality based on the intrinsic characteristics of data and the resulting partitions, without relying on external labels. Examples of these indices include the Silhouette score (Rousseeuw,\n1987), Calinski-Harabasz index (Cali\u00b4nski & Harabasz, 1974), Davies-Bouldin index (Davies &\nBouldin, 1979), Cubic clustering criterion (CCC) (Sarle, 1983), Dunn index (Dunn, 1974), Cindex\n(Hubert & Levin, 1976), SDbw index (Halkidi & Vazirgiannis, 2001), and CDbw index (Halkidi\n& Vazirgiannis, 2008). Given the data X and a resulting partition \u03c1, we use the notation \u03c0(\u03c1|X)\nto indicate the clustering validity index. Since the focus in this paper is on the embedding space, we use \u03c0(\u03c1|Z) to represent \u03c0(\u03c1|Z), which denotes the score based on the embedded data Z. For a comprehensive understanding of each index, including definitions and details, please refer to Appendix A.4.\n"
    },
    {
        "level": "##",
        "title": "3 Theoretical Analysis For Deep Clustering Evaluation",
        "content": "\nGiven the established preliminaries, in this section, we provide a theoretical analysis for deep clustering evaluation. The proofs substantiating the theorems and corollaries are available in Appendix A.2 for further reference.\n\nLemma 1. *[Theorem 1 in Beyer* et al. (1999)] Denote n random points {X1, ..., Xn} where each point Xi is a p-dimensional vector. Let X0 be a random query point that is chosen independently from {X1, ..., Xn}. Let f be the probability density function of any fixed distribution on R. For any distance function d*, define* dmax = maxi\u2208{1,...,n} d(Xi, X0) *and* dmin = mini\u2208{1,...,n} d(Xi, X0).\n\nGiven a fixed n, for any \u03f5 > 0, we have\n\n$$\\operatorname*{lim}_{p\\to\\infty}\\mathbb{P}({\\frac{d_{\\operatorname*{max}}}{d_{\\operatorname*{min}}}}\\leq1+\\epsilon)=1,$$\nwhere the expectation is taken over the product distribution f \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 f.\n\nTheorem 1. [Distance Meaningless in High Dimensions] The clustering validity index based on the high-dimensional space will go to 0 as the dimension increases.\n\nAs shown in Theorem 1, as the dimensionality increases, the distance between data points converges, rendering the computed similarities and dissimilarities between points in the input space X meaningless.\n\nCalculating distances based on the reduced embedding space Z has been used in the literature as an alternative when assessing the clustering quality. The common practice of utilizing paired embedding spaces to compare partitioning results \u03c1 (Figure 1) may lead to erroneous conclusions, as different deep clustering models often produce distinct latent spaces Z. Even within the same category of methods, variations in the training process, such as hyperparameters (e.g., learning rates), random initializations, and data shuffling, can further contribute to variations in Z. We will demonstrate in Theorem 2 that comparing different partitioning results based on their paired embedding spaces will fail, even when all the embedding spaces are ideal. Before stating the theorem, we provide some definitions.\n\nDefinition 1. Let \u03c1\u2217 denote the unknown true partition. For two partitions, \u03c1i is better than \u03c1j if V (\u03c1\u2217, \u03c1i) > V (\u03c1\u2217, \u03c1j), where we denote V as the external validation measure.\n\nLet \u03f1(X) denote the collection of all possible partitions on the given data X.\n\nDefinition 2. Define\n\n$A:=\\{(\\phi(X),\\phi^{{}^{\\prime}}(X))|\\phi(X),\\phi^{{}^{\\prime}}(X)\\in\\varrho(X),$\n\n$$(\\pi(\\phi|\\mathcal{Z})-\\pi(\\phi^{{}^{\\prime}}|\\mathcal{Z}))\\cdot(V(\\rho^{*},\\phi)-V(\\rho^{*},\\phi^{{}^{\\prime}}))\\geq0\\}$$\nas the set of pairs of partitions whose validity index ranking is consistent with the truth. A\nclustering validity index \u03c0 is \u03f5Z*-consistent* in space Z if\n\n$$\\operatorname*{lim}_{n\\to\\infty}\\mathbb{P}(A)=\\epsilon\\,\\varepsilon$$\nfor some constant \u03f5Z > 0.\n\nIn particular, \u03c0 is *inadmissible* if \u03f5Z < 0.5 and \u03c0 is *admissible* if \u03f5Z \u2265 0.5. In addition, \u03c0 is consistent if \u03f5Z = 1 and \u03c0 is *inconsistent* if \u03f5Z = 0.\n\nRemark 1. Note that the constant \u03f5Z depends on the space Z. In turn, we call a space Z\nadmissible for the validity index \u03c0 if \u03f5Z \u2265 0.5 and Z is *inadmissible* if \u03f5Z < 0.5.\n\nDefinition 3. A space Z is *as good as* another space Z\n\u2032 if PX(\u03c0(\u03d5(X)|Z)\u2212\u03c0(\u03d5(X)|Z\n\u2032) \u2265 0) \u2192 1\nfor any clustering method \u03d5, which we denote as Z \u2ab0 Z\n\u2032.\n\nRemark 2. It follows from the above definition that Z is not as good as Z\n\u2032 if P(\u03c0(\u03d5(X)|Z) \u2212\n\u03c0(\u03d5(X)|Z\n\u2032) \u2265 0) does not converge to 1, which we denote as Z \u227a Z\n\u2032. Note that Z \u227a Z\n\u2032 and Z\n\u2032 *\u227a Z* can happen simultaneously. For the purpose of theoretical analysis, for a pair of spaces\n(Z, Z\n\u2032), we only consider three cases: Z \u2ab0 Z\n\u2032, Z\n\u2032 *\u2ab0 Z* , or the two spaces are the same (denoted as Z = Z\n\u2032).\n\n**Definition 4**.: Two spaces $\\mathcal{Z},\\mathcal{Z}^{\\prime}$ are _distinguishable_ if the set\n\n$$B_{\\phi}:=\\left\\{\\max_{\\phi^{{}^{\\prime}}}\\left[\\pi(\\phi^{{}^{\\prime}}(X)|\\mathcal{Z})-\\pi(\\phi(X)|\\mathcal{Z})\\right]\\right.$$ $$\\left.<\\pi(\\phi(X)|\\mathcal{Z}^{{}^{\\prime}})-\\pi(\\phi(X)|\\mathcal{Z})\\right\\}$$\n\nsatisfies that $\\lim_{n\\rightarrow\\infty}\\mathbb{P}_{X}(B_{\\phi})=c_{\\phi}$ for any given $\\phi$, where $0<c_{\\phi}\\leq1$.\n\nTheorem 2. Consider two distinguishable spaces Z1, Z2 and a clustering validity index \u03c0 that is consistent in both Z1 and Z2. Assume that the partition \u03d51(X) is as good as \u03d52(X). Then P(\u03c0(\u03d51(X)|Z1) \u2265 \u03c0(\u03d52(X)|Z2)) does not always converge to 1.\n\nRemark 3. Theorem 2 implies that even in the most ideal case where \u03c0 is consistent with the truth, comparing the *paired scores* does not guarantee the rank consistency.\n\nIn Theorem 2, we show that comparing the goodness between the partitions \u03d5 := g(f(\u00b7)) and\n\u03d5\n\u2032 := g\n\u2032(f\n\u2032(\u00b7)) is not equivalent to comparing \u03c0(\u03d5(\u00b7)|f(\u00b7)) and \u03c0(\u03d5\n\u2032(\u00b7)|f\n\u2032(\u00b7)). In this endeavor, Theorem 3 motivates us to develop a more effective approach that can better align with external measures.\n\nTheorem 3. Consider two spaces Z1, Z2 and \u03c0 is admissible in both Z1 and Z2. For any pair of partitions \u03d51(X) and \u03d52(X), their validity indices under the two spaces are highly rank correlated.\n\nThat is,\n\n$\\lim_{n\\to\\infty}\\mathbb{P}\\left((\\pi(\\phi_{1}(X)|\\mathcal{Z}_{1})-\\pi(\\phi_{2}(X)|\\mathcal{Z}_{1}))\\right.$\n\n$$\\left.\\cdot(\\pi(\\phi_{1}(X)|\\mathcal{Z}_{2})-\\pi(\\phi_{2}(X)|\\mathcal{Z}_{2}))\\geq0)\\geq0.5.\\right.$$\nCorollary 1. Suppose we have M partitioning results to compare: \u03d51(X), ..., \u03d5M(X). Assume\n\u03c0 is admissible in both Z1 and Z2*. Then the scores* a := (\u03c0(\u03d51|Z1), ..., \u03c0(\u03d5L|Z1)) *and* b :=\n(\u03c0(\u03d51|Z2), ..., \u03c0(\u03d5M|Z2)) satisfies\n\n$\\lim\\mathbb{P}\\left(\\text{the rankings in}\\mathbf{a}\\text{and}\\mathbf{b}\\text{agree}\\right)$\n\n$$=\\left(1-\\left(\\epsilon_{\\mathcal{I}_{1}}+\\epsilon_{\\mathcal{I}_{2}}-2\\epsilon_{\\mathcal{I}_{1}}\\epsilon_{\\mathcal{I}_{2}}\\right)\\right)^{\\binom{L}{2}}\\text{.}$$\nRemark 4. As we can see, the probability is affected by M. When M increase, the probability P (the rankings in a and b agree) will converge to a small quantity. In fact, when M *\u2192 \u221e*, we have limM\u2192\u221e limn\u2192\u221e P ( rank correlation of a and b is 1) = 0 if \u03f5Z1 + \u03f5Z2 < 2. The only case limM\u2192\u221e limn\u2192\u221e P ( rank correlation of a and b is 1) = 1 is when \u03c0 is consistent in both Z1 and Z2, i.e., \u03f5Z1 = \u03f5Z2 = 1. It suggests that the choice of validity index \u03c0 itself is important for comparing multiple deep clustering results. If the validity index is not consistent, a large M will naturally make this task challenging, even infeasible.\n\nRemark 5. If two spaces satisfy that \u03f5Z1 = \u03f5Z2, then Theorem 3 still holds.\n"
    },
    {
        "level": "##",
        "title": "4 Proposed Strategy",
        "content": "\nIn practice, identifying a consistent space Z is often challenging and may be deemed impossible.\n\nConsequently, our objective is to detect a group of admissible spaces Z1*, . . . ,* ZL for the selected validity index, aiming for a rank measurement more likely to align with the external measure than not. To reduce variance in both detection and estimation, we employ an ensemble-style scoring scheme to estimate a final score across different spaces. A straightforward version of this ensemble-style score involves averaging the scores over all obtained embedding spaces, defined as the *pooled score* (Figure 1), which we include as a comparative approach. Based on these ideas, we introduce an Adaptive Clustering Evaluation (ACE) strategy for deep clustering assessment.\n\nLet \u03d5m = (Zm, \u03c1m) denote the outputs generated from m-th deep clustering trials, m = 1*, ..., M*.\n\nThese trials are conducted on the same task but may involve different algorithms or configurations.\n\nHere, {\u03c1m}M\nm=1, represents the clustering results that we evaluate. We propose a three-step algorithm, which is also presented in Algorithm 1. Step 1: Multimodality test.\n\nIntuitively, we expect an admissible space to be multimodal.\n\nIn this step, we introduce a procedure to select admissible spaces from the set {Zm}M\nm=1 by their capacity to exhibit multimodality in the data distribution. We employ the widely applied multimodality testing method known as the *Dip test* (Hartigan & Hartigan, 1985), which assesses the presence of more than one mode in the data distribution without assuming a specific form for the underlying distribution. We retain the models that are significantly multi-modal. More details of the Dip test are in Appendix A.5.1. Step 2: Space screening and grouping.\n\nFor each retained embedding space Zm, based on the chosen internal measure, we calculate the measure values across all clustering results, denoted as (\u03c0(\u03c11|Zm), \u03c0(\u03c12|Zm)*, ..., \u03c0*(\u03c1M|Zm)). Following Remark 5, as spaces with similar \u03f5Z\nvalues are highly rank correlated, we divide the retained spaces into groups based on their rank correlation. Identifying the group of spaces with the highest \u03f5Z is challenging since \u03f5Z depends on the unknown external measure. In practice, we rely on Definition 3 and select the group with the highest value of the validity index (see more details in Step 3). Considering the absence of prior knowledge about the number of groups, we adopt density-based clustering approaches Algorithm 1 Adaptive clustering evaluation (ACE) for deep clustering models Input: Clustering outputs \u03d5m = (Zm, \u03c1m), m \u2208 {1, \u00b7 \u00b7 \u00b7 *, M*}; internal measure \u03c0\n\n1: Multimodality test: for each Zm, perform Dip test and get the p-value, and apply a multiple\ntesting procedure to select retained spaces. To ease the notation, we still denote Z1, ..., ZM\nas the retained spaces.\n2: Space screening and grouping:\n1. For\neach\nretained\nembedding\nspace\nm\n\u2208\n{1*, ..., M*},\ncalculate\n\u03c0m\n=\n(\u03c0(\u03c11|Zm), \u03c0(\u03c12|Zm)*, ..., \u03c0*(\u03c1M|Zm)).\n2. Calculate rank correlation rmm\u2032 := *RankCorr*(\u03c0m, \u03c0m\u2032 ) for each pair (m, m\n\u2032).\n3. Based on the rank correlation matrix {rmm\u2032 }M\nm,m\u2032=1, perform density-based stage-wise\ngrouping (Appendix A.5.2) to divide the M embedding spaces into S mutually exclusive\nsubgroups {Gs}S\ns=1.\n3: Ensemble analysis:\n1. For each subgroup Gs, build an undirected graph Gs = (Vs, Es) where Vs = Gs and\nEs = {emm\u2032 }m,m\u2032\u2208Gs with emm\u2032 = rmm\u2032 for significantly positive-correlated spaces Zm\nand Zm\u2032 , else emm\u2032 = 0.\n2. For each Zm in the s-th group Gs, run a link analysis to get the rating w(s)\nm . Then\nm\u2208Gs w(s)\nm \u03c0(\u03c1m\u2032 |Zm) for each m\n\u2032 = 1, \u00b7 \u00b7 \u00b7 *, M*.\ncalculate \u03c0(\u03c1m\u2032 |Gs) = \ufffd\n3. Select Gs\u2217 = arg maxGs\n\ufffdM\nm\u2032=1 \u03c0(\u03c1m\u2032 |Gs)/M\nOutput: \u03c0(\u03c11|Gs\u2217), \u00b7 \u00b7 \u00b7 *, \u03c0*(\u03c1M|Gs\u2217)\nlike HDBSCAN (McInnes *et al.*, 2017) as suitable methods. These approaches are particularly well-suited because they eliminate the need to specify the number of groups and can identify outlier spaces during grouping. We aim to maintain a manageable number of selected spaces because including any inadmissible space can significantly impair the evaluation. Therefore, within spaces in the same group, we further create subgroups of spaces with similar scales. Hence, we have developed a stage-wise grouping scheme based on a density-based approach. In this algorithm, we initially group embedding spaces based on their rank correlations. Subsequently, we create subgroups, denoted as {Gs}S\ns=1, within the generated groups based on the score values of each space. Ultimately, among all these subgroups, we select the group of spaces that yields the highest aggregated measure score as the final evaluation result. Please refer to Appendix A.5.2 for more details on implementing the stage-wise algorithm. The subsequent section will discuss the aggregation of scores obtained from a subgroup of spaces. Step 3: Ensemble analysis.\n\nFor each subgroup with more than one space, we propose an ensemble analysis to obtain an aggregated score. Consider a subgroup G with mG embedding spaces denoted as {Zm}m\u2208G. Within the same subgroup, we treat each space as a vertex and represent the rank correlation between two spaces using an undirected graph, G. Thus, G = (*V, E*), where V is the vertex set of embedding spaces, and E is the edge with the magnitude of rank correlation *RankCorr*(Zm, Zm\u2032). For the edge set, we only connect the vertices representing spaces that are significantly rank correlated, determined through a multiple testing procedure. Note that in testing, our null hypothesis assumes that the correlation is non-positive. After obtaining the graph, we can run a link analysis to rate each space based on the magnitude of its link to other spaces. The basic idea is that a top-rated space in a subgroup should be a hub, demonstrating high rank correlation with many other spaces in the same subgroup. We consider implementing algorithms for link analysis (e.g., PageRank (Ding *et al.*, 2002)), and their details can be found in Appendix A.5.3. With this implementation, we obtain a rating wG\nm for each space.\n\nUsing these ratings, we generate a score by aggregating the scores of all the embedding spaces m\u2208G wG\nm\u03c0(\u00b7|Zm). In the case of a subgroup with only one space, we directly consider the scores from this space as the aggregated score. This way, within the subgroup, represented as \u03c0(\u00b7|G) = \ufffd\nwe generate a score based on a subgroup that rates the \"hub\" spaces higher. After obtaining\n\u03c0(\u03c1j|Gs) for each subgroup Gs, we ultimately select the subgroup Gs\u2217 where the vector of scores\n{\u03c0(\u03c11|Gs\u2217), \u00b7 \u00b7 \u00b7 *, \u03c0*(\u03c1M|Gs\u2217)} has the largest average value among all the subgroups. This ensures the selection of embedding spaces that are both highly rank correlated and have high scores.\n"
    },
    {
        "level": "##",
        "title": "5 Experiments",
        "content": "\nAs outlined in Section 2.1, deep clustering methods are broadly categorized into two types:\nautoencoder-based and clustering deep neural network-based approaches. In our experiments, we focus on evaluating two well-known methods from each category, namely *DEPICT* (Ghasedi Dizaji et al., 2017) 1 as a representative autoencoder-based approach and *JULE* (Yang *et al.*, 2016)\n2 as a prominent CDNN-based approach. We ran DEPICT and JULE source code on the datasets mentioned in their original papers. These datasets consist of COIL20 and COIL100\n(multi-view object image datasets) (Nene *et al.*, 1996), USPS and MNIST-test (handwritten digits datasets) (LeCun *et al.*, 1998), UMist, FRGC-v2.02, CMU-PIE, and Youtube-Face (YTF)\n(face image datasets) (Graham & Allinson, 1998; Sim *et al.*, 2002; Wolf *et al.*, 2011). USPS, MNIST-test, YTF, FRGC, and CMU-PIE are employed in both JULE and DEPICT papers, while COIL-20, COIL-100, and YTF are used exclusively in JULE. Table 3 provides details on sample size, image size, and the number of classes for all datasets. Additionally, we conducted experiments using another deep clustering method, *DeepCluster* (Caron *et al.*, 2018) , renowned for its success on large-scale datasets like ImageNet. In our experiment, we ran *DeepCluster* 3 on the validation set of ImageNet. Please see Appendix A.6.3 for implementation details.\n\nTo validate the concepts proposed in this paper, we conducted three experiments addressing critical aspects of deep clustering: hyperparameter tuning, determining the number of clusters, and checkpoint selection. The main text covers the results of the first two experiments, while detailed discussions and findings from the third experiment are available in Appendix A.6.4. Our experiments employed clustering validity indices, as outlined in Section 2.2, including Silhouette score, Calinski-Harabasz index, and Davies-Bouldin index\u2014with relevant results presented in the main text. Additionally, for the Silhouette score, we experimented with different distance metrics, including the commonly used Euclidean distance and cosine distance, to examine the impact of metric choices on evaluation performance. We also utilized cubic clustering criterion (CCC), Dunn index, Cindex, SDbw index, and CDbw index\u2014with corresponding results detailed in Appendix A.6.4. For evaluation, we assessed the performance of different approaches by comparing their ranking consistency using two external measure scores: normalized mutual information (NMI)\nand clustering accuracy (ACC), as introduced in Section 2.2. To quantify rank consistency, we reported Spearman's rank correlation coefficient (rs) and Kendall rank coefficient (\u03c4B), as defined in Appendix A.6.2. Experimental details can be found in Appendix A.6.3. We present scores calculated based on the input space as *raw score*; scores obtained from paired embeddings as paired score; scores obtained through pooling over all embeddings as *pooled score*; and scores derived from our proposed strategy represented as *ACE*.\n\nHyperparameter tuning In this context, we employ a grid search with m hyperparameter combinations, and focus on crucial parameters for *JULE* (learning rate and unfolding rate) and *DEPICT* (learning rate and balancing parameter). We train corresponding deep clustering models for each combination, calculating internal measure scores using chosen validity indices and evaluating performance across different scoring approaches. Table 1 reveals that, consistent with Theorem 1, scores computed on embedding spaces consistently outperform *raw scores* for both JULE and *DEPICT*. Additionally, Theorem 2 is validated, with *pooled scores* and *ACE* scores exhibiting higher NMI rank correlations than paired scores. *ACE* scores consistently yield the highest average rank correlation, affirming the efficacy of our proposed strategies. Similar results across various scenarios in Appendix A.6.4 underscore the unreliable nature of using paired scores for evaluation and the need for admissible spaces. Similar conclusions are drawn from the rank correlation with ACC reported in Appendix A.6.4, reinforcing our findings.\n\nUSPS\nYTF\nFRGC\nMNIST-test\nCMU-PIE\nUMist\nCOIL-20\nCOIL-100\nAverage\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nJULE: Calinski-Harabasz index\nRaw score\n0.58\n0.47\n0.79\n0.62\n-0.44\n-0.28\n0.81\n0.62\n-0.99\n-0.93\n-0.57\n-0.40\n-0.31\n-0.18\n0.32\n0.21\n0.02\n0.01\nPaired score\n0.17\n0.13\n0.52\n0.40\n-0.13\n-0.10\n0.49\n0.34\n-0.13\n-0.08\n0.70\n0.50\n0.53\n0.38\n0.20\n0.19\n0.29\n0.22\nPooled score\n0.84\n0.68\n0.91\n0.79\n0.29\n0.22\n0.82\n0.67\n0.94\n0.82\n0.81\n0.60\n0.62\n0.47\n0.89\n0.73\n0.77\n0.62\nACE\n0.80\n0.63\n0.90\n0.73\n0.39\n0.26\n0.87\n0.71\n0.98\n0.90\n0.81\n0.61\n0.60\n0.45\n0.95\n0.82\n0.79\n0.64\nJULE: Davies-Bouldin index\nRaw score\n-0.48\n-0.30\n-0.47\n-0.32\n-0.43\n-0.30\n-0.83\n-0.67\n-0.97\n-0.88\n-0.70\n-0.50\n-0.58\n-0.40\n-0.79\n-0.61\n-0.66\n-0.50\nPaired score\n-0.10\n-0.03\n-0.32\n-0.21\n-0.08\n-0.05\n-0.13\n-0.06\n0.26\n0.20\n0.62\n0.44\n0.61\n0.42\n0.43\n0.35\n0.16\n0.13\nPooled score\n-0.26\n-0.12\n-0.46\n-0.34\n0.11\n0.07\n-0.16\n-0.07\n0.92\n0.78\n0.30\n0.20\n-0.25\n-0.17\n-0.46\n-0.35\n-0.03\n-0.00\nACE\n-0.08\n-0.02\n-0.30\n-0.21\n0.22\n0.16\n0.73\n0.55\n0.10\n0.06\n0.38\n0.27\n0.23\n0.22\n0.48\n0.33\n0.22\n0.17\nJULE: Silhouette score (cosine distance)\nRaw score\n0.68\n0.51\n0.84\n0.69\n0.03\n0.01\n0.64\n0.49\n0.66\n0.50\n-0.46\n-0.34\n-0.14\n-0.11\n0.12\n0.08\n0.30\n0.23\nPaired score\n0.28\n0.22\n0.73\n0.56\n0.09\n0.06\n0.63\n0.47\n0.50\n0.36\n0.71\n0.50\n0.68\n0.50\n0.74\n0.54\n0.54\n0.40\nPooled score\n0.70\n0.56\n0.93\n0.81\n0.40\n0.27\n0.79\n0.64\n0.95\n0.85\n0.77\n0.56\n0.27\n0.16\n0.68\n0.52\n0.69\n0.55\nACE\n0.89\n0.73\n0.93\n0.83\n0.52\n0.35\n0.81\n0.66\n0.99\n0.93\n0.79\n0.59\n0.44\n0.38\n0.92\n0.78\n0.79\n0.66\nJULE: Silhouette score (euclidean distance)\nRaw score\n0.81\n0.62\n0.85\n0.70\n0.07\n0.04\n0.71\n0.53\n0.32\n0.29\n-0.45\n-0.32\n-0.13\n-0.05\n0.23\n0.15\n0.30\n0.24\nPaired score\n0.27\n0.20\n0.72\n0.55\n0.04\n0.03\n0.56\n0.41\n0.42\n0.30\n0.70\n0.50\n0.64\n0.46\n0.55\n0.41\n0.49\n0.36\nPooled score\n0.71\n0.58\n0.90\n0.77\n0.41\n0.28\n0.78\n0.63\n0.96\n0.85\n0.79\n0.57\n0.26\n0.16\n0.70\n0.54\n0.69\n0.55\nACE\n0.88\n0.72\n0.89\n0.75\n0.42\n0.28\n0.81\n0.65\n0.98\n0.90\n0.88\n0.70\n0.41\n0.36\n0.92\n0.78\n0.77\n0.64\nDEPICT: Calinski-Harabasz index\nRaw score\n-0.05\n-0.10\n0.73\n0.62\n0.43\n0.25\n0.43\n0.35\n-0.95\n-0.83\n0.12\n0.06\nPaired score\n0.76\n0.57\n0.44\n0.26\n0.76\n0.57\n0.89\n0.72\n0.49\n0.44\n0.67\n0.51\nPooled score\n0.96\n0.83\n0.53\n0.41\n0.90\n0.77\n0.96\n0.87\n0.61\n0.56\n0.79\n0.69\nACE\n0.91\n0.77\n0.56\n0.44\n0.94\n0.82\n0.96\n0.87\n0.96\n0.87\n0.87\n0.75\nDEPICT: Davies-Bouldin index\nRaw score\n0.05\n-0.10\n0.63\n0.48\n0.48\n0.32\n-0.01\n-0.03\n-0.14\n-0.18\n0.20\n0.10\nPaired score\n0.81\n0.59\n0.45\n0.31\n0.90\n0.74\n0.89\n0.72\n0.63\n0.59\n0.73\n0.59\nPooled score\n0.96\n0.88\n0.49\n0.35\n0.64\n0.48\n0.43\n0.32\n-0.77\n-0.61\n0.35\n0.28\nACE\n0.91\n0.82\n0.76\n0.58\n0.91\n0.79\n0.96\n0.87\n0.98\n0.92\n0.90\n0.80\nDEPICT: Silhouette score (cosine distance)\nRaw score\n0.37\n0.29\n0.68\n0.53\n0.68\n0.54\n0.80\n0.60\n0.46\n0.32\n0.60\n0.46\nPaired score\n0.81\n0.62\n0.45\n0.33\n0.90\n0.75\n0.89\n0.72\n0.77\n0.58\n0.76\n0.60\nPooled score\n0.96\n0.86\n0.68\n0.56\n0.94\n0.82\n0.97\n0.90\n0.93\n0.79\n0.90\n0.78\nACE\n0.97\n0.90\n0.71\n0.56\n0.94\n0.82\n0.97\n0.90\n0.94\n0.83\n0.91\n0.80\nDEPICT: Silhouette score (euclidean distance)\nRaw score\n0.50\n0.36\n0.76\n0.61\n0.57\n0.41\n0.74\n0.59\n-0.21\n-0.12\n0.47\n0.37\nPaired score\n0.73\n0.50\n0.47\n0.36\n0.79\n0.65\n0.86\n0.69\n0.59\n0.52\n0.69\n0.54\nPooled score\n0.96\n0.86\n0.65\n0.53\n0.94\n0.82\n0.97\n0.90\n0.92\n0.75\n0.89\n0.77\nACE\n0.97\n0.88\n0.65\n0.50\n0.95\n0.83\n0.98\n0.90\n0.94\n0.82\n0.90\n0.79\n\nQualitative analysis In both tasks, we analyze the rank correlation between retained spaces after the multimodality test, considering various indices (Figures 3 to 33). The observed grouping behavior varies with validity measures, and the number of generated spaces influences clustering outcomes, underscoring the impact of these factors. Additionally, we employ t-SNE plots (Van der Maaten & Hinton, 2008) to compare embedding spaces selected and excluded by *ACE* (Figures 4 to 34). Two representative examples respectively based on Silhouette score (cosine distance) with JULE and Calinski-Harabasz index with *DEPICT*, are presented in Figure 2. In these figures, selected spaces tend to exhibit more compact and well-separated clusters aligned with true labels, highlighting their superior clustering performance. Further details and discussions are available in Appendix A.6.4.\n\nDetermination of the number of clusters In this experiment, we address the challenge of an unknown number of clusters, denoted as K, in the clustering process across all datasets. Similar to the hyperparameter tuning experiment, we conduct a grid search to explore various values of K and identify the optimal one. Specifically, running both *JULE* and *DEPICT* with M = 10 evenly distributed values of K covering the true K, we compute internal measure scores from resulting pairs of embedded data and partitioning results. In Table 2, we find that, similar to hyperparameter tuning experiments, *ACE* scores consistently exhibit the highest average rank correlation, while *raw scores* yield the lowest correlation. Additionally, *ACE* and pooled scores, calculated by averaging over embedding spaces, achieve better correlation than paired scores across most scenarios. We also report the optimal number of clusters K obtained by each approach in brackets, revealing that *ACE* and *pooled scores* contribute to the choice of K. For instance, in DEPICT, *ACE* selects K = 40 and K = 50 for different indices for YTF with true K = 41, while *paired scores* suggest K = 5. Results for other indices and ACC comparison are reported in Appendix A.6.4, showing similar findings.\n\nUSPS (10)\nYTF (41)\nFRGC (20)\nMNIST-test (10)\nCMU-PIE (68)\nUMist (20)\nCOIL-20 (20)\nCOIL-100 (100)\nAverage\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nJULE: Calinski-Harabasz index\nRaw score\n0.44 (5)\n0.56 (5)\n0.95 (50)\n0.89 (50)\n-0.93 (10)\n-0.83 (10)\n0.43 (5)\n0.51 (5)\n-0.37 (10)\n-0.24 (10)\n-0.33 (5)\n-0.24 (5)\n0.74 (15)\n0.64 (15)\n0.53 (80)\n0.47 (80)\n0.18\n0.22\nPaired score\n0.65 (10)\n0.64 (10)\n0.1 (50)\n0.06 (50)\n-0.93 (15)\n-0.83 (15)\n0.64 (10)\n0.6 (10)\n-0.03 (20)\n-0.02 (20)\n-0.13 (5)\n-0.07 (5)\n0.76 (15)\n0.71 (15)\n0.74 (80)\n0.56 (80)\n0.22\n0.21\nPooled score\n0.65 (10)\n0.64 (10)\n0.9 (50)\n0.78 (50)\n-0.87 (15)\n-0.72 (15)\n0.64 (10)\n0.6 (10)\n0.9 (70)\n0.73 (70)\n-0.14 (5)\n-0.11 (5)\n0.74 (15)\n0.64 (15)\n0.72 (80)\n0.64 (80)\n0.44\n0.40\nACE\n0.65 (10)\n0.64 (10)\n0.93 (50)\n0.83 (50)\n-0.72 (15)\n-0.67 (15)\n0.64 (10)\n0.6 (10)\n0.88 (70)\n0.73 (70)\n-0.14 (5)\n-0.11 (5)\n0.74 (15)\n0.64 (15)\n0.79 (80)\n0.69 (80)\n0.47\n0.42\nJULE: Davies-Bouldin index\nRaw score\n-0.27 (45)\n-0.29 (45)\n0.92 (45)\n0.78 (45)\n0.87 (50)\n0.72 (50)\n-0.46 (45)\n-0.42 (45)\n0.72 (100)\n0.47 (100)\n0.19 (50)\n0.16 (50)\n-0.88 (45)\n-0.79 (45)\n-0.92 (20)\n-0.82 (20)\n0.02\n-0.02\nPaired score\n0.54 (15)\n0.38 (15)\n0.15 (50)\n0.17 (50)\n0.85 (45)\n0.67 (45)\n0.43 (10)\n0.29 (10)\n0.78 (100)\n0.56 (100)\n-0.08 (45)\n0.02 (45)\n-0.26 (40)\n-0.14 (40)\n-0.9 (20)\n-0.78 (20)\n0.19\n0.15\nPooled score\n0.98 (15)\n0.91 (15)\n0.83 (50)\n0.67 (50)\n0.82 (40)\n0.61 (40)\n0.79 (10)\n0.6 (10)\n0.82 (90)\n0.64 (90)\n-0.21 (45)\n-0.02 (45)\n-0.76 (50)\n-0.57 (50)\n-0.92 (20)\n-0.82 (20)\n0.29\n0.25\nACE\n0.98 (15)\n0.91 (15)\n0.83 (50)\n0.67 (50)\n0.87 (40)\n0.72 (40)\n0.79 (10)\n0.6 (10)\n0.85 (90)\n0.69 (90)\n-0.21 (45)\n-0.02 (45)\n-0.69 (50)\n-0.57 (50)\n-0.94 (20)\n-0.82 (20)\n0.31\n0.27\nJULE: Silhouette score (cosine distance)\nRaw score\n0.69 (20)\n0.51 (20)\n1.0 (50)\n1.0 (50)\n0.67 (30)\n0.5 (30)\n0.07 (10)\n0.02 (10)\n-0.28 (60)\n-0.11 (60)\n0.13 (50)\n0.07 (50)\n-0.52 (45)\n-0.43 (45)\n0.42 (200)\n0.24 (200)\n0.27\n0.23\nPaired score\n0.99 (10)\n0.96 (10)\n0.3 (50)\n0.22 (50)\n0.72 (25)\n0.61 (25)\n0.87 (10)\n0.69 (10)\n0.98 (70)\n0.91 (70)\n-0.07 (45)\n0.07 (45)\n0.52 (25)\n0.36 (25)\n0.39 (200)\n0.2 (200)\n0.59\n0.50\nPooled score\n0.95 (10)\n0.87 (10)\n0.98 (50)\n0.94 (50)\n0.68 (45)\n0.56 (45)\n0.96 (10)\n0.87 (10)\n0.98 (70)\n0.91 (70)\n-0.07 (45)\n-0.02 (45)\n0.71 (20)\n0.57 (20)\n0.41 (200)\n0.24 (200)\n0.70\n0.62\nACE\n0.95 (10)\n0.87 (10)\n0.98 (50)\n0.94 (50)\n0.7 (45)\n0.61 (45)\n0.96 (10)\n0.87 (10)\n0.98 (70)\n0.91 (70)\n-0.07 (45)\n-0.02 (45)\n0.74 (20)\n0.5 (20)\n0.46 (180)\n0.33 (180)\n0.71\n0.63\nRaw score\n0.56 (10)\n0.47 (10)\n1.0 (50)\n1.0 (50)\n-0.18 (10)\n-0.17 (10)\n0.61 (30)\n0.47 (30)\n0.55 (60)\n0.38 (60)\n0.19 (50)\n0.16 (50)\n-0.41 (30)\n-0.36 (30)\n0.39 (200)\n0.2 (200)\n0.34\n0.27\nPaired score\n0.85 (10)\n0.73 (10)\n0.33 (50)\n0.28 (50)\n0.72 (25)\n0.61 (25)\n0.88 (10)\n0.69 (10)\n0.96 (80)\n0.87 (80)\n0.07 (45)\n0.16 (45)\n0.55 (25)\n0.43 (25)\n0.44 (200)\n0.29 (200)\n0.60\n0.51\nPooled score\n0.95 (10)\n0.87 (10)\n0.97 (50)\n0.89 (50)\n0.68 (45)\n0.56 (45)\n0.95 (10)\n0.82 (10)\n0.98 (70)\n0.91 (70)\n0.14 (45)\n0.11 (45)\n0.76 (25)\n0.57 (25)\n0.47 (200)\n0.33 (200)\n0.74\n0.63\nACE\n0.95 (10)\n0.87 (10)\n0.98 (50)\n0.94 (50)\n0.78 (45)\n0.67 (45)\n0.95 (10)\n0.82 (10)\n0.98 (70)\n0.91 (70)\n0.14 (45)\n0.11 (45)\n0.71 (25)\n0.43 (25)\n0.47 (200)\n0.33 (200)\n0.74\n0.64\nRaw score\n0.46 (5)\n0.6 (5)\n-0.69 (5)\n-0.56 (5)\n-0.88 (10)\n-0.78 (10)\n0.46 (5)\n0.6 (5)\n-0.92 (10)\n-0.82 (10)\n-0.31\n-0.19\nPaired score\n0.46 (5)\n0.6 (5)\n-0.99 (5)\n-0.96 (5)\n-0.85 (10)\n-0.72 (10)\n0.44 (5)\n0.56 (5)\n-0.92 (10)\n-0.82 (10)\n-0.37\n-0.27\nPooled score\n0.46 (5)\n0.6 (5)\n-0.98 (5)\n-0.91 (5)\n-0.85 (10)\n-0.72 (10)\n0.46 (5)\n0.6 (5)\n0.44 (10)\n0.56 (10)\n-0.09\n0.03\nACE\n0.46 (5)\n0.6 (5)\n-0.66 (5)\n-0.51 (5)\n0.77 (30)\n0.61 (30)\n0.46 (5)\n0.6 (5)\n0.92 (80)\n0.82 (80)\n0.39\n0.42\nDEPICT: Davies-Bouldin index\nRaw score\n-0.39 (45)\n-0.42 (45)\n0.99 (50)\n0.96 (50)\n0.68 (50)\n0.39 (50)\n-0.22 (35)\n-0.16 (35)\n0.92 (100)\n0.82 (100)\n0.40\n0.32\nPaired score\n0.46 (5)\n0.6 (5)\n-0.78 (5)\n-0.64 (5)\n-0.85 (10)\n-0.72 (10)\n0.44 (5)\n0.56 (5)\n-0.1 (10)\n0.02 (10)\n-0.17\n-0.04\nPooled score\n0.6 (15)\n0.51 (15)\n0.88 (50)\n0.73 (50)\n-0.13 (20)\n-0.17 (20)\n0.74 (10)\n0.64 (10)\n0.92 (100)\n0.82 (100)\n0.60\n0.51\nACE\n0.62 (10)\n0.6 (10)\n0.95 (50)\n0.87 (50)\n0.77 (35)\n0.67 (35)\n0.78 (10)\n0.69 (10)\n0.96 (70)\n0.91 (70)\n0.82\n0.75\nDEPICT: Silhouette score (cosine distance)\nRaw score\n-0.13 (25)\n-0.11 (25)\n1.0 (50)\n1.0 (50)\n0.97 (45)\n0.89 (45)\n0.71 (15)\n0.56 (15)\n-0.43 (60)\n-0.33 (60)\n0.42\n0.40\nPaired score\n0.44 (5)\n0.56 (5)\n-0.7 (5)\n-0.6 (5)\n-0.85 (10)\n-0.72 (10)\n0.44 (5)\n0.56 (5)\n0.07 (10)\n0.11 (10)\n-0.12\n-0.02\nPooled score\n0.6 (15)\n0.51 (15)\n0.61 (40)\n0.47 (40)\n0.07 (25)\n0.06 (25)\n0.71 (10)\n0.64 (10)\n0.98 (80)\n0.91 (80)\n0.59\n0.52\nACE\n0.65 (15)\n0.64 (15)\n0.87 (40)\n0.78 (40)\n0.93 (35)\n0.83 (35)\n0.85 (10)\n0.78 (10)\n0.99 (80)\n0.96 (80)\n0.86\n0.80\nRaw score\n-0.34 (25)\n-0.29 (25)\n1.0 (50)\n1.0 (50)\n0.3 (50)\n0.11 (50)\n0.39 (10)\n0.33 (10)\n-0.43 (10)\n-0.33 (10)\n0.18\n0.16\nPaired score\n0.44 (5)\n0.56 (5)\n-0.61 (5)\n-0.47 (5)\n-0.85 (10)\n-0.72 (10)\n0.44 (5)\n0.56 (5)\n-0.12 (10)\n-0.02 (10)\n-0.14\n-0.02\nPooled score\n0.6 (15)\n0.51 (15)\n0.98 (50)\n0.91 (50)\n0.07 (25)\n0.06 (25)\n0.73 (10)\n0.69 (10)\n0.99 (80)\n0.96 (80)\n0.67\n0.63\nACE\n0.46 (5)\n0.6 (5)\n0.94 (40)\n0.87 (40)\n0.02 (25)\n0.06 (25)\n0.85 (10)\n0.78 (10)\n0.98 (80)\n0.91 (80)\n0.65\n0.64\n\nAblation studies In our two experiments, we conducted ablation studies to gain insights into crucial aspects of our proposed approach (see Appendix A.6.5). Our findings emphasize the significant role of the Dip test in enhancing *ACE*'s performance in specific tasks, while its impact on the *pooled score* remains marginal. Exploring different family-wise error rates (\u03b1) for edge inclusion in link analysis revealed consistent performance for different \u03b1, underscoring the robustness of *ACE* across varying \u03b1. The comparison of including all edges further highlighted the importance of the testing procedure for edge inclusion, as it led to significantly lower correlations in specific cases. Additionally, our examination of an alternative density-based clustering method, DBSCAN (Ester *et al.*, 1996), showcased comparable evaluation performance, but the simplicity of HDBSCAN made it the preferred choice for grouping in our approach. Lastly, the comparison between two link analysis algorithms (*HITS* (Kleinberg, 1999) and *PageRank*) favored *PageRank*, indicating slightly better performance, particularly due to its consideration of both incoming and outgoing links simultaneously. Collectively, these findings deepen our understanding of the components influencing *ACE*'s performance, offering valuable insights for its effective application across various clustering tasks.\n"
    },
    {
        "level": "##",
        "title": "6 Discussion And Future Work",
        "content": "\nThis paper addresses the challenges in evaluating deep clustering methods by introducing a theoretical framework that revisits traditional validation measures' limitations. The contributions encompass formal justifications, highlighting the necessity of rethinking evaluation approaches in the deep clustering setting, along with proposing a strategy based on admissible embedding spaces. Extensive experiments demonstrate the framework's effectiveness in scenarios such as hyperparameter tuning, cluster number selection, and checkpoint selection. Considering the complexity introduced in the deep clustering setting, the paper is primarily focused on providing a systematic guideline and insights for deep clustering evaluation. Different indices define clustering goodness in distinct ways, highlighting the need for a nuanced understanding of each metric, which we leave as future research. The *ACE* approach relies on the existence of admissible spaces, and challenges arise in scenarios with too few or even no admissible spaces. The proposed strategy, demonstrated to be effective with M = 10, can be adapted for scenarios with too few admissible spaces, as discussed in Appendix A.6.5. The challenging scenario of no admissible spaces is discussed in the checkpoint selection experiment (Appendix A.6.4), where despite no significant departure from unimodality, *pooled scores* outperform *paired scores* across all indices. This suggests that direct pooling could be a viable solution when M is small or no retained space after the multimodality test. Additionally, practitioners are encouraged to leverage empirical knowledge and exploratory data visualization techniques when deciding which spaces to incorporate. The analysis in Appendix A.6.4 underscores that effective spaces typically show compact and well-separated clusters. Our future work will further delve into providing detailed insights for various metrics in deep clustering evaluation.\n"
    },
    {
        "level": "##",
        "title": "References",
        "content": "\nAgresti, Alan. 2010. *Analysis of ordinal categorical data*. Vol. 656. John Wiley & Sons.\n\nBeyer, Kevin, Goldstein, Jonathan, Ramakrishnan, Raghu, & Shaft, Uri. 1999. When Is \"Nearest\nNeighbor\" Meaningful? *Pages 217\u2013235 of:* Beeri, Catriel, & Buneman, Peter (eds), Database\nTheory - ICDT'99. Berlin, Heidelberg: Springer Berlin Heidelberg.\nCali\u00b4nski, Tadeusz, & Harabasz, Jerzy. 1974. A dendrite method for cluster analysis. Communications in Statistics-theory and Methods, 3(1), 1\u201327.\nCaron, Mathilde, Bojanowski, Piotr, Joulin, Armand, & Douze, Matthijs. 2018. Deep clustering\nfor unsupervised learning of visual features. Pages 132\u2013149 of: Proceedings of European\nConference on Computer Vision.\nDavies, David L, & Bouldin, Donald W. 1979. A cluster separation measure. IEEE transactions\non pattern analysis and machine intelligence, 224\u2013227.\nDeng, Jia, Dong, Wei, Socher, Richard, Li, Li-Jia, Li, Kai, & Fei-Fei, Li. 2009. Imagenet: A\nlarge-scale hierarchical image database. Pages 248\u2013255 of: IEEE Conference on Computer\nVision and Pattern Recognition.\nDesgraupes, Bernard. 2013. Clustering indices. *University of Paris Ouest-Lab Modal'X*, 1(1), 34.\nDing, Chris, He, Xiaofeng, Husbands, Parry, Zha, Hongyuan, & Simon, Horst D. 2002. PageRank,\nHITS and a unified framework for link analysis. Pages 353\u2013354 of: Proceedings of the 25th\nannual international ACM SIGIR conference on Research and development in information\nretrieval.\nDunn, Joseph C. 1974. Well-separated clusters and optimal fuzzy partitions. *Journal of cybernetics*,\n4(1), 95\u2013104.\nEster, Martin, Kriegel, Hans-Peter, Sander, J\u00a8org, Xu, Xiaowei, *et al.* 1996. A density-based\nalgorithm for discovering clusters in large spatial databases with noise. *Pages 226\u2013231 of: kdd*,\nvol. 96.\nGhasedi Dizaji, Kamran, Herandi, Amirhossein, Deng, Cheng, Cai, Weidong, & Huang, Heng.\n2017. Deep clustering via joint convolutional autoencoder embedding and relative entropy\nminimization. Pages 5736\u20135745 of: Proceedings of IEEE International Conference on Computer\nVision.\nGraham, Daniel B, & Allinson, Nigel M. 1998. Characterising virtual eigensignatures for general\npurpose face recognition. *Pages 446\u2013456 of: Face Recognition*. Springer.\nHadipour, Hamid, Liu, Chengyou, Davis, Rebecca, Cardona, Silvia T, & Hu, Pingzhao. 2022.\nDeep clustering of small molecules at large-scale via variational autoencoder embedding and K-means. *BMC bioinformatics*, 23(4), 1\u201322.\nHagberg, Aric, Swart, Pieter, & S Chult, Daniel. 2008. Exploring network structure, dynamics,\nand function using NetworkX. Tech. rept. Los Alamos National Lab.(LANL), Los Alamos, NM\n(United States).\nHalkidi, Maria, & Vazirgiannis, Michalis. 2001. Clustering validity assessment: Finding the\noptimal partitioning of a data set. Pages 187\u2013194 of: Proceedings 2001 IEEE international\nconference on data mining. IEEE.\nHalkidi, Maria, & Vazirgiannis, Michalis. 2008. A density-based cluster validity approach using\nmulti-representatives. *Pattern Recognition Letters*, 29(6), 773\u2013786.\nHartigan, John A, & Hartigan, Pamela M. 1985. The dip test of unimodality. The annals of\nStatistics, 70\u201384.\nHennig, Christian. 2023. *fpc: Flexible Procedures for Clustering*. R package version 2.2-11.\nHolm, Sture. 1979. A simple sequentially rejective multiple test procedure. Scandinavian journal\nof statistics, 65\u201370.\nHuang, Yufang, Liu, Yifan, Steel, Peter AD, Axsom, Kelly M, Lee, John R, Tummalapalli,\nSri Lekha, Wang, Fei, Pathak, Jyotishman, Subramanian, Lakshminarayanan, & Zhang,\nYiye. 2021a. Deep significance clustering: a novel approach for identifying risk-stratified and\npredictive patient subgroups. *Journal of the American Medical Informatics Association*, 28(12),\n2641\u20132653.\nHuang, Yufang, Axsom, Kelly M, Lee, John, Subramanian, Lakshminarayanan, & Zhang, Yiye.\n2021b. DICE: Deep Significance Clustering for Outcome-Aware Stratification. arXiv preprint\narXiv:2101.02344.\nHubert, Lawrence J, & Levin, Joel R. 1976.\nA general statistical framework for assessing\ncategorical clustering in free recall. *Psychological bulletin*, 83(6), 1072.\nJAIN, AK, MURTY, MN, & FLYNN, PJ. 1999. Data Clustering: A Review. ACM Computing\nSurveys, 31(3).\nKendall, Maurice G. 1938. A new measure of rank correlation. *Biometrika*, 30(1/2), 81\u201393.\nKiefer, J. 1964. *The Advanced Theory of Statistics, Volume 2,\" Inference and Relationship.\"*.\nKleinberg, Jon M. 1999. Authoritative sources in a hyperlinked environment. Journal of the\nACM (JACM), 46(5), 604\u2013632.\nKnight, William R. 1966. A computer method for calculating Kendall's tau with ungrouped data.\nJournal of the American Statistical Association, 61(314), 436\u2013439.\nLangville, Amy N, & Meyer, Carl D. 2005. A survey of eigenvector methods for web information\nretrieval. *SIAM review*, 47(1), 135\u2013161.\nLeCun, Yann, Bottou, L\u00b4eon, Bengio, Yoshua, & Haffner, Patrick. 1998. Gradient-based learning\napplied to document recognition. *Proceedings of the IEEE*, 86(11), 2278\u20132324.\nLi, Shenghao, Guo, Hui, Zhang, Simai, Li, Yizhou, & Li, Menglong. 2023. Attention-based deep\nclustering method for scRNA-seq cell type identification. *PLOS Computational Biology*, 19(11), e1011641.\nLiu, Yanchi, Li, Zhongmou, Xiong, Hui, Gao, Xuedong, & Wu, Junjie. 2010. Understanding of\ninternal clustering validation measures. Pages 911\u2013916 of: 2010 IEEE international conference\non data mining. IEEE.\nMalika, Charrad, Ghazzali, Nadia, Boiteau, Veronique, & Niknafs, Azam. 2014. NbClust: an R\npackage for determining the relevant number of clusters in a data Set. *J. Stat. Softw*, 61, 1\u201336.\nMasci, Jonathan, Meier, Ueli, Cire\u00b8san, Dan, & Schmidhuber, J\u00a8urgen. 2011. Stacked convolutional\nauto-encoders for hierarchical feature extraction. Pages 52\u201359 of: International Conference on\nArtificial Neural Networks.\nMcInnes, Leland, Healy, John, & Astels, Steve. 2017. hdbscan: Hierarchical density based\nclustering. *The Journal of Open Source Software*, 2(11), 205.\nMin, Erxue, Guo, Xifeng, Liu, Qiang, Zhang, Gen, Cui, Jianjing, & Long, Jun. 2018. A survey of\nclustering with deep learning: From the perspective of network architecture. *IEEE Access*, 6,\n39501\u201339514.\nNene, Sameer A, Nayar, Shree K, Murase, Hiroshi, *et al.* 1996. Columbia object image library\n(coil-20).\nNeville, Zachariah, Brownstein, Naomi, Ackerman, Maya, & Adolfsson, Andreas. 2020. clusterability: Performs Tests for Cluster Tendency of a Data Set. R package version 0.1.1.0.\nPage, Lawrence, Brin, Sergey, Motwani, Rajeev, & Winograd, Terry. 1998. The pagerank citation\nranking: Bring order to the web. Tech. rept. Technical report, stanford University.\nPedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M.,\nPrettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher,\nM., Perrot, M., & Duchesnay, E. 2011. Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research, 12, 2825\u20132830.\n\nRonen, Meitar, Finder, Shahaf E, & Freifeld, Oren. 2022. Deepdpm: Deep clustering with an\nunknown number of clusters. Pages 9861\u20139870 of: Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition.\nRousseeuw, Peter J. 1987. Silhouettes: a graphical aid to the interpretation and validation of\ncluster analysis. *Journal of computational and applied mathematics*, 20, 53\u201365.\nSarle, WS. 1983. SAS Technical report a-108, cubic clustering criterion, SAS Institute Inc. URL:\nhttps://support. sas. com/documentation/onlinedoc/v82/techreport a108. pdf.\nSeabold, Skipper, & Perktold, Josef. 2010. statsmodels: Econometric and statistical modeling\nwith python. *In: 9th Python in Science Conference*.\nSim, Terence, Baker, Simon, & Bsat, Maan. 2002. The CMU pose, illumination, and expression\n(PIE) database. Pages 53\u201358 of: Proceedings of fifth IEEE international conference on automatic\nface gesture recognition. IEEE.\nSong, Chunfeng, Liu, Feng, Huang, Yongzhen, Wang, Liang, & Tan, Tieniu. 2013. Auto-encoder\nbased data clustering. *Pages 117\u2013124 of: Iberoamerican Congress on Pattern Recognition*.\nSpearman, Charles. 1961. The proof and measurement of association between two things.\nVan der Maaten, Laurens, & Hinton, Geoffrey. 2008. Visualizing data using t-SNE. Journal of\nmachine learning research, 9(11).\nVincent, Pascal, Larochelle, Hugo, Bengio, Yoshua, & Manzagol, Pierre-Antoine. 2008. Extracting\nand composing robust features with denoising autoencoders. Pages 1096\u20131103 of: Proceedings\nof the 25th international conference on Machine learning.\nWang, Jinghua, & Jiang, Jianmin. 2018.\nAn Unsupervised Deep Learning Framework via\nIntegrated Optimization of Representation Learning and GMM-Based Modeling. Pages 249\u2013\n265 of: Asian Conference on Computer Vision. Springer.\nWang, Yiqi, Shi, Zhan, Guo, Xifeng, Liu, Xinwang, Zhu, En, & Yin, Jianping. 2018. Deep\nembedding for determining the number of clusters. In: Proceedings of the AAAI Conference\non Artificial Intelligence, vol. 32.\nWang, Zeya, Ni, Yang, Jing, Baoyu, Wang, Deqing, Zhang, Hao, & Xing, Eric. 2021. DNB: A\njoint learning framework for deep Bayesian nonparametric clustering. IEEE Transactions on\nNeural Networks and Learning Systems, 33(12), 7610\u20137620.\nWolf, Lior, Hassner, Tal, & Maoz, Itay. 2011. Face recognition in unconstrained videos with\nmatched background similarity. Pages 529\u2013534 of: IEEE Conference on Computer Vision and\nPattern Recognition.\nYang, Bo, Fu, Xiao, Sidiropoulos, Nicholas D, & Hong, Mingyi. 2017.\nTowards k-meansfriendly spaces: Simultaneous deep learning and clustering. Pages 3861\u20133870 of: international\nconference on machine learning.\nYang, Jianwei, Parikh, Devi, & Batra, Dhruv. 2016.\nJoint unsupervised learning of deep\nrepresentations and image clusters. Pages 5147\u20135156 of: IEEE Conference on Computer Vision\nand Pattern Recognition.\nZwillinger, Daniel, & Kokoska, Stephen. 1999. CRC standard probability and statistics tables and\nformulae. Crc Press.\n"
    },
    {
        "level": "##",
        "title": "A Appendix. A.1 Deep Clustering Algorithm",
        "content": "\nDeep clustering encompasses the projection of high-dimensional data into a low-dimensional feature space using deep neural networks, followed by the partitioning of the embedded data within the feature space to generate cluster labels. The primary learning objective of most deep clustering methods typically involves minimizing a clustering loss through the generated embedded data. In this paper, we discuss two primary categories of deep clustering methods: autoencoder-based and clustering deep neural network (CDNN)-based approaches, as outlined in (Min *et al.*, 2018). The key distinction between these classes lies in the integration of autoencoders.\n\nThe autoencoder, a widely utilized neural network structure, is employed extensively for tasks involving reconstruction and feature extraction. Consisting of an encoder and a decoder, each of which can be either a fully-connected neural network or a convolutional neural network, the autoencoder's decoder architecture typically mirrors that of the encoder. The encoder compresses input data into an embedding space, while the decoder reconstructs the input data based on these embeddings. In methods utilizing autoencoders, cluster analysis is conducted using the embedded data from the encoder component (Song *et al.*, 2013; Yang *et al.*, 2017; Ghasedi Dizaji et al., 2017). Convolutional autoencoders, renowned for learning image representations by jointly minimizing both reconstruction loss and clustering loss, find frequent application in clustering tasks (Vincent *et al.*, 2008; Masci *et al.*, 2011; Ronen *et al.*, 2022).\n\nAnother category of deep clustering methods has emerged, aiming to jointly learn image clusters and embeddings without incorporating an autoencoder (Yang *et al.*, 2016; Ghasedi Dizaji et al., 2017; Caron *et al.*, 2018; Wang *et al.*, 2021). These methods demonstrate promising performance in recovering true labels. Within this category, some approaches either train or fine-tune data embeddings from autoencoders and estimate cluster structures using conventional clustering techniques like k-means (Yang *et al.*, 2017) and Gaussian mixture models (Wang & Jiang,\n2018). Others introduce an end-to-end clustering pipeline within a unified learning framework, enhancing model scalability by directly minimizing a clustering loss atop a network (Yang *et al.*,\n2016; Caron *et al.*, 2018; Wang *et al.*, 2021). CDNN-based methods, in particular, exclusively necessitate a clustering loss and involve an iterative procedure for jointly updating the network and estimating cluster labels. They can circumvent the need for a decoder, a requirement in autoencoder-based models, making CDNN-based methods more efficient. This efficiency enables their wider applicability to large-scale datasets (Caron *et al.*, 2018).\n\nIn the following sections, we provide more details regarding the deep clustering algorithms evaluated in this paper: *JULE* (Yang *et al.*, 2016), *DEPICT* (Ghasedi Dizaji *et al.*, 2017) and DeepCluster (Caron *et al.*, 2018).\n"
    },
    {
        "level": "##",
        "title": "A.1.1 Jule",
        "content": "\nJULE (Yang *et al.*, 2016) stands out as a joint unsupervised learning approach that employs agglomerative clustering techniques to train its feature extractor, deviating from the conventional use of autoencoders. *JULE* formulates joint learning within a recurrent framework. Here, the merging operations of agglomerative clustering serve as a forward pass for creating cluster labels, while the representation learning of deep neural networks constitutes the backward pass. JULE\nintroduces a unified weighted triplet loss, optimizing it end-to-end to concurrently estimate cluster labels and deep embeddings. In each epoch, *JULE* systematically merges two clusters, computing the loss for the backward pass. The proposed loss in *JULE* achieves a dual purpose: it reduces inner-cluster distances and simultaneously increases intra-cluster distances.\n"
    },
    {
        "level": "##",
        "title": "A.1.2 Depict",
        "content": "\nDEPICT (Ghasedi Dizaji *et al.*, 2017) follows an autoencoder-based framework. The approach includes stacking a multinomial logistic regression function on a multilayer convolutional autoencoder. *DEPICT* introduces a novel clustering loss designed to efficiently map data into a discriminative embedding subspace and precisely predict cluster assignments. This loss is defined through relative entropy minimization, further regularized by a prior on the frequency of cluster assignments. *DEPICT* employs a joint learning framework to concurrently minimize both the clustering loss and the reconstruction loss.\n"
    },
    {
        "level": "##",
        "title": "A.1.3 Deepcluster",
        "content": "\nDeepCluster is an end-to-end approach that simultaneously updates network parameters and image clusters. This method employs k-means on features extracted from large deep convolutional neural networks, such as AlexNet and VGG-16, to predict cluster assignments. Subsequently, it utilizes these cluster assignments as \"pseudo-labels\" to optimize the parameters of the convolutional neural networks. Successfully applied to extensive datasets like ImageNet (Deng *et al.*, 2009), this method has exhibited promising performance in learning visual features (Caron *et al.*, 2018).\n"
    },
    {
        "level": "##",
        "title": "A.2 Technical Proofs A.2.1 Proof Of Theorem 1",
        "content": "\nProof. By Lemma 1, the distance function is meaningless in high dimension since all the points has asymptotically the same distance to the query point. Thus, any distance-based clustering validity index will converge to 0.\n"
    },
    {
        "level": "##",
        "title": "A.2.2 Proof Of Theorem 2",
        "content": "\nProof. Since \u03c0 is a consistent score, we have \u03c0(\u03d51(X)|Z2) \u2265 \u03c0(\u03d52(X)|Z2).\n\n(1) If Z1 *\u2ab0 Z*2, by definition we have P(\u03c0(\u03d51(X)|Z1) \u2212 \u03c0(\u03d51(X)|Z2) \u2265 0) \u2192 1. Thus P(\u03c0(\u03d51(X)|Z1) \u2265 \u03c0(\u03d52(X)|Z2))\n\u2265P(\u03c0(\u03d51(X)|Z1) *> \u03c0*(\u03d51(X)|Z2) and \u03c0(\u03d51(X)|Z2) \u2265 \u03c0(\u03d52(X)|Z2))\n\u2265P(\u03c0(\u03d51(X)|Z1) *> \u03c0*(\u03d51(X)|Z2)) + P(\u03c0(\u03d51(X)|Z2) \u2265 \u03c0(\u03d52(X)|Z2)) \u2212 1\n\u21921 + 1 \u2212 1 = 1\nas n *\u2192 \u221e*.\n\n(2) If Z1 *\u227a Z*2, i) Consider the case where \u03d51(X) = \u03d52(X), i.e., \u03d51(X) and \u03d52(X) are the same.\n\n$$\\mathbb{P}(\\pi(\\phi_{1}(X)|\\mathcal{Z}_{1})-\\pi(\\phi_{2}(X)|\\mathcal{Z}_{2}))\\geq0)$$\n\n$$=\\mathbb{P}(\\pi(\\phi_{1}(X)|\\mathcal{Z}_{1})-\\pi(\\phi_{1}(X)|\\mathcal{Z}_{2}))\\geq0)$$\n\n$$=1-\\mathbb{P}(\\pi(\\phi_{1}(X)|\\mathcal{Z}_{1})-\\pi(\\phi_{1}(X)|\\mathcal{Z}_{2}))<0)$$\n\n$$\\to0.$$\nSo P(\u03c0(\u03d51(X)|Z1) \u2212 \u03c0(\u03d52(X)|Z2)) \u2265 0) does not converge to 1.\n\nii) Consider the case where \u03d51(X) \u0338= \u03d52(X), without loss of generality we assume \u03d51(X) >\n\u03d52(X). Then we have the following decomposition:\n\u03c0(\u03d51(X)|Z1) \u2212 \u03c0(\u03d52(X)|Z2) = [\u03c0(\u03d51(X)|Z1) \u2212 \u03c0(\u03d52(X)|Z1)] \u2212 [\u03c0(\u03d52(X)|Z2) \u2212 \u03c0(\u03d52(X)|Z1)] .\n\nThe first quantity [\u03c0(\u03d51(X)|Z1) \u2212 \u03c0(\u03d52(X)|Z1)] represents the clustering difference on space Z1, and the second quantity [\u03c0(\u03d52(X)|Z2) \u2212 \u03c0(\u03d52(X)|Z1)] represents the space difference. If the clustering difference is larger than the space difference, we then have \u03c0(\u03d51(X)|Z1) *> \u03c0*(\u03d52(X)|Z2).\n\nSince Z1 and Z2 are distinguishable, by definition we have P(max\u03d51 [\u03c0(\u03d51(X)|Z1) \u2212 \u03c0(\u03d52(X)|Z1)] <\n[\u03c0(\u03d52(X)|Z2) \u2212 \u03c0(\u03d52(X)|Z1)]) \u2192 c for some 0 *< c <* 1. So P(\u03c0(\u03d51(X)|Z1) \u2212 \u03c0(\u03d52(X)|Z2) > 0)\n=1 \u2212 P(\u03c0(\u03d51(X)|Z1) \u2212 \u03c0(\u03d52(X)|Z1) *< \u03c0*(\u03d52(X)|Z2) \u2212 \u03c0(\u03d52(X)|Z1))\n\u22641 \u2212 P(max\n\u03d51 [\u03c0(\u03d51(X)|Z1) \u2212 \u03c0(\u03d52(X)|Z1)] *< \u03c0*(\u03d52(X)|Z2) \u2212 \u03c0(\u03d52(X)|Z1))\n\u21921 \u2212 c < 1.\n\nIn summary, P(\u03c0(\u03d51(X)|Z1) *> \u03c0*(\u03d52(X)|Z2)) \u2192 1 happens only when Z1 *\u2ab0 Z*2.\n"
    },
    {
        "level": "##",
        "title": "A.2.3 Proof Of Theorem 3",
        "content": "\nProof. By definition we have lim n\u2192\u221e P((\u03c0(\u03d51(X)|Z1) \u2212 \u03c0(\u03d52(X)|Z1)) \u00b7 (V (\u03c1\u2217, \u03d51(X)) \u2212 V (\u03c1\u2217, \u03d52(X))) \u2265 0) = \u03f5Z1\nand lim n\u2192\u221e P((\u03c0(\u03d51(X)|Z2) \u2212 \u03c0(\u03d52(X)|Z2)) \u00b7 (V (\u03c1\u2217, \u03d51(X)) \u2212 V (\u03c1\u2217, \u03d52(X))) \u2265 0) = \u03f5Z2.\n\nThus, lim n\u2192\u221e P((\u03c0(\u03d51(X)|Z1) \u2212 \u03c0(\u03d52(X)|Z1)) \u00b7 (\u03c0(\u03d51(X)|Z2) \u2212 \u03c0(\u03d52(X)|Z2))) \u2265 0) = 1 \u2212 (\u03f5Z1 + \u03f5Z2 \u2212 2\u03f5Z1\u03f5Z2)\n\u2265 0.5\nsince \u03f5Z1,n \u2265 0.5 and \u03f5Z2,n \u2265 0.5.\n\nFor the special case where \u03c0 is consistent in both Z1 and Z2. We have \u03c0(\u03d51(X)|Z1) \u2265\n\u03c0(\u03d52(X)|Z1) a.s. if and only if \u03c0(\u03d51(X)|Z2) \u2212 \u03c0(\u03d52(X)|Z2) a.s.. Thus, P((\u03c0(\u03d51(X)|Z1) \u2212 \u03c0(\u03d52(X)|Z1)) \u00b7 (\u03c0(\u03d51(X)|Z2) \u2212 \u03c0(\u03d52(X)|Z2))) \u2265 0) = 1.\n"
    },
    {
        "level": "##",
        "title": "A.2.4 Proof Of Corollary 1",
        "content": "\ncomparison. Proof. To set up the rank among the m clusterings, we need to do\n\ufffdm\n2\n\ufffd\ntimes of pairwise For any i \u0338= j \u2208 {1*, ..., m*}, by definition we have lim n\u2192\u221e P((\u03c0(\u03d5i|Z1) \u2212 \u03c0(\u03d5j|Z1)) \u00b7 (V (\u03c1\u2217, \u03d5i) \u2212 V (\u03c1\u2217, \u03d5j)) \u2265 0) = \u03f5Z1\nand limn\u2192\u221e P((\u03c0(\u03d5i|Z2) \u2212 \u03c0(\u03d5j|Z2)) \u00b7 (V (\u03c1\u2217, \u03d5i) \u2212 V (\u03c1\u2217, \u03d5j)) \u2265 0) = \u03f5Z2. So for any fixed pair of (*i, j*), we have lim n\u2192\u221e P ((\u03c0(li|Z1) \u2212 \u03c0(lj|Z1)) \u00b7 (\u03c0(li|Z2) *> \u03c0*(lj|Z2))) = 1 \u2212 (\u03f5Z1 + \u03f5Z2 \u2212 2\u03f5Z1\u03f5Z2)\nand thus lim n\u2192\u221e P (the rankings in a and b agree)\n= lim n\u2192\u221e P ((\u03c0(li|S1) \u2212 \u03c0(lj|S1)) \u00b7 (\u03c0(li|S2) *> \u03c0*(lj|S2)) for all i \u0338= j \u2208 {1*, ..., m*})\n= (1 \u2212 (\u03f5Z1 + \u03f5Z2 \u2212 2\u03f5Z1\u03f5Z2))(\nm\n2) .\n"
    },
    {
        "level": "##",
        "title": "A.3 External Validation Measure",
        "content": "\nNormalized Mutual Information Normalized Mutual Information (NMI) is a widely adopted metric for gauging the similarity between two distinct cluster assignments, denoted by sets A\nand B. The NMI is computed using the formula:\n\n$$NMI(A;B)=\\frac{2\\times I(A;B)}{H(A)+H(B)}\\tag{1}$$\nHere, I denotes the mutual information between A and B, and H stands for the entropy function.\n\nThe NMI ranges between 0 (indicating no mutual information) and 1 (reflecting perfect correlation).\n\nIn the context of clustering performance evaluation, when provided with true partition labels denoted as Y and estimated partition labels denoted as \u02c6Y , we can leverage *NMI*(Y ; \u02c6Y ) as a reliable metric. Clustering accuracy Clustering accuracy (ACC) is defined as the proportion of correctly matched pairs resulting from the optimal alignment of true class labels and predicted cluster labels. The clustering accuracy of \u02c6Y with respect to Y is expressed as:\n\n$$ACC(Y,\\hat{Y})=\\max_{\\text{perm}\\in F}\\frac{\\sum_{i=0}^{N-1}I\\{\\text{perm}(\\hat{y}_{i})=y_{i}\\}}{N}\\tag{2}$$\nwhere P denotes the set of all permutations of partition indices. Like accuracy in classification, clustering ACC computes the ratio of correct predictions to total predictions. However, it differs from classification accuracy by utilizing the best one-to-one mappings between predicted class memberships and ground-truth ones.\n"
    },
    {
        "level": "##",
        "title": "A.4 Clustering Validity Indices",
        "content": "\nIn this section, we provide additional details for the clustering indices mentioned in the paper, which include the Silhouette score(Rousseeuw, 1987), Dunn index (Dunn, 1974; Desgraupes,\n2013),cubic clustering criterion (CCC) (Sarle, 1983), Cindex (CIND) (Hubert & Levin, 1976;\nDesgraupes, 2013), Calinski-Harabasz index (Cali\u00b4nski & Harabasz, 1974; Desgraupes, 2013), Davies-Bouldin index (DB) (Davies & Bouldin, 1979; Desgraupes, 2013), SDBW index (SDBW)\n(Halkidi & Vazirgiannis, 2001; Desgraupes, 2013), and CDbw index (CDbw) (Halkidi & Vazirgiannis, 2008). The data in Rp used for clustering and evaluation purposes is denoted as x1, \u00b7 \u00b7 \u00b7 *, x*N.\n\nHere, Ck represents the index set for the k-th cluster, and its size is denoted as nk.\n\nLet \u00b5{k} represent the barycenter of the observations in cluster Ck, and let \u00b5 denote the barycenter of all observations (Desgraupes, 2013).\n\n$$\\mu^{\\{k\\}}=\\frac{1}{n_{k}}\\sum_{i\\in C_{k}}x_{i}\\tag{3}$$ $$\\mu=\\frac{1}{N}\\sum_{i=1}^{N}x_{i}$$\n"
    },
    {
        "level": "##",
        "title": "A.4.1 Silhouette Score (Rousseeuw, 1987)",
        "content": "\nUsing a chosen distance function d(*i, j*) to calculate the distance between observations i and j\n(i.e., xi and xj), let a(i) represent the mean distance between the i-th observation and all other observations in the same cluster CI.\n\n$$a(i)=\\frac{1}{|C_{I}|}-1\\sum_{j\\in C_{I},i\\neq j}d(i,j)\\tag{4}$$\nLet b(i) represents the smallest mean distance of the i-th observation to all observations in any other cluster, where CJ represents clusters other than CI.\n\n$$b(i)=\\min_{J\\neq I}\\frac{1}{|C_{J}|}\\sum_{j\\in C_{J}}d(i,j)\\tag{5}$$\nThen, a silhouette value of the observation i can be defined as:\n\n$$s(i)=\\frac{b(i)-a(i)}{\\max\\{a(i),b(i)\\}}\\tag{6}$$ The silhouette score is defined as the mean of the mean silhouette value of a cluster throughout \nall clusters.:\n\n$$\\pi_{Silhouette}=\\frac{1}{K}\\sum_{k=1}^{K}\\frac{1}{N_{k}}\\sum_{i\\in C_{k}}s\\left(i\\right)\\tag{7}$$\n"
    },
    {
        "level": "##",
        "title": "A.4.2 Dunn Index (Dunn, 1974)",
        "content": "\nLet dmin represent the minimal distance between points of different clusters, and dmax denote the largest within-cluster distance. The distance dkk\u2032 between clusters Ck and Ck\u2032 is defined as the distance between their closest points:\n\n$$d_{kk^{\\prime}}=\\min_{\\begin{array}{c}i\\in C_{k}\\\\ j\\in C_{k^{\\prime}}\\end{array}}\\|x_{i}-x_{j}\\|\\tag{8}$$\nand dmin corresponds to the smallest among these distances dkk\u2032 :\n\n$$d_{\\min}=\\min_{k\\neq k^{\\prime}}d_{kk^{\\prime}}\\tag{9}$$\nFor each cluster Ck, let dk denote the largest distance between two distinct points within the cluster:\n\n$$d_{k}=\\max_{\\begin{subarray}{c}i,j\\in C_{k}\\\\ i\\neq j\\end{subarray}}\\|x_{i}-x_{j}\\|\\tag{10}$$\nand dmax corresponds to the largest of these distances dk :\n\n$$d_{\\max}=\\max_{1\\leq k\\leq K}d_{k}\\tag{11}$$\nThe Dunn index is defined as the quotient of dmin and dmax :\n\n$$\\pi_{Dunn}=\\frac{d_{\\rm min}}{d_{\\rm max}}\\tag{12}$$\n"
    },
    {
        "level": "##",
        "title": "A.4.3 Davies-Bouldin Index (Davies & Bouldin, 1979)",
        "content": "\nLet \u03b4k denote the mean distance of the points belonging to cluster Ck to their barycenter \u00b5{k}:\n\n$$\\delta_{k}=\\frac{1}{n_{k}}\\sum_{i\\in C_{k}}\\left\\|x_{i}-\\mu^{\\{k\\}}\\right\\|\\tag{13}$$\n\nLet $\\Delta_{kk^{\\prime}}$ denote the distance between the barycenters $\\mu^{\\{k\\}}$ and $mu^{\\{k^{\\prime}\\}}$ of clusters $C_{k}$ and \nCk\u2032.\n\n$$\\Delta_{kk^{\\prime}}=d\\left(\\mu^{\\{k\\}},\\mu^{\\{k^{\\prime}\\}}\\right)=\\left\\|\\mu^{\\{k^{\\prime}\\}}-\\mu^{\\{k\\}}\\right\\|\\tag{14}$$\n\nFor each cluster $k$, $M_{k}$ is defined as:\n\n$$M_{k}=\\max_{k^{\\prime}\\neq k}\\left(\\frac{\\delta_{k}+\\delta_{k^{\\prime}}}{\\Delta_{kk^{\\prime}}}\\right)\\tag{15}$$\n\nThe Davies-Bouldin index is the mean value of $M_{k}$ across all the clusters:\n\n$$\\pi_{Davies-Bouldin}=\\frac{1}{K}\\sum_{k=1}^{K}M_{k}\\tag{16}$$\n"
    },
    {
        "level": "##",
        "title": "A.4.4 Calinski-Harabasz Index (Cali\u00b4Nski & Harabasz, 1974)",
        "content": "\nThe within-cluster dispersion *WGSS*k is defined as the sum of squared distances between the\n\nobservations $x_{i\\,i\\in C_{k}}$ and the barycenter $\\mu^{k}$ of the cluster:\n\n$$WGSS^{\\{k\\}}=\\sum_{i\\,\\in C_{k}}\\|x_{i}-\\mu^{(k)}\\|^{2}=\\frac{1}{n_{k}}\\sum_{i<j\\in C_{k}}|x_{i}-x_{j}|^{2}\\tag{17}$$\nThen, the pooled within-cluster sum of squares (WGSS) is the sum of the within-cluster dispersions for all the clusters:\n\n$$WGSS=\\sum_{k=0}^{K}WGSS^{(k)}\\tag{18}$$\nDefine the between-group dispersion (BGSS) as the dispersion of the cluster centers $\\mu^{\\{k\\}}$ with \nrespect to the center \u00b5 of the entire dataset.\n\n$$BGS=\\sum_{k=1}^{K}n_{k}\\left\\|\\mu^{\\{k\\}}-\\mu\\right\\|^{2}\\tag{19}$$\n\nThe Calinski-Harabasz index is defined as:\n\n$$\\pi_{Calinski-Harabasz}=\\frac{BGS/(K-1)}{WGSS/(N-K)}\\tag{20}$$\n"
    },
    {
        "level": "##",
        "title": "A.4.5 Cindex (Hubert & Levin, 1976)",
        "content": "\n2\nrepresent the total number of pairs of distinct points in the cluster. Also, let NT = N(N\u22121)\n2\ndenote the total number of pairs of distinct points in the For cluster Ck, let NW = \ufffdK\nk=1\nnk(nk\u22121)\nwhole dataset.\n\nDefine SW as the sum of the NW distances between all pairs of points inside each cluster.\n\nDefine Smin as the sum of the NW smallest distances between all pairs of points in the whole dataset. There are NT such pairs: one takes the sum of the NW smallest values.\n\nDefine Smax as the sum of the NW largest distances between all pairs of points in the whole dataset. There are NT such pairs: one takes the sum of the NW largest values.\n\nThe C index is defined as:\n\n\u03c0*Cindex* = SW \u2212 Smin Smax \u2212 Smin (21)\n"
    },
    {
        "level": "##",
        "title": "A.4.6 Sdbw Index (Halkidi & Vazirgiannis, 2001)",
        "content": "\nConsider the vector of variances for each variable in the data set X = (xT\n1 , \u00b7 \u00b7 \u00b7 , xT\nn)T , which is defined as:\n\n${\\cal V}=diag(Cov(X))$ (22)\nFor the cluster Ck, let its associated data be denoted by Xk. Then, we have:\n\n$\\mathcal{V}^{(k)}=diag(Cov(X_{k}))$ (23)\nLet S be the mean of the norms of the vectors V(k) divided by the norm of vector V:\n\n$$\\mathcal{S}=\\frac{\\frac{1}{K}\\sum_{k=1}^{K}||\\mathcal{V}^{(k)}||}{||\\mathcal{V}||}\\tag{24}$$\n\nDefine $\\sigma$ as the square root of the sum of the norms of the variance vectors $\\mathcal{V}^{(k)}$ divided by \nthe number of clusters:\n\n$$\\sigma=\\frac{1}{K}\\sqrt{\\sum_{k=1}^{K}\\left\\|\\mathcal{V}^{(k)}\\right\\|}\\tag{25}$$\n\nThe density $\\gamma_{kk^{\\prime}}$ for a given point, with respect to two clusters $C_{k}$ and $C_{k^{\\prime}}$, is determined \nby the number of points in these two clusters whose distance to this point is less than \u03c3. In geometric terms, this involves considering the ball with a radius of \u03c3 centered at the given point and counting the number of points belonging to Ck \u222a Ck\u2032 located within this ball.\n\nFor each pair of clusters k and k\u2032, calculate the densities for the barycenters \u00b5{k} and \u00b5{k\u2032}\nof the clusters, as well as for their midpoint Hkk\u2032. Define the quotient Rkk\u2032 as the ratio between the density at the midpoint and the larger density of the two barycenters:\n\n$$R_{kk^{\\prime}}=\\frac{\\gamma_{kk^{\\prime}}\\left(H_{kk^{\\prime}}\\right)}{\\max\\left(\\gamma_{kk^{\\prime}}\\left(\\mu^{(k)}\\right),\\gamma_{kk^{\\prime}}\\left(\\mu^{(k^{\\prime})}\\right)\\right)}\\tag{26}$$\n\nDefine the between-cluster density $\\mathcal{G}$ as the average of the quotients $R_{kk^{\\prime}}$:\n\n$$\\mathcal{G}=\\frac{2}{K(K-1)}\\sum_{k<k^{\\prime}}R_{kk^{\\prime}}\\tag{27}$$\nThe SDbw index is defined as :\n\n$$\\pi_{SDbw}={\\cal S}+{\\cal G}\\tag{28}$$\n"
    },
    {
        "level": "##",
        "title": "A.4.7 Cubic Clustering Criterion (Sarle, 1983)",
        "content": "\nLet AN\u00d7K represent a one-hot encoding matrix for the clustering membership of the observations in the data set. Assuming X is the centered data, we can express this as:\n\n$$\\overline{X}=(A^{T}A)^{-1}A^{T}X\\tag{29}$$\nDefine the total-sample sum-of-square and crossproducts (SSCP) matrix as:\n\n$$T=X^{T}X\\tag{30}$$\nDefine the between-cluster SSCP matrix as:\n\n$$B=\\overline{X}^{T}A^{T}A\\overline{X}\\tag{31}$$\nThen the with-cluster SSCP matrix is defined as:\n\n$$W=T-B\\tag{32}$$\nThen the observed \u02c6R2 for the clustering result can be expressed as:\n\n$$\\hat{R}^{2}=1-\\frac{trace(W)}{trace(T)}\\tag{33}$$\nConsider approximating the value of R2 for a population uniformly distributed on a hyperbox.\n\nAssume that the edges of the hyperbox are aligned with the coordinate axes. Let sj be the edge length of the hyperbox along the j-th dimension, and given a sample X, sj is the square root of the j-th eigenvalue of T/(n \u2212 1). Assume further that the sj's are in decreasing order. Let v\u2217 be the volume of the hyperbox. If the hyperbox is divided into q (i.e., K) hypercubes with edge length c, then the volume of the hyperbox equals the total volume of the hypercubes. uj represents the number of hypercubes along the j-th dimension of the hyperbox. Let p\u2217 be the largest integer less than q such that u\u2217\np is not less than one. Hence, we have\n\n$$v^{*}=\\prod_{j=1}^{p^{*}}s_{j},$$ $$c=\\left(\\frac{v^{*}}{q}\\right)^{\\frac{1}{p^{*}}},\\tag{34}$$ $$u_{j}=\\frac{s_{j}}{c},$$\nThen, we can derive the following small-sample approximation for the expected value of R2:\n\n$$E\\left(R^{2}\\right)=1-\\left[\\frac{\\sum_{j=1}^{p^{*}}\\frac{1}{n+w_{j}}+\\sum_{j=p^{*}+1}^{p^{*}}\\frac{w_{j}^{*}}{n+w_{j}}}{\\sum_{j=1}^{p^{*}}\\frac{w_{j}^{2}}{n_{j}}}\\right]\\left[\\frac{\\left(n-q\\right)^{2}}{n}\\right]\\left[1+\\frac{4}{n}\\right].\\tag{35}$$\n\nThe CCC is computed as\n\n$$\\pi_{CCC}=\\ln\\left[\\frac{1-E\\left(R^{2}\\right)}{1-\\hat{R}^{2}}\\right]\\frac{\\sqrt{\\frac{w_{p^{*}}}{2}}}{\\left(0.001+E\\left(R^{2}\\right)\\right)^{1.2}}\\tag{36}$$\n"
    },
    {
        "level": "##",
        "title": "A.4.8 Cdbw Index (Halkidi & Vazirgiannis, 2008)",
        "content": "\nConsider C as a partitioning of the data. Let Vk be the set of representative points for cluster Ci, capturing the geometry of the Ci. A representative point vik of cluster Ci is deemed the closest representative in Ci to the representative vjl of cluster Cj, denoted as *closest rep*i(vjl), if vik is the representative point of Ci with the minimum distance from vjl. The respective Closest Representative points (RCRij) between Ci and Cj are defined as the set of mutual closest representatives of the two clusters. Let clos repp ij = (vik, vjl) be the p-th pair of respective closest representative points of clusters Ci and Cj.\n\nThe density between clusters Ci and Cj is defined as follows:\n\n$$\\mathrm{Dens}\\left(\\mathrm{C}_{\\mathrm{i}},\\mathrm{C}_{\\mathrm{j}}\\right)=\\frac{1}{|\\mathrm{RCR}_{\\mathrm{ij}}|}\\sum_{\\mathrm{t}=1}^{|\\mathrm{RCR}_{\\mathrm{ij}}|}\\left(\\frac{\\mathrm{d}\\left(\\mathrm{d}\\left(\\mathrm{d}\\mathrm{s}_{-}\\mathrm{rep}_{\\mathrm{ij}}^{\\mathrm{p}}\\right)}{2\\cdot\\mathrm{stdev}}\\right)\\cdot\\mathrm{cardinality}\\left(\\mathrm{u}_{\\mathrm{ij}}^{\\mathrm{p}}\\right)\\right)\\tag{37}$$\n\nwhere $d\\left(\\mathit{d}\\mathrm{s}_{-}\\mathit{rep}_{\\mathrm{ij}}^{\\mathrm{p}}\\right)$ denotes the Euclidean distance between the pair of points defined by $\\mathit{d}\\mathrm{s}_{-}\\mathit{rep}_{\\mathrm{ij}}^{\\mathrm{p}}\\in\\mathit{RCR}_{\\mathrm{ij}}$, $|\\mathrm{RCR}_{\\mathrm{ij}}|$ represents the cardinality of the set $\\mathrm{RCR}_{\\mathrm{ij}}$, and the term $\\mathit{stdev}$\nclos repp ij \u2208 *RCR*ij, |RCRij| represents the cardinality of the set RCRij, and the term stddev indicates the average standard deviation of the considered clusters. The cardinality\n\ufffd\nup ij\n\ufffd\ndenotes the average number of points in Ci and Cj that belong to the neighborhood of up ij.\n\nThe inter-cluster density is defined to measure, for each cluster Ci \u2208 C, the maximum density between Ci and the other clusters in C:\n\nInter_dens(**C**) = $\\frac{1}{c}\\sum_{i=1}^{c}\\max_{\\begin{subarray}{c}j=1,\\ldots,c\\\\ j\\neq i\\end{subarray}}\\left\\{\\text{Dens}\\left(C_{i},C_{j}\\right)\\right\\}$ (38)\nCluster separation (Sep) is defined to measure the separation of clusters, considering the inter-cluster density in relation to the distance between clusters:\n\n$$\\text{Sep}(\\textbf{C})=\\frac{\\frac{1}{c}\\sum_{i=1}^{c}\\min_{j\\neq i}\\left\\{\\text{Dist}\\left(C_{i},C_{j}\\right)\\right\\}}{1+\\text{Inter.dens}(\\textbf{C})},\\quad c>1,c\\neq n\\tag{39}$$\n\nwhere $\\text{Dist}(C_{i},C_{j})=\\frac{1}{|\\textit{RC}R_{ij}|}\\sum_{j=1}^{|\\textit{RC}R_{ij}|}d\\left(\\textit{close.rep}_{ij}^{n}\\right)$.\n\nThen the relative intra-cluster density w.r.t.a shrinkage factor $s$ is defined as follows:\n\n$$\\text{Intra.dens}(\\textbf{C},s)=\\frac{\\text{Dens.cl}(\\textbf{C},s)}{c\\cdot\\text{stdev}},c>1\\tag{40}$$\n\nwhere $\\text{Dens.cl}(\\textbf{C},s)=\\frac{1}{r}$\nThe cardinality of a point vij represents the proportion of points in cluster Ci that belong\n\n$\\frac{1}{\\mathrm{r}}\\sum_{\\mathrm{i}=1}^{\\mathrm{c}}\\sum_{\\mathrm{j}=1}^{\\mathrm{r}}cardinality\\left(v_{ij}\\right)$\n\noint $v_{ij}\\,$ represents the proportion.\n\nto the neighborhood of a representative vij determined by a factor s (i.e., the representatives of Ci shrunk by s), where the neighborhood of a data point, vij, is defined to be a hypersphere centered at it with a radius equal to the average standard deviation of the considered clusters, stdev.\n\nThe compactness of a clustering C in terms of density is defined as:\n\n$$\\text{Compactness}(\\textbf{C})=\\sum_{s}\\text{Intra\\_dens}(\\textbf{C},s)/n_{s}\\tag{41}$$\n\nwhere $n_{s}$ represents the number of different values that the factor $s$ takes, determining the density \nat various areas within clusters.\n\nIntra-density changes is defined to measure the changes of density within clusters:\n\n$$\\text{Intra\\_change}(\\mathbf{C})=\\frac{\\sum_{i=1}^{n_{s}}\\mid\\text{Intra\\_dens}\\left(\\mathbf{C},\\mathbf{s}_{i}\\right)-\\text{Intra\\_dens}\\left(\\mathbf{C},\\mathbf{s}_{i-1}\\right)\\mid}{\\left(\\mathbf{n}_{s}-1\\right)}\\tag{42}$$\n\nColesion is defined to measure the density within clusters w.r.t. the density changes observed\nwithin them:\n\n$$\\text{C cohesion}(\\textbf{C})=\\frac{\\text{Compactness}(\\textbf{C})}{1+\\text{Intra\\_change}(\\textbf{C})}\\tag{43}$$\nSC (Separation w.r.t. Compactness) is defined to evaluate the clusters' separation (the density between clusters) w.r.t. their compactness (the density within the clusters:\n\nSC(**C**) = Sep(**C**) $\\cdot$ Compactness (**C**) (44)\nThen the CDbw index is defined as:\n\n$\\pi_{CDbw}({\\bf C})=$ Cohesion $({\\bf C})\\cdot{\\rm SC}({\\bf C}),{\\rm c}>1$ (45)\n"
    },
    {
        "level": "##",
        "title": "A.5 Additional Algorithms Details A.5.1 Dip Statistics ((Hartigan & Hartigan, 1985))",
        "content": "\nIn our quality assessment of each Zm, the initial step involves ensuring that the embedded data Zm is clusterable. Various methods have been developed for testing clusterability, typically achieving this by identifying the presence of more than one mode in the data distribution. This can be accomplished through kernel density estimation or testing order statistics, intervals, or distribution functions. In this paper, we opt for a widely applied multimodality testing method known as the *Dip test*.\n\nThis method refrains from assuming any specific form for the underlying data distribution, making it straightforward to implement. The Dip test is designed to estimate the discrepancy between the cumulative distribution function (CDF) of the data and the nearest multimodal function. For a given CDF F(z), the Dip D(F) is defined as infG\u2208A supx |F(z) \u2212 G(z)|, where A represents the class of unimodal CDFs. Considering the empirical CDF Fn(x) of the embedded data z1, z2, \u00b7 \u00b7 \u00b7 *, z*n, the Dip of Fn(x) asymptotically converges to the Dip of F (i.e., D(Fn) \u2192 D(F)). In the Dip test, a uniform distribution *Unif*(0, 1)\nis chosen as a\"null\" model. Hartigan and Hartigan (Hartigan & Hartigan, 1985) conjectured that *Unif*(0, 1) is the \"asymptotically least favorable\" unimodal distribution\u2014essentially, the most challenging to distinguish from multimodal distributions as n increases. The Dip of the empirical CDF can be obtained through an O(n) algorithm. For detailed implementation, please refer to Appendix A.5.1. Following that, p-values for the Dip test under the null hypothesis that F is a unimodal distribution are derive through Monte Carlo simulations with *Unif*(0, 1). From these computed p-values from different Zm, with a multiple testing procedure (specifically, the Holm\u2013Bonferroni method with family-wise error rate (FWER) of 0.05 applied in this paper (Holm,\n1979)), we will select only those embedding spaces that reject the null hypothesis, indicating that F is not unimodal.\n\nThe Dip statistic, denoted as D(F), for the empirical cumulative distribution function (CDF)\ncan be computed using the analogy of stretching a taut string. Further details of the algorithm can be found below:\n\n1. Set: zL = z1, zU = zn, D = 0.\n2. Calculate the greatest convex minorant G and least concave majorant L for F in [zL, zU];\nsuppose the points of contact with F are respectively g1, g2, \u00b7 \u00b7 \u00b7 , gi *\u00b7 \u00b7 \u00b7* and l1, l2, \u00b7 \u00b7 \u00b7 , lj, *\u00b7 \u00b7 \u00b7* .\n3. Suppose d = sup |G(gi) \u2212 L(gi)| > sup |G(li) \u2212 L(li)| and that the supreme occurs at\nlj \u2264 gi \u2264 lj+l. Define z0\nL = gi, z0\nU = lj+l.\n4. Suppose d = sup |G(li) \u2212 L(li)| \u2265 sup |G(gi) \u2212 L(gi)| and that the supreme occurs at\ngi \u2264 lj \u2264 gi+l. Define z0\nL = gi, z0\nU = lj.\n5. If d \u2264 D, stop and set D(F) = D.\n6. If *d > D*, set D = sup{D, supzL\u2264z\u2264z0\nL |G(z) \u2212 F(z)|, supz0\nU\u2264z\u2264zU |L(z) \u2212 F(z)|}.\n7. Set zU = z0\nU, zL = z0\nL and return to step 2.\n"
    },
    {
        "level": "##",
        "title": "A.5.2 Stage-Wise Clustering",
        "content": "\nThe details of the stage-wise clustering algorithm can be seen in Algorithm 2.\n\nAlgorithm 2 Algorithm for stage-wise clustering\n\n1: Input:\n{corr(i, j)}i,j\u2208M and {\u03c0i}i\u2208M, where M is the set of retained spaces after multimodality test\n\n2: Phase 1: clustering based on rank correlations:\n1. For each *i, j* \u2208 M \u2032, define the distance dij = 1 \u2212 corr(*i, j*)\n2. Run density-based clustering method based on {di,j}i,j\u2208M \u2032\n3. Return S*phase*1 groups of spaces {G*phase*1\ns\n}S*phase*1\ns=1\n(each G*phase*1\ns\n\u2286 M) excluding outlier\nspaces\n3: Phase 2: clustering based on score values\n1. For each group G*phase*1\ns\n, apply density-based clustering on {\u03c0i}i\u2208G*phase*1\ns\n2. For each group G*phase*1\ns\n, generate subgroups SG(s)\n1 , *\u00b7 \u00b7 \u00b7* SG(s)\nN s and outlier spaces\n{O(s)\nl\n}Ls\nl=1 \u2286 G*phase*1\ns\n3. Treat each outlier space O(s)\nl\nas a singleton subgroup. Incorporate these singleton\nsubgroups with all the subgroups {SG(s)\nn }N s\nn=1 created for all groups s to obtain S\nmutually exclusive subgroups {Gs}S\ns=1\n4: Output:\n{Gs}S\ns=1, where each Gs \u2286 M \u2032\nNote that in Algorithm 2, we omit outlier spaces from density-based clustering in the first phase, treating them as rank uncorrelated spaces. In the second phase, we handle and incorporate outlier spaces as singleton subgroups. The distinction lies in the fact that the second phase is solely intended for grouping spaces with similar score magnitudes, while the first phase is employed to identify rank-correlated spaces. Further details on the decision to include or exclude outlier spaces in Phase 1 can be found in Appendix A.6.5.\n"
    },
    {
        "level": "##",
        "title": "A.5.3 Link Analysis",
        "content": "\nGiven a graph or network, link analysis is a valuable technique for assessing relationships between nodes and assigning importance to each node. Two prominent algorithms commonly employed for link analysis are: Hyperlink-Induced Topic Search (HITS) algorithm The HITS algorithm is based on an intuition that a good *authority* node is linked to by numerous quality *hub* nodes, and a good hub node links to numerous trusted authorities. For each node vi, HITS computes an *auth*(vi) value based on incoming links and a *hub*(vi) value based on outgoing links. This mutually reinforcing relationship is mathematically expressed through the following operations:\n\n$$auth(v_{i})=\\sum_{j:e_{ji}\\in E}hub(v_{j}),\\quad hub(v_{i})=\\sum_{j:e_{ij}\\in E}auth(v_{j})\\tag{46}$$\nThe final authority and hub scores for each node are obtained through an iterative updating process. Additional details can be found in(Ding *et al.*, 2002; Langville & Meyer, 2005; Kleinberg,\n1999).\n\nPageRank (PR) algorithm The PageRank (PR) algorithm shares a similar idea with HITS\nthat a good node should be connected to or pointed to by other good nodes. PR adopts a web surfing model based on a Markov process, introducing a different approach for determine the scores compared to the mutual reinforcement concept in HITS.\n\nLet P = (Pij) be a stochastic matrix, obtained by rescaling the adjacency matrix such that each row sums to one. Here, Pij represents the probability of transitioning from node vi to vj.\n\nIncorporating the idea of link-interrupting jumps, the matrix P is adjusted by adding a matrix \u039b\nconsisting of all ones, resulting in \u03b1P + (1 \u2212 \u03b1)\u039b, where 0 *< \u03b1 <* 1. Then the authority score in k \u03b6k = 1 through the equation:\nPR, indicating each node's importance, is determined by the equilibrium distribution \u03b6, satisfying\n\ufffd\n\n$$P^{T}\\zeta=\\zeta\\tag{47}$$\nThis solution can be obtained iteratively. Further details are available in (Ding *et al.*, 2002;\nLangville & Meyer, 2005; Page *et al.*, 1998).\n"
    },
    {
        "level": "##",
        "title": "A.6 Additional Experimental Details",
        "content": "\nThe data information for the datasets COIL20 (Nene *et al.*, 1996), COIL100 (Nene *et al.*, 1996), CMU-PIE (Sim *et al.*, 2002), YTF (Wolf *et al.*, 2011), USPS 4, MNIST-test (LeCun *et al.*, 1998), UMist (Graham & Allinson, 1998), FRGC 5 is provided in Table 3.\n"
    },
    {
        "level": "##",
        "title": "A.6.1 Data Information",
        "content": "\n| Dataset    |   #Samples |   Image Size |\n|------------|------------|--------------|\n| COIL20     |       1440 |          128 |\n| \u00d7          |            |              |\n| 128        |         20 |              |\n| COIL100    |       7200 |          128 |\n| \u00d7          |            |              |\n| 128        |        100 |              |\n| CMU-PIE    |       2856 |           32 |\n| \u00d7          |            |              |\n| 32         |         68 |              |\n| YTF        |          1 |           55 |\n| \u00d7          |            |              |\n| 55         |         41 |              |\n| USPS       |      11000 |           16 |\n| \u00d7          |            |              |\n| 16         |         10 |              |\n| MNIST-test |          1 |           28 |\n| \u00d7          |            |              |\n| 28         |         10 |              |\n| UMist      |        575 |          112 |\n| \u00d7          |            |              |\n| 92         |         20 |              |\n| FRGC       |       2462 |           32 |\n| \u00d7          |            |              |\n| 32         |         20 |              |\n"
    },
    {
        "level": "##",
        "title": "A.6.2 Evaluation Metrics",
        "content": "\nSpearman's rank correlation coefficient (Spearman, 1961; Zwillinger & Kokoska,\n1999; Kiefer, 1964)\nSpearman's rank correlation coefficient, denoted as rs, is a nonparametric measure of rank correlation that assesses the strength and direction of monotonic relationships between two variables. It is calculated by considering the Pearson correlation, denoted as rp, between the ranks of the variables and has a range between \u22121 and 1.\n\nGiven n raw scores of two variables X and Y , the scores are initially converted into their respective ranks, denoted as R(X) and R(Y ). With these ranks, rs is then computed as:\n\n$$r_{s}=r_{\\mathbb{P}}(\\mathrm{R}(X),\\mathrm{R}(Y))=\\frac{\\mathrm{cov}(\\mathrm{R}(X),\\mathrm{R}(Y))}{\\sigma_{\\mathrm{R}(X)}\\sigma_{\\mathrm{R}(Y)}},\\tag{48}$$\nwhere cov(R(X), R(Y )) is the covariance of the rank variables. \u03c3R(X) and \u03c3R(Y ) are the standard deviations of the rank variables.\n\nThe test for Spearman's rho tests the following null hypothesis (H0): rs = 0, which corresponds to no monotonic relationship between the two variables in the population. The alternative hypothesis (H1) can be two-sided: rs \u0338= 0, right-sided: rs > 0, and left-sided: rs < 0. The test statistic is given by:\n\n$$t=r_{s}\\sqrt{\\frac{N-2}{1-r_{s}^{2}}}\\tag{49}$$\nwhich follows an approximate distribution as Student's t-distribution tn\u22122 under the null hypothesis.\n\nKendall rank correlation coefficient (Kendall, 1938; Agresti, 2010; Knight, 1966)\nThe Kendall rank correlation coefficient (\u03c4) serves as a statistical metric quantifying the ordinal association between two measured quantities. As a measure of rank correlation, it ranges from \u22121\n(indicating perfect inversion) to 1 (representing perfect agreement), with a value of zero signifying an absence of association. A higher \u03c4 between two variables suggests that observations share similar ranks across both variables, while a lower correlation indicates dissimilar ranks between the observations in the two variables.\n\nConsider the set of observations (x1, y1), \u00b7 \u00b7 \u00b7 , (xn, yn) for the joint random variables X and Y . For any pair of observations (xi, yi) and (xj, yj), where *i < j*, they are deemed concordant if the sort order of (xi, xj) and (yi, yj) aligns. In other words, if either both xi *> x*j and yi > yj or both xi *< x*j and yi *< y*j holds, the observations are concordant. When either xi = xj or yi = yj, (xi, yi) and (xj, yj) form a tied pair; when a pair is neither concordant nor tied, they are discordant.\n\nThe Kendall coefficient \u03c4Bis defined as:\n\n$$\\tau_{B}=\\frac{n_{c}-n_{d}}{\\sqrt{(n_{0}-n_{1})(n_{0}-n_{2})}}\\tag{50}$$\n\n$\\sum_{i}t_{i}(t_{i}-1)/2$, $n_{2}=\\sum_{j}u_{j}(u_{j}-1)/2$. $n_{c}$ represents the count \nof concordant pairs, while nd indicates the count of discordant pairs. Moreover, ti denotes the\n\nwhere $n_{0}=n(n-1)/2$, $n_{1}=\\sum_{i}t_{i}(t_{i}-1)/2$, $n_{2}=\\sum_{i}t_{i}$.\n\nnumber of tied values in the i-th group of ties for the first quantity (e.g., X for the pair {*X, Y* }), and uj signifies the number of tied values in the j-th group of ties for the second quantity (e.g., Y for the pair {*X, Y* }). The count of discordant pairs is equivalent to the inversion number, representing the count of rearrangements needed to permute the Y -sequence with the order of the X-sequence.\n"
    },
    {
        "level": "##",
        "title": "A.6.3 Additional Implementation Details",
        "content": "\nIn this section, we provide additional details regarding our experiments. The deep clustering models, JULE, *DEPICT*, and *DeepCluster*, are executed using the source code from the respective original papers. For computing clustering validity indices such as Silhouette score (Rousseeuw,\n1987), Calinski-Harabasz index, and Davies-Bouldin index (Davies & Bouldin, 1979), we utilize functions from the *sklearn* (Pedregosa *et al.*, 2011) library in *Python*. The computation of Cubic clustering criterion (CCC) (Sarle, 1983), Dunn index (Dunn, 1974), Cindex (Hubert & Levin, 1976), SDbw index (Halkidi & Vazirgiannis, 2001) involves using R and follows the implementation detailed in (Malika *et al.*, 2014). For CDbw index (Halkidi & Vazirgiannis, 2008), we calculate scores using the function provided by the R package *fpc* (Hennig, 2023). The Dip test is implemented in R using the function from the R package *clusterability* (Neville *et al.*,\n2020). By default, the package conducts a PCA dimension reduction on the tested data before performing the test. The link analysis algorithms are implemented using the *Python* library networkx (Hagberg *et al.*, 2008). All other statistical tests are implemented using the Python library *statsmodels* (Seabold & Perktold, 2010). The *HDBSCAN* clustering is implemented using the *Python* library *hdbscan* (McInnes *et al.*, 2017), and *DBSCAN* is implemented using *sklearn*.\n\nThe entire implementation of *ACE* is carried out in *Python*.\n\nHyperparameter tuning For the *JULE* algorithm, we construct the search space by selecting the learning rate from the list [0.0005, 0.001, 0.005, 0.01, 0.05, 0.1] and the unfolding rate (\u03b7) from the list [0.2, 0.3, 0.4, 0.5, 0.7, 0.8, 0.9], resulting in 42 hyperparameter combinations. For the *DEPICT* algorithm, we define the search space by choosing the learning rate from the list [0.0005, 0.001, 0.005, 0.01, 0.05, 0.1] and the balancing parameter of the reconstruction loss function from the list [0.1, 1.0, 10.0], yielding 18 hyperparameter combinations. For each combination, we execute the two algorithms, and if a training trial fails, we consider the clustering results as missing and exclude that specific combination from the final evaluation.\n\nDetermination of the number of clusters In this experimental setup, when running JULE and *DEPICT* across datasets, we search for K among ten different values that are evenly distributed, covering the true K.\n\nTo create the search space for K, we specify the following intervals: for the datasets FRGC, MNIST-test, USPS, UMist, YTF, and COIL-20, we use linspace(5, 50*, num* = 10); for CMU-PIE, we choose *linspace*(10, 100*, num* = 10); and for COIL-100, we apply *linspace*(20, 200*, num* = 10). Here, linspace(*start, end, num*) denotes the generation of evenly spaced numbers over the specified interval [*start, end*] with a total of num values. For each K, we run the clustering algorithm, and in the event of a training trial failure, we consider the clustering results as missing, excluding that specific K from the final evaluation.\n\nSelection of checkpoints In consideration of training time and computational resources, we choose to download the validation set from ImageNet (Deng *et al.*, 2009) rather than the training set, which consists of approximately 50, 000 images uniformly distributed across 1, 000 classes.\n\nTo expedite training, we initialize the network by loading pre-trained weights from *DeepCluster*, which were obtained through training on 1.3 million images from the ImageNet training set. We adhere to the training settings specified in the source code, making adjustments only to the maximum number of clusters, set to 1000. The deep clustering process runs for 100 epochs, with checkpoints saved every five epochs, resulting in a total of 20 checkpoints. At each checkpoint, we input the data to generate 256-dimensional features used for clustering and the corresponding estimated cluster assignments for evaluation.\n"
    },
    {
        "level": "##",
        "title": "A.6.4 Additional Results",
        "content": "\nHyperparameter tuning - NMI\nIn this section, we delve into additional results for the hyperparameter tuning task, with a specific focus on the rank correlation between measure scores and Normalized Mutual Information (NMI). The evaluated validity indices, including Cubic Clustering Criterion (CCC), Dunn index, Cindex, SDbw index, and CDbw index, are presented in Table 4. Both *ACE* and *pooled scores* demonstrate superior rank correlation with NMI in comparison to *paired scores*, showcasing their effectiveness over *raw scores*. It's important to highlight the practical challenges of obtaining *raw scores* due to the high dimensional input data, as indicated by the dash mark. Furthermore, in certain cases, all four scores exhibit negative rank correlation with NMI, indicating the absence of admissible spaces for this metric in the dataset.\n\nAdditionally, for *JULE*, the density-based validity index CDbw shows a noteworthy negative correlation of NMI with paired scores, *pooled scores*, and *ACE* scores across several datasets. However, it achieves high correlation on datasets UMist, COIL-20, and COIL-100, which displays non-convex shaped clusters in the output embedding spaces (suggested by Figures 4 to 6)). This observation suggests that density-based validity indices can offer more accurate evaluations for non-convex shape clustering results.\n\nUSPS\nYTF\nFRGC\nMNIST-test\nCMU-PIE\nUMist\nCOIL-20\nCOIL-100\nAverage\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nJULE: Cubic clustering criterion\nRaw score\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nPaired score\n0.17\n0.13\n0.62\n0.49\n0.61\n0.45\n0.46\n0.33\n0.82\n0.66\n0.71\n0.51\n0.74\n0.57\n0.68\n0.51\n0.60\n0.46\nPooled score\n0.84\n0.68\n0.92\n0.80\n0.30\n0.22\n0.82\n0.67\n0.94\n0.82\n0.80\n0.59\n0.61\n0.46\n0.89\n0.73\n0.77\n0.62\nACE\n0.87\n0.72\n0.93\n0.83\n0.23\n0.15\n0.82\n0.65\n0.98\n0.91\n0.84\n0.64\n0.93\n0.78\n0.93\n0.80\n0.82\n0.69\nJULE: Dunn index\nRaw score\n0.12\n0.10\n0.56\n0.43\n-0.07\n-0.04\n-0.17\n-0.13\n-0.37\n-0.21\n-\n-\n0.54\n0.44\n0.71\n0.52\n0.19\n0.16\nPaired score\n-0.23\n-0.16\n0.59\n0.42\n0.42\n0.29\n-0.23\n-0.14\n0.90\n0.74\n0.37\n0.28\n0.58\n0.43\n0.37\n0.25\n0.35\n0.26\nPooled score\n-0.12\n-0.04\n0.71\n0.53\n0.50\n0.34\n-0.42\n-0.30\n0.89\n0.76\n0.63\n0.48\n0.73\n0.54\n0.85\n0.67\n0.47\n0.37\nACE\n-0.57\n-0.39\n0.63\n0.47\n0.27\n0.19\n-0.13\n-0.09\n0.93\n0.82\n0.61\n0.47\n0.74\n0.54\n0.80\n0.59\n0.41\n0.33\nJULE: Cindex\nRaw score\n0.49\n0.37\n0.27\n0.20\n-0.46\n-0.31\n0.17\n0.14\n-0.81\n-0.68\n-\n-\n0.50\n0.36\n0.80\n0.62\n0.14\n0.10\nPaired score\n0.27\n0.19\n0.09\n0.06\n-0.28\n-0.19\n0.47\n0.33\n-0.49\n-0.35\n0.53\n0.37\n0.06\n0.04\n-0.17\n-0.09\n0.06\n0.05\nPooled score\n0.65\n0.45\n0.67\n0.52\n0.02\n0.02\n0.73\n0.57\n-0.11\n-0.08\n0.58\n0.42\n0.51\n0.37\n0.76\n0.57\n0.48\n0.36\nACE\n0.78\n0.62\n0.20\n0.13\n-0.16\n-0.11\n0.83\n0.67\n-0.55\n-0.35\n0.58\n0.42\n0.77\n0.58\n0.69\n0.52\n0.39\n0.31\nJULE: SDbw index\nRaw score\n-0.44\n-0.26\n-\n-\n-0.18\n-0.11\n-0.76\n-0.58\n-0.99\n-0.92\n-\n-\n-0.17\n-0.07\n-\n-\n-0.51\n-0.39\nPaired score\n-0.16\n-0.08\n-0.54\n-0.38\n-0.12\n-0.08\n-0.44\n-0.30\n-0.25\n-0.16\n0.69\n0.48\n0.52\n0.37\n0.24\n0.22\n-0.01\n0.01\nPooled score\n-0.38\n-0.24\n-0.62\n-0.45\n0.18\n0.12\n-0.56\n-0.40\n-0.76\n-0.65\n0.24\n0.17\n0.10\n0.13\n0.61\n0.41\n-0.15\n-0.11\nACE\n-0.35\n-0.18\n-0.64\n-0.45\n0.47\n0.36\n-0.18\n-0.11\n-0.52\n-0.52\n0.64\n0.46\n0.61\n0.45\n0.74\n0.54\n0.10\n0.07\nJULE: CDbw index\nRaw score\n-0.26\n-0.21\n-\n-\n-\n-\n-\n-\n-0.27\n-0.22\n-\n-\n-\n-\n-\n-\n-0.26\n-0.21\nPaired score\n-0.24\n-0.16\n-0.23\n-0.17\n-0.38\n-0.27\n-0.60\n-0.43\n-0.07\n-0.05\n0.07\n0.06\n0.33\n0.21\n0.50\n0.35\n-0.08\n-0.06\nPooled score\n-0.38\n-0.25\n-0.55\n-0.40\n0.26\n0.17\n-0.73\n-0.54\n0.71\n0.63\n0.73\n0.52\n0.85\n0.68\n0.90\n0.72\n0.22\n0.19\nACE\n-0.31\n-0.20\n-0.58\n-0.41\n0.31\n0.21\n-0.70\n-0.52\n0.62\n0.52\n0.75\n0.55\n0.80\n0.61\n0.97\n0.85\n0.23\n0.20\nDEPICT: Cubic clustering criterion\nRaw score\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nPaired score\n0.74\n0.52\n0.50\n0.35\n0.95\n0.83\n0.89\n0.71\n0.89\n0.70\n0.79\n0.62\nPooled score\n0.96\n0.83\n0.61\n0.48\n0.92\n0.82\n0.98\n0.90\n0.95\n0.84\n0.88\n0.77\nACE\n0.96\n0.84\n0.76\n0.62\n0.95\n0.83\n0.96\n0.87\n0.95\n0.84\n0.91\n0.80\nDEPICT: Dunn index\nRaw score\n0.56\n0.41\n0.42\n0.26\n0.59\n0.47\n0.88\n0.73\n0.15\n0.05\n0.52\n0.39\nPaired score\n0.85\n0.66\n0.55\n0.41\n0.81\n0.62\n0.91\n0.78\n0.39\n0.29\n0.70\n0.55\nPooled score\n0.85\n0.67\n0.75\n0.59\n0.82\n0.62\n0.91\n0.74\n0.81\n0.65\n0.83\n0.66\nACE\n0.92\n0.78\n0.68\n0.53\n0.65\n0.50\n0.84\n0.67\n0.94\n0.80\n0.80\n0.66\nDEPICT: Cindex\nRaw score\n-0.27\n-0.19\n-0.35\n-0.27\n0.52\n0.41\n0.09\n0.06\n-0.23\n-0.28\n-0.05\n-0.05\nPaired score\n0.53\n0.36\n-0.03\n-0.02\n0.24\n0.19\n0.44\n0.35\n-0.18\n0.01\n0.20\n0.18\nPooled score\n0.70\n0.54\n0.53\n0.40\n0.88\n0.74\n0.92\n0.80\n0.53\n0.37\n0.71\n0.57\nACE\n0.90\n0.75\n0.61\n0.45\n0.91\n0.77\n-0.39\n-0.27\n0.73\n0.57\n0.55\n0.45\nDEPICT: SDbw index\n47\nRaw score\n0.18\n0.09\n-\n-\n0.57\n0.43\n0.14\n0.09\n-0.94\n-0.80\n-0.01\n-0.05\nPaired score\n0.84\n0.67\n0.55\n0.36\n0.91\n0.77\n0.89\n0.74\n0.57\n0.46\n0.75\n0.60\nPooled score\n0.93\n0.79\n0.62\n0.48\n0.75\n0.61\n0.96\n0.87\n0.67\n0.49\n0.79\n0.65\nACE\n0.93\n0.79\n0.64\n0.48\n0.89\n0.75\n0.97\n0.90\n0.95\n0.84\n0.87\n0.75\nDEPICT: CDbw index\nRaw score\n-\n-\n-\n-\n-\n-\n-\n-\n0.18\n0.14\n0.18\n0.14\nPaired score\n0.48\n0.35\n0.61\n0.40\n0.83\n0.66\n0.63\n0.46\n0.88\n0.70\n0.69\n0.51\nPooled score\n0.95\n0.86\n0.64\n0.48\n0.78\n0.63\n0.50\n0.32\n0.92\n0.79\n0.76\n0.62\nACE\n0.69\n0.53\n0.64\n0.48\n0.81\n0.66\n0.50\n0.32\n0.94\n0.83\n0.72\n0.56\n\nHyperparameter tuning - ACC\nIn this section, we present the rank correlation between different scores and clustering accuracy (ACC) across all validity indices, detailed in Table 5 and Table 6. The findings are consistent with our observations in Tables 1 and 4, which assess performance using NMI, thereby reinforcing our conclusions regarding the evaluation of deep clustering using these four scores.\n\nUSPS\nYTF\nFRGC\nMNIST-test\nCMU-PIE\nUMist\nCOIL-20\nCOIL-100\nAverage\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nJULE: Davies-Bouldin index\nRaw score\n-0.67\n-0.43\n-0.45\n-0.30\n-0.04\n-0.01\n-0.94\n-0.80\n-0.96\n-0.86\n-0.77\n-0.60\n-0.56\n-0.38\n-0.83\n-0.64\n-0.65\n-0.50\nPaired score\n-0.27\n-0.15\n-0.14\n-0.09\n-0.23\n-0.14\n-0.35\n-0.19\n0.20\n0.16\n0.53\n0.36\n0.63\n0.44\n0.33\n0.26\n0.09\n0.08\nPooled score\n-0.49\n-0.20\n-0.35\n-0.23\n0.48\n0.36\n-0.35\n-0.21\n0.89\n0.75\n0.17\n0.11\n-0.29\n-0.22\n-0.48\n-0.34\n-0.05\n0.00\nACE\n-0.30\n-0.09\n-0.07\n-0.07\n0.53\n0.38\n0.79\n0.64\n0.07\n0.03\n0.27\n0.20\n0.21\n0.18\n0.44\n0.28\n0.24\n0.19\nJULE: Calinski-Harabasz index\nRaw score\n0.70\n0.59\n0.54\n0.39\n-0.52\n-0.35\n0.91\n0.76\n-0.98\n-0.91\n-0.50\n-0.35\n-0.29\n-0.17\n0.36\n0.23\n0.03\n0.02\nPaired score\n0.04\n0.05\n0.39\n0.27\n-0.26\n-0.18\n0.31\n0.21\n-0.20\n-0.12\n0.64\n0.45\n0.57\n0.40\n0.09\n0.08\n0.20\n0.14\nPooled score\n0.91\n0.78\n0.78\n0.61\n0.30\n0.21\n0.91\n0.77\n0.95\n0.83\n0.81\n0.60\n0.58\n0.43\n0.90\n0.75\n0.77\n0.62\nACE\n0.90\n0.77\n0.73\n0.54\n0.49\n0.36\n0.95\n0.82\n0.97\n0.87\n0.81\n0.61\n0.57\n0.40\n0.93\n0.81\n0.79\n0.65\nJULE: Silhouette score (cosine distance)\nRaw score\n0.77\n0.59\n0.64\n0.47\n0.31\n0.21\n0.79\n0.61\n0.69\n0.54\n-0.37\n-0.27\n-0.16\n-0.13\n0.06\n0.02\n0.34\n0.26\nPaired score\n0.17\n0.14\n0.59\n0.41\n0.07\n0.06\n0.47\n0.33\n0.45\n0.33\n0.64\n0.46\n0.70\n0.51\n0.64\n0.45\n0.47\n0.34\nPooled score\n0.74\n0.68\n0.73\n0.55\n0.71\n0.53\n0.90\n0.73\n0.96\n0.88\n0.75\n0.55\n0.20\n0.11\n0.61\n0.44\n0.70\n0.56\nACE\n0.96\n0.85\n0.74\n0.55\n0.82\n0.65\n0.92\n0.78\n0.98\n0.92\n0.78\n0.58\n0.41\n0.32\n0.84\n0.68\n0.81\n0.67\nJULE: Silhouette score (euclidean distance)\nRaw score\n0.92\n0.77\n0.59\n0.43\n0.27\n0.19\n0.83\n0.66\n0.35\n0.32\n-0.35\n-0.24\n-0.14\n-0.05\n0.14\n0.08\n0.33\n0.27\nPaired score\n0.14\n0.12\n0.54\n0.39\n-0.08\n-0.02\n0.41\n0.27\n0.36\n0.27\n0.64\n0.46\n0.67\n0.48\n0.44\n0.31\n0.39\n0.28\nPooled score\n0.73\n0.67\n0.66\n0.49\n0.70\n0.53\n0.89\n0.72\n0.97\n0.88\n0.77\n0.57\n0.20\n0.11\n0.62\n0.45\n0.69\n0.55\nACE\n0.93\n0.78\n0.63\n0.48\n0.71\n0.53\n0.92\n0.78\n0.98\n0.91\n0.86\n0.68\n0.39\n0.30\n0.84\n0.68\n0.78\n0.64\nJULE: Cubic clustering criterion\nRaw score\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nPaired score\n0.04\n0.05\n0.43\n0.30\n0.52\n0.35\n0.30\n0.20\n0.84\n0.67\n0.65\n0.48\n0.76\n0.58\n0.67\n0.49\n0.53\n0.39\nPooled score\n0.91\n0.78\n0.76\n0.59\n0.33\n0.23\n0.91\n0.77\n0.95\n0.84\n0.80\n0.59\n0.57\n0.42\n0.90\n0.76\n0.77\n0.62\nACE\n0.94\n0.82\n0.76\n0.59\n0.21\n0.14\n0.88\n0.72\n0.99\n0.93\n0.84\n0.65\n0.91\n0.74\n0.93\n0.79\n0.81\n0.67\nJULE: Dunn index\nRaw score\n0.02\n0.02\n0.21\n0.17\n-0.18\n-0.13\n-0.09\n-0.08\n-0.33\n-0.20\n-\n-\n0.50\n0.40\n0.62\n0.44\n0.11\n0.09\nPaired score\n-0.36\n-0.24\n0.29\n0.19\n0.55\n0.39\n-0.20\n-0.14\n0.89\n0.73\n0.31\n0.24\n0.56\n0.42\n0.40\n0.28\n0.31\n0.23\nPooled score\n-0.35\n-0.16\n0.35\n0.23\n0.62\n0.44\n-0.52\n-0.37\n0.87\n0.72\n0.59\n0.45\n0.72\n0.53\n0.73\n0.54\n0.38\n0.30\nACE\n-0.77\n-0.56\n0.38\n0.25\n0.46\n0.33\n-0.10\n-0.09\n0.92\n0.79\n0.58\n0.44\n0.73\n0.54\n0.67\n0.49\n0.36\n0.27\nJULE: Cindex\nRaw score\n0.56\n0.45\n0.35\n0.24\n-0.52\n-0.37\n0.24\n0.23\n-0.80\n-0.67\n-\n-\n0.56\n0.40\n0.78\n0.61\n0.17\n0.13\nPaired score\n0.13\n0.10\n-0.09\n-0.06\n-0.47\n-0.32\n0.33\n0.21\n-0.56\n-0.40\n0.54\n0.38\n0.09\n0.06\n-0.21\n-0.14\n-0.03\n-0.02\nPooled score\n0.82\n0.63\n0.62\n0.45\n-0.24\n-0.16\n0.87\n0.67\n-0.06\n-0.05\n0.67\n0.51\n0.60\n0.45\n0.77\n0.59\n0.51\n0.38\nACE\n0.85\n0.70\n0.17\n0.13\n-0.39\n-0.28\n0.93\n0.76\n-0.52\n-0.32\n0.67\n0.50\n0.81\n0.63\n0.68\n0.52\n0.40\n0.33\nJULE: SDbw index\nRaw score\n-0.53\n-0.33\n-\n-\n0.04\n0.05\n-0.89\n-0.72\n-1.00\n-0.97\n-\n-\n-0.14\n-0.07\n-\n-\n-0.50\n-0.41\nPaired score\n-0.32\n-0.20\n-0.30\n-0.19\n-0.35\n-0.24\n-0.61\n-0.42\n-0.31\n-0.20\n0.61\n0.42\n0.56\n0.39\n0.14\n0.10\n-0.07\n-0.04\nPooled score\n-0.58\n-0.31\n-0.39\n-0.26\n0.51\n0.39\n-0.70\n-0.54\n-0.75\n-0.64\n0.11\n0.08\n0.07\n0.10\n0.67\n0.46\n-0.13\n-0.09\nACE\n-0.51\n-0.22\n-0.39\n-0.26\n0.69\n0.48\n-0.25\n-0.17\n-0.50\n-0.50\n0.54\n0.38\n0.57\n0.40\n0.80\n0.59\n0.12\n0.09\nJULE: CDbw index\nRaw score\n-0.27\n-0.22\n-\n-\n-\n-\n-\n-\n-0.27\n-0.22\n-\n-\n-\n-\n-\n-\n-0.27\n-0.22\n49\nPaired score\n-0.41\n-0.28\n-0.43\n-0.30\n-0.48\n-0.31\n-0.73\n-0.54\n-0.12\n-0.07\n0.10\n0.08\n0.35\n0.22\n0.41\n0.28\n-0.16\n-0.12\nPooled score\n-0.62\n-0.40\n-0.29\n-0.18\n0.50\n0.39\n-0.88\n-0.67\n0.73\n0.66\n0.62\n0.45\n0.83\n0.64\n0.89\n0.72\n0.22\n0.20\nACE\n-0.55\n-0.34\n-0.36\n-0.26\n0.59\n0.44\n-0.85\n-0.65\n0.58\n0.51\n0.67\n0.50\n0.75\n0.55\n0.90\n0.75\n0.22\n0.19\nUSPS\nYTF\nFRGC\nMNIST-test\nCMU-PIE\nUMist\nCOIL-20\nCOIL-100\nAverage\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nDEPICT: Davies-Bouldin index\nRaw score\n0.06\n-0.09\n0.48\n0.33\n0.53\n0.39\n0.13\n0.07\n-0.14\n-0.20\n0.21\n0.10\nPaired score\n0.61\n0.42\n0.48\n0.32\n0.92\n0.74\n0.88\n0.69\n0.62\n0.56\n0.70\n0.55\nPooled score\n0.95\n0.84\n0.40\n0.28\n0.64\n0.48\n0.38\n0.28\n-0.76\n-0.60\n0.32\n0.26\nACE\n0.99\n0.96\n0.65\n0.46\n0.90\n0.74\n0.99\n0.96\n0.96\n0.87\n0.90\n0.80\nDEPICT: Calinski-Harabasz index\nRaw score\n-0.10\n-0.19\n0.65\n0.50\n0.54\n0.38\n0.59\n0.47\n-0.95\n-0.83\n0.14\n0.07\nPaired score\n0.56\n0.40\n0.54\n0.35\n0.76\n0.57\n0.88\n0.69\n0.48\n0.43\n0.64\n0.49\nPooled score\n0.94\n0.82\n0.54\n0.45\n0.92\n0.79\n0.95\n0.86\n0.62\n0.55\n0.79\n0.69\nACE\n0.82\n0.72\n0.61\n0.45\n0.91\n0.82\n0.97\n0.91\n0.96\n0.87\n0.86\n0.75\nDEPICT: Silhouette score (cosine distance)\nRaw score\n0.43\n0.33\n0.69\n0.52\n0.77\n0.62\n0.83\n0.64\n0.43\n0.26\n0.63\n0.47\nPaired score\n0.62\n0.45\n0.53\n0.42\n0.91\n0.75\n0.88\n0.69\n0.77\n0.58\n0.74\n0.58\nPooled score\n0.96\n0.87\n0.75\n0.59\n0.94\n0.82\n0.96\n0.88\n0.93\n0.76\n0.91\n0.78\nACE\n0.95\n0.88\n0.70\n0.54\n0.91\n0.77\n0.96\n0.88\n0.94\n0.83\n0.89\n0.78\nDEPICT: Silhouette score (euclidean distance)\nRaw score\n0.45\n0.27\n0.75\n0.59\n0.69\n0.51\n0.79\n0.63\n-0.23\n-0.13\n0.49\n0.37\nPaired score\n0.52\n0.33\n0.57\n0.45\n0.80\n0.62\n0.85\n0.65\n0.59\n0.48\n0.67\n0.51\nPooled score\n0.94\n0.84\n0.72\n0.57\n0.94\n0.82\n0.96\n0.88\n0.92\n0.75\n0.90\n0.77\nACE\n0.95\n0.87\n0.63\n0.49\n0.91\n0.78\n0.97\n0.91\n0.95\n0.84\n0.88\n0.78\nDEPICT: Cubic clustering criterion\nRaw score\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nPaired score\n0.52\n0.35\n0.59\n0.43\n0.92\n0.79\n0.88\n0.67\n0.87\n0.68\n0.76\n0.59\nPooled score\n0.96\n0.87\n0.54\n0.44\n0.94\n0.82\n0.96\n0.88\n0.96\n0.85\n0.87\n0.77\nACE\n0.96\n0.88\n0.65\n0.53\n0.93\n0.83\n0.97\n0.91\n0.96\n0.85\n0.89\n0.80\nDEPICT: Dunn index\nRaw score\n0.56\n0.39\n0.42\n0.27\n0.68\n0.50\n0.80\n0.64\n0.10\n0.03\n0.51\n0.37\nPaired score\n0.70\n0.52\n0.48\n0.35\n0.85\n0.67\n0.86\n0.71\n0.36\n0.28\n0.65\n0.50\nPooled score\n0.70\n0.53\n0.66\n0.50\n0.84\n0.67\n0.88\n0.70\n0.80\n0.63\n0.78\n0.61\nACE\n0.79\n0.63\n0.62\n0.49\n0.67\n0.53\n0.77\n0.61\n0.93\n0.79\n0.76\n0.61\nDEPICT: Cindex\nRaw score\n-0.31\n-0.20\n-0.23\n-0.18\n0.45\n0.36\n0.18\n0.10\n-0.22\n-0.25\n-0.02\n-0.03\nPaired score\n0.57\n0.40\n0.13\n0.10\n0.23\n0.16\n0.49\n0.39\n-0.18\n-0.03\n0.25\n0.20\nPooled score\n0.91\n0.74\n0.61\n0.46\n0.92\n0.77\n0.95\n0.84\n0.55\n0.41\n0.79\n0.64\nACE\n0.92\n0.82\n0.68\n0.52\n0.88\n0.71\n-0.35\n-0.20\n0.76\n0.63\n0.58\n0.49\nDEPICT: SDbw index\nRaw score\n0.27\n0.12\n-\n-\n0.72\n0.59\n0.21\n0.10\n-0.93\n-0.80\n0.07\n0.00\nPaired score\n0.66\n0.48\n0.51\n0.35\n0.90\n0.74\n0.88\n0.70\n0.59\n0.48\n0.71\n0.55\n50\nPooled score\n0.98\n0.91\n0.51\n0.39\n0.74\n0.61\n0.97\n0.88\n0.67\n0.50\n0.77\n0.66\nACE\n0.98\n0.91\n0.52\n0.39\n0.87\n0.72\n0.97\n0.91\n0.95\n0.85\n0.86\n0.76\nDEPICT: CDbw index\nRaw score\n-\n-\n-\n-\n-\n-\n-\n-\n0.21\n0.17\n0.21\n0.17\nPaired score\n0.53\n0.39\n0.55\n0.36\n0.83\n0.66\n0.68\n0.50\n0.90\n0.72\n0.70\n0.53\nPooled score\n0.86\n0.74\n0.54\n0.41\n0.82\n0.66\n0.44\n0.31\n0.92\n0.76\n0.71\n0.58\nACE\n0.79\n0.62\n0.54\n0.39\n0.86\n0.69\n0.44\n0.31\n0.94\n0.80\n0.71\n0.56\n\nHyperparameter tuning - Qualitative Analysis In this section, we present the qualitative analysis results for the hyperparameter tuning task using both *JULE* and *DEPICT*. Graphs depicting the rank correlation between the retained spaces after the multimodality test, based on different validity indices, are provided in Figures 3 (Davies-Bouldin index), 5 (Calinski-Harabasz index), 7 (*DEPICT*: Silhouette score (cosine distance)) and 9 (*DEPICT*: Silhouette score (euclidean distance)) for the hyperparameter tuning task performed with *JULE* for deep clustering. Similarly, Figures 11 (Davies-Bouldin index), 13 (Calinski-Harabasz index), 15 (Silhouette score\n(cosine distance)), and 17 (Silhouette score (euclidean distance)) present these graphs for the hyperparameter tuning task with *DEPICT*. In each graph, spaces grouped together by a densitybased clustering approach share the same color, while outlier spaces are uniformly colored in grey.\n\nFrom these figures, discerning grouping behaviors within the retained spaces post the multimodality test becomes evident. In the case of *JULE*, where approximately 40 models (or spaces) are generated in our experiment, multiple groups are often detected. However, a few instances, such as those depicted in Figure 3 (a), (e), Figure 5 (b), Figure 7 (b), and Figure 9 (b), reveal scenarios where only a single group is identified. In contrast, for *DEPICT*, which generates around 18 spaces in the experiment, there is a tendency to observe more cases with only one group. Across these figures, examining the same set of retained spaces (derived from the same dataset with the same task) highlights that the grouping behavior can vary depending on the chosen validity measures. As a reminder from Appendix A.4, the silhouette score emphasizes individual data points and their relationships to their own and other clusters, the Davies-Bouldin index considers the overall compactness and separation of clusters, and the Calinski-Harabasz index measures the ratio of between-cluster variance to within-cluster variance. The distinctions in how these measures define the quality of clustering elucidate the variations in their observed clustering behavior. It's noteworthy that, when considering the silhouette score, a comparison is made using two distance metrics for its calculation: cosine distance and euclidean distance. Interestingly, we find that they exhibit more similar clustering behavior across spaces than when comparing two different validity measures. This observation implies a greater impact of the chosen measure itself compared to the choice of distance metric.\n\nWe also utilize t-SNE plots (Van der Maaten & Hinton, 2008) to visualize the discriminative capability of embedding subspaces between the finally selected embedding space by *ACE* and the spaces excluded by *ACE*. The t-SNE algorithm, known for its effectiveness in preserving the local structure of data and maintaining relative distances between neighboring points in high-dimensional space, is employed to create a non-linear mapping from the embedding space to a\n2-dimensional feature space for visualization. We present this comparison for the hyperparameter tuning task with *JULE* based on different validity indices in Figures 4 (Davies-Bouldin index), 6\n(Calinski-Harabasz index), 10 (*DEPICT*: Silhouette score (euclidean distance)), and 8 (Silhouette score (cosine distance)). Similarly, in Figures 12 (Davies-Bouldin index), 14 (Calinski-Harabasz index), 18 (*DEPICT*: Silhouette score (euclidean distance)), and 16 (Silhouette score (cosine distance)), we provide the comparison between selected and excluded embedding spaces for DEPICT. In each figure, we plot and compare one selected space with an excluded space for each dataset. Different colors in each subfigure correspond to different true clusters. Due to space constraints, we have chosen one representative space from the retained spaces, resembling an admissible space, and one from the excluded spaces for a concise comparison.\n\nAcross these figures, it is evident that the selected spaces exhibit more compact and wellseparated clusters of data points aligned with their true cluster labels. In contrast, many of the excluded spaces demonstrate poor clustering behavior. For instance, in the case of *JULE*, the comparison between (o) and (p) in Figures Figure 4 to Figure 10 reveals that the selected spaces showcase clear separation between different clusters, while some excluded spaces exhibit multiple areas with intermixed clusters. Similarly, the comparison between (e) and (f) in Figures Figure 4 to Figure 10 highlights that the selected spaces present regular cluster shapes, whereas excluded spaces show irregular shapes resembling strings of different clusters. This phenomenon is consistent for *DEPICT* as well. For instance, the comparison between (g) and (h) in Figures Figure 12 to Figure 18 reveals that some excluded spaces lack clear clustering behavior, whereas the selected spaces exhibit compact and well-separated clusters. Similarly, between (i) and (j) in Figures Figure 12 to Figure 18, the selected spaces demonstrate well-separated clusters, while the excluded spaces group points from different true clusters into the same cluster.\n\nDetermination of the number of clusters - NMI\nIn this section, we explore additional results for the hyperparameter tuning task, specifically concentrating on the rank correlation between measure scores and Normalized Mutual Information (NMI). The evaluated validity indices, including Cubic Clustering Criterion (CCC), Dunn index, Cindex, SDbw index, and CDbw index, are presented in Table 7. Both *ACE* and *pooled scores* demonstrate superior rank correlation with NMI compared to *paired scores* across most cases. The prevalence of missing values for *raw score*s underscores the practical challenges associated with obtaining them due to computational resource constraints. It is important to note that in some instances, *ACE* scores and *pooled scores* may not outperform *paired scores*, particularly when all three exhibit negative rank correlation with NMI, suggesting the absence of admissible spaces for this metric in the dataset.\n\nDetermination of the number of clusters - ACC\nIn this section, we present the rank correlation between different scores and clustering accuracy (ACC) across all validity indices, detailed in Table 8 and Table 9. The findings are consistent with our observations in Tables 2 and 7, which assess performance using NMI, thereby reinforcing our conclusions regarding the evaluation of deep clustering using these four scores.\n\nUSPS (10)\nYTF (41)\nFRGC (20)\nMNIST-test (10)\nCMU-PIE (68)\nUMist (20)\nCOIL-20 (20)\nCOIL-100 (100)\nAverage\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nJULE: Cubic clustering criterion\nRaw score\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nPaired score\n0.79 (10)\n0.73 (10)\n-0.22 (50)\n-0.17 (50)\n0.65 (25)\n0.5 (25)\n0.82 (10)\n0.69 (10)\n0.96 (70)\n0.87 (70)\n0.33 (35)\n0.29 (35)\n0.43 (40)\n0.29 (40)\n0.69 (80)\n0.49 (80)\n0.56\n0.46\nPooled score\n0.87 (10)\n0.78 (10)\n0.78 (50)\n0.61 (50)\n0.67 (45)\n0.56 (45)\n0.84 (10)\n0.73 (10)\n0.99 (70)\n0.96 (70)\n0.24 (45)\n0.24 (45)\n-0.74 (45)\n-0.64 (45)\n0.64 (80)\n0.49 (80)\n0.54\n0.47\nACE\n0.87 (10)\n0.78 (10)\n0.92 (50)\n0.78 (50)\n0.53 (25)\n0.39 (25)\n0.84 (10)\n0.73 (10)\n0.99 (70)\n0.96 (70)\n0.24 (45)\n0.24 (45)\n-0.67 (40)\n-0.5 (40)\n0.64 (80)\n0.49 (80)\n0.55\n0.48\nJULE: Dunn index\nRaw score\n0.42 (10,15,35,5)\n0.4 (10,15,35,5)\n-0.9 (15)\n-0.78 (15)\n0.58 (25,50)\n0.42 (25,50)\n0.39 (15)\n0.33 (15)\n0.74 (100,70,80,90)\n0.65 (100,70,80,90)\n-\n-\n0.64 (20)\n0.57 (20)\n-\n-\n0.31\n0.26\nPaired score\n0.28 (5)\n0.29 (5)\n-0.48 (25)\n-0.33 (25)\n0.6 (50)\n0.5 (50)\n0.44 (15)\n0.42 (15)\n0.55 (50)\n0.38 (50)\n-0.2 (5)\n-0.11 (5)\n0.64 (15)\n0.5 (15)\n0.12 (60)\n0.11 (60)\n0.24\n0.22\nPooled score\n0.43 (5)\n0.51 (5)\n-0.93 (11)\n-0.83 (11)\n-0.4 (10)\n-0.33 (10)\n0.14 (5)\n0.16 (5)\n-0.27 (50)\n-0.16 (50)\n0.02 (5)\n0.07 (5)\n0.48 (15)\n0.36 (15)\n-0.39 (60)\n-0.2 (60)\n-0.11\n-0.05\nACE\n0.43 (5)\n0.51 (5)\n-0.98 (11)\n-0.94 (11)\n-0.2 (20)\n-0.06 (20)\n0.14 (5)\n0.16 (5)\n-0.27 (50)\n-0.16 (50)\n0.02 (5)\n0.07 (5)\n0.38 (15)\n0.29 (15)\n-0.44 (60)\n-0.24 (60)\n-0.12\n-0.05\nJULE: Cindex\nRaw score\n-0.41 (40)\n-0.47 (40)\n-0.02 (40)\n0.11 (40)\n0.75 (35)\n0.56 (35)\n-0.19 (30)\n-0.11 (30)\n0.77 (100)\n0.56 (100)\n-\n-\n-0.69 (50)\n-0.57 (50)\n-\n-\n0.04\n0.01\nPaired score\n-0.12 (45)\n-0.16 (45)\n-0.5 (11)\n-0.39 (11)\n0.47 (30)\n0.39 (30)\n-0.03 (30)\n0.02 (30)\n-0.56 (20)\n-0.38 (20)\n0.07 (45)\n0.16 (45)\n-0.43 (50)\n-0.36 (50)\n0.55 (140)\n0.38 (140)\n-0.07\n-0.04\nPooled score\n-0.44 (45)\n-0.56 (45)\n0.88 (50)\n0.72 (50)\n0.98 (50)\n0.94 (50)\n-0.34 (40)\n-0.38 (40)\n0.77 (100)\n0.6 (100)\n0.12 (50)\n0.02 (50)\n-0.59 (50)\n-0.5 (50)\n0.44 (200)\n0.29 (200)\n0.23\n0.14\nACE\n-0.42 (45)\n-0.51 (45)\n0.63 (25)\n0.5 (25)\n0.98 (50)\n0.94 (50)\n-0.34 (40)\n-0.38 (40)\n0.76 (100)\n0.56 (100)\n0.13 (50)\n0.07 (50)\n-0.59 (50)\n-0.5 (50)\n0.39 (200)\n0.2 (200)\n0.19\n0.11\nJULE: SDbw index\nRaw score\n-0.37 (50)\n-0.42 (50)\n-0.41 (15)\n-0.35 (15)\n0.82 (50)\n0.72 (50)\n-0.46 (45)\n-0.42 (45)\n0.6 (100)\n0.38 (100)\n-\n-\n-0.91 (45)\n-0.79 (45)\n-\n-\n-0.12\n-0.15\nPaired score\n-0.16 (45)\n-0.16 (45)\n0.75 (35)\n0.67 (35)\n0.97 (50)\n0.89 (50)\n-0.18 (50)\n-0.24 (50)\n0.65 (100)\n0.51 (100)\n0.2 (45)\n0.16 (45)\n-0.24 (40)\n-0.21 (40)\n-0.93 (20)\n-0.82 (20)\n0.13\n0.10\nPooled score\n-0.43 (45)\n-0.51 (45)\n0.98 (50)\n0.94 (50)\n0.98 (50)\n0.94 (50)\n-0.39 (45)\n-0.42 (45)\n0.77 (100)\n0.56 (100)\n0.19 (50)\n0.16 (50)\n-0.74 (50)\n-0.64 (50)\n-0.99 (20)\n-0.96 (20)\n0.05\n0.01\nACE\n-0.43 (45)\n-0.51 (45)\n0.98 (50)\n0.94 (50)\n0.98 (50)\n0.94 (50)\n-0.39 (45)\n-0.42 (45)\n0.77 (100)\n0.56 (100)\n0.19 (50)\n0.16 (50)\n-0.74 (50)\n-0.64 (50)\n-0.99 (20)\n-0.96 (20)\n0.05\n0.01\nJULE: CDbw index\nRaw score\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nPaired score\n0.01 (15)\n-0.02 (15)\n-0.25 (30)\n-0.11 (30)\n0.82 (45)\n0.61 (45)\n-0.19 (45)\n-0.16 (45)\n-0.52 (20)\n-0.33 (20)\n0.09 (45)\n0.11 (45)\n0.26 (15)\n0.14 (15)\n-0.73 (20)\n-0.6 (20)\n-0.06\n-0.04\nPooled score\n-0.38 (50)\n-0.47 (50)\n0.95 (50)\n0.89 (50)\n0.97 (50)\n0.89 (50)\n-0.37 (45)\n-0.33 (45)\n0.89 (70)\n0.78 (70)\n0.31 (45)\n0.24 (45)\n0.52 (15)\n0.43 (15)\n-0.41 (20)\n-0.24 (20)\n0.31\n0.27\nACE\n-0.37 (45)\n-0.42 (45)\n0.98 (50)\n0.94 (50)\n0.97 (50)\n0.89 (50)\n-0.37 (45)\n-0.33 (45)\n0.88 (70)\n0.73 (70)\n0.31 (45)\n0.24 (45)\n0.57 (15)\n0.5 (15)\n-0.39 (20)\n-0.2 (20)\n0.32\n0.29\nDEPICT: Cubic clustering criterion\nRaw score\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nPaired score\n-0.19 (25)\n-0.11 (25)\n0.98 (50)\n0.91 (50)\n0.53 (25)\n0.39 (25)\n0.13 (35)\n0.11 (35)\n0.98 (80)\n0.91 (80)\n0.49\n0.44\nPooled score\n-0.25 (40)\n-0.29 (40)\n1.0 (50)\n1.0 (50)\n0.83 (50)\n0.67 (50)\n0.1 (40)\n0.07 (40)\n0.92 (100)\n0.82 (100)\n0.52\n0.45\nACE\n-0.25 (40)\n-0.29 (40)\n1.0 (50)\n1.0 (50)\n0.83 (50)\n0.67 (50)\n0.06 (40)\n0.02 (40)\n0.92 (100)\n0.82 (100)\n0.51\n0.44\nDEPICT: Dunn index\nRaw score\n-0.16 (5)\n-0.11 (5)\n0.82 (50)\n0.69 (50)\n0.83 (35)\n0.67 (35)\n0.07 (10)\n0.02 (10)\n0.2 (100)\n0.24 (100)\n0.35\n0.30\nPaired score\n0.04 (25)\n0.07 (25)\n-0.22 (5)\n-0.16 (5)\n-0.57 (15)\n-0.44 (15)\n0.34 (15)\n0.29 (15)\n0.02 (50)\n-0.02 (50)\n-0.08\n-0.05\nPooled score\n-0.12 (5)\n-0.07 (5)\n-0.32 (5)\n-0.24 (5)\n0.0 (30)\n0.0 (30)\n0.24 (10)\n0.2 (10)\n0.22 (50)\n0.16 (50)\n0.00\n0.01\nACE\n-0.38 (5)\n-0.29 (5)\n0.04 (15)\n0.02 (15)\n0.0 (30)\n0.0 (30)\n0.24 (5)\n0.24 (5)\n0.22 (50)\n0.16 (50)\n0.02\n0.03\nDEPICT: Cindex\nRaw score\n-0.22 (40)\n-0.24 (40)\n0.65 (35)\n0.42 (35)\n0.72 (40)\n0.56 (40)\n-0.42 (45)\n-0.38 (45)\n0.85 (100)\n0.73 (100)\n0.32\n0.22\nPaired score\n0.46 (5)\n0.6 (5)\n-0.54 (5)\n-0.47 (5)\n-0.92 (10)\n-0.83 (10)\n0.42 (5)\n0.47 (5)\n0.12 (10)\n0.16 (10)\n-0.09\n-0.01\nPooled score\n-0.44 (50)\n-0.56 (50)\n1.0 (50)\n1.0 (50)\n0.85 (50)\n0.72 (50)\n-0.37 (50)\n-0.42 (50)\n0.92 (100)\n0.82 (100)\n0.39\n0.31\nACE\n-0.44 (50)\n-0.56 (50)\n1.0 (50)\n1.0 (50)\n0.78 (50)\n0.61 (50)\n-0.37 (50)\n-0.42 (50)\n0.92 (100)\n0.82 (100)\n0.38\n0.29\nDEPICT: SDbw index\nRaw score\n-0.41 (45)\n-0.47 (45)\n-0.51 (15)\n-0.4 (15)\n0.72 (50)\n0.5 (50)\n-0.36 (40)\n-0.38 (40)\n0.92 (100)\n0.82 (100)\n0.07\n0.01\nPaired score\n0.43 (5)\n0.51 (5)\n-0.41 (5)\n-0.29 (5)\n-0.85 (10)\n-0.72 (10)\n0.55 (10)\n0.6 (10)\n0.26 (10)\n0.29 (10)\n-0.00\n0.08\nPooled score\n-0.44 (50)\n-0.56 (50)\n1.0 (50)\n1.0 (50)\n0.85 (50)\n0.72 (50)\n-0.34 (45)\n-0.38 (45)\n0.92 (100)\n0.82 (100)\n0.40\n0.32\nACE\n-0.44 (50)\n-0.56 (50)\n1.0 (50)\n1.0 (50)\n0.85 (50)\n0.72 (50)\n-0.38 (45)\n-0.47 (45)\n0.93 (100)\n0.87 (100)\n0.39\n0.31\nDEPICT: CDbw index\nRaw score\n-\n-\n-\n-\n-\n-\n-\n-\n-0.43 (20)\n-0.38 (20)\n-0.43\n-0.38\nPaired score\n0.42 (5)\n0.51 (5)\n-0.72 (5)\n-0.6 (5)\n-0.83 (10)\n-0.67 (10)\n0.44 (5)\n0.56 (5)\n-0.81 (10)\n-0.64 (10)\n-0.30\n-0.17\nPooled score\n-0.55 (50)\n-0.47 (50)\n-0.07 (5)\n0.07 (5)\n-0.9 (10)\n-0.78 (10)\n-0.76 (5)\n-0.56 (5)\n0.85 (100)\n0.73 (100)\n-0.29\n-0.20\nACE\n-0.01 (50)\n-0.11 (50)\n0.39 (5)\n0.42 (5)\n-0.25 (10)\n-0.17 (10)\n-0.73 (5)\n-0.51 (5)\n0.9 (100)\n0.78 (100)\n0.06\n0.08\nUSPS (10)\nYTF (41)\nFRGC (20)\nMNIST-test (10)\nCMU-PIE (68)\nUMist (20)\nCOIL-20 (20)\nCOIL-100 (100)\nAverage\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nJULE: Davies-Bouldin index\nRaw score\n-0.49\n-0.38\n0.85\n0.67\n0.37\n0.20\n-0.41\n-0.38\n0.77\n0.51\n0.02\n-0.16\n-0.86\n-0.71\n-0.82\n-0.78\n-0.07\n-0.13\nPaired score\n0.39\n0.29\n0.10\n0.06\n0.37\n0.25\n0.49\n0.33\n0.83\n0.60\n-0.28\n-0.29\n-0.29\n-0.21\n-0.87\n-0.73\n0.09\n0.04\nPooled score\n0.89\n0.73\n0.80\n0.67\n0.71\n0.54\n0.83\n0.64\n0.85\n0.69\n-0.42\n-0.33\n-0.79\n-0.64\n-0.79\n-0.69\n0.26\n0.20\nACE\n0.89\n0.73\n0.80\n0.67\n0.60\n0.42\n0.83\n0.64\n0.88\n0.73\n-0.42\n-0.33\n-0.71\n-0.64\n-0.82\n-0.69\n0.26\n0.19\nJULE: Calinski-Harabasz index\nRaw score\n0.71\n0.64\n1.00\n1.00\n-0.46\n-0.25\n0.41\n0.47\n-0.38\n-0.29\n-0.09\n-0.02\n0.76\n0.71\n0.36\n0.33\n0.29\n0.32\nPaired score\n0.84\n0.73\n0.03\n-0.06\n-0.49\n-0.31\n0.61\n0.56\n-0.09\n-0.07\n-0.04\n0.07\n0.74\n0.64\n0.60\n0.51\n0.27\n0.26\nPooled score\n0.84\n0.73\n0.88\n0.78\n-0.37\n-0.20\n0.61\n0.56\n0.85\n0.69\n-0.07\n0.02\n0.76\n0.71\n0.56\n0.51\n0.51\n0.48\nACE\n0.84\n0.73\n0.92\n0.83\n-0.11\n-0.03\n0.61\n0.56\n0.83\n0.69\n-0.07\n0.02\n0.76\n0.71\n0.65\n0.56\n0.55\n0.51\nJULE: Silhouette score (cosine distance)\nRaw score\n0.58\n0.42\n0.95\n0.89\n0.52\n0.42\n-0.01\n-0.02\n-0.32\n-0.16\n0.08\n0.02\n-0.50\n-0.36\n0.53\n0.38\n0.23\n0.20\nPaired score\n0.89\n0.78\n0.27\n0.22\n0.21\n0.09\n0.81\n0.64\n0.99\n0.96\n-0.26\n-0.24\n0.55\n0.43\n0.52\n0.33\n0.50\n0.40\nPooled score\n0.95\n0.87\n0.98\n0.94\n0.61\n0.48\n0.94\n0.82\n0.99\n0.96\n-0.32\n-0.24\n0.67\n0.50\n0.54\n0.38\n0.67\n0.59\nACE\n0.95\n0.87\n0.98\n0.94\n0.64\n0.54\n0.94\n0.82\n0.99\n0.96\n-0.32\n-0.24\n0.76\n0.57\n0.60\n0.47\n0.69\n0.61\nJULE: Silhouette score (euclidean distance)\nRaw score\n0.62\n0.56\n0.95\n0.89\n-0.17\n-0.14\n0.53\n0.42\n0.53\n0.33\n0.04\n-0.07\n-0.38\n-0.29\n0.52\n0.33\n0.33\n0.25\nPaired score\n0.93\n0.82\n0.30\n0.28\n0.21\n0.09\n0.82\n0.64\n0.98\n0.91\n-0.13\n-0.16\n0.52\n0.36\n0.55\n0.42\n0.52\n0.42\nPooled score\n0.95\n0.87\n0.97\n0.89\n0.61\n0.48\n0.92\n0.78\n0.99\n0.96\n-0.03\n-0.11\n0.74\n0.50\n0.59\n0.47\n0.72\n0.60\nACE\n0.95\n0.87\n0.98\n0.94\n0.57\n0.48\n0.92\n0.78\n0.99\n0.96\n-0.03\n-0.11\n0.74\n0.50\n0.59\n0.47\n0.71\n0.61\nJULE: Cubic clustering criterion\nRaw score\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nPaired score\n0.90\n0.82\n-0.28\n-0.28\n0.26\n0.14\n0.77\n0.64\n0.93\n0.82\n0.13\n0.07\n0.45\n0.36\n0.52\n0.36\n0.46\n0.37\nPooled score\n0.94\n0.87\n0.77\n0.61\n0.51\n0.37\n0.81\n0.69\n1.00\n1.00\n0.04\n-0.07\n-0.71\n-0.57\n0.47\n0.36\n0.48\n0.41\nACE\n0.94\n0.87\n0.92\n0.78\n0.39\n0.31\n0.81\n0.69\n1.00\n1.00\n0.04\n-0.07\n-0.69\n-0.57\n0.47\n0.36\n0.48\n0.42\nJULE: Dunn index\nRaw score\n0.71\n0.60\n-0.83\n-0.67\n0.31\n0.23\n0.42\n0.38\n0.78\n0.69\n-\n-\n0.62\n0.50\n-\n-\n0.33\n0.29\nPaired score\n0.52\n0.38\n-0.40\n-0.22\n0.09\n0.09\n0.49\n0.47\n0.50\n0.33\n-0.28\n-0.24\n0.67\n0.57\n-0.03\n-0.02\n0.19\n0.17\nPooled score\n0.69\n0.60\n-0.85\n-0.72\n-0.35\n-0.20\n0.20\n0.20\n-0.37\n-0.20\n-0.07\n-0.07\n0.52\n0.43\n-0.54\n-0.33\n-0.10\n-0.04\nACE\n0.69\n0.60\n-0.93\n-0.83\n-0.31\n-0.25\n0.20\n0.20\n-0.37\n-0.20\n-0.07\n-0.07\n0.45\n0.36\n-0.59\n-0.38\n-0.12\n-0.07\nJULE: Cindex\nRaw score\n-0.71\n-0.64\n0.17\n0.22\n0.07\n0.09\n-0.21\n-0.16\n0.83\n0.60\n-\n-\n-0.71\n-0.64\n-\n-\n-0.09\n-0.09\nPaired score\n-0.36\n-0.24\n-0.58\n-0.50\n0.14\n0.20\n-0.04\n-0.02\n-0.60\n-0.42\n-0.06\n-0.07\n-0.48\n-0.43\n0.60\n0.42\n-0.17\n-0.13\nPooled score\n-0.72\n-0.64\n0.87\n0.72\n0.45\n0.31\n-0.33\n-0.33\n0.78\n0.64\n0.04\n-0.11\n-0.64\n-0.57\n0.55\n0.42\n0.12\n0.05\nACE\n-0.70\n-0.60\n0.58\n0.39\n0.45\n0.31\n-0.33\n-0.33\n0.77\n0.60\n0.06\n-0.07\n-0.64\n-0.57\n0.52\n0.33\n0.09\n0.01\nJULE: SDbw index\nRaw score\n-0.61\n-0.51\n-0.41\n-0.35\n0.18\n0.09\n-0.41\n-0.38\n0.66\n0.42\n-\n-\n-0.88\n-0.71\n-\n-\n-0.24\n-0.24\nPaired score\n-0.42\n-0.24\n0.78\n0.67\n0.35\n0.25\n-0.15\n-0.20\n0.71\n0.56\n0.01\n-0.16\n-0.29\n-0.29\n-0.95\n-0.87\n0.01\n-0.03\nPooled score\n-0.70\n-0.60\n0.92\n0.83\n0.45\n0.31\n-0.36\n-0.38\n0.82\n0.60\n0.02\n-0.16\n-0.76\n-0.71\n-0.96\n-0.91\n-0.07\n-0.13\nACE\n-0.70\n-0.60\n0.92\n0.83\n0.45\n0.31\n-0.36\n-0.38\n0.82\n0.60\n0.02\n-0.16\n-0.76\n-0.71\n-0.96\n-0.91\n-0.07\n-0.13\nJULE: CDbw index\nRaw score\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nPaired score\n-0.20\n-0.11\n-0.23\n-0.11\n0.59\n0.42\n-0.13\n-0.11\n-0.55\n-0.38\n-0.15\n-0.20\n0.24\n0.07\n-0.81\n-0.64\n-0.16\n-0.13\n71\nPooled score\n-0.65\n-0.56\n0.87\n0.78\n0.54\n0.37\n-0.31\n-0.29\n0.90\n0.82\n0.06\n-0.07\n0.43\n0.36\n-0.53\n-0.38\n0.16\n0.13\nACE\n-0.64\n-0.51\n0.92\n0.83\n0.54\n0.37\n-0.31\n-0.29\n0.89\n0.78\n0.06\n-0.07\n0.48\n0.43\n-0.52\n-0.33\n0.18\n0.15\nDEPICT: Davies-Bouldin index\nRaw score\n-0.82\n-0.64\n1.00\n1.00\n0.03\n-0.11\n-0.50\n-0.33\n0.92\n0.82\n0.13\n0.15\nPaired score\n0.88\n0.82\n-0.77\n-0.60\n-0.37\n-0.22\n0.79\n0.73\n-0.10\n0.02\n0.09\n0.15\nPooled score\n0.90\n0.73\n0.90\n0.78\n0.47\n0.33\n0.88\n0.82\n0.92\n0.82\n0.81\n0.70\nACE\n0.93\n0.82\n0.96\n0.91\n0.92\n0.83\n0.93\n0.87\n0.96\n0.91\n0.94\n0.87\nUSPS (10)\nYTF (41)\nFRGC (20)\nMNIST-test (10)\nCMU-PIE (68)\nUMist (20)\nCOIL-20 (20)\nCOIL-100 (100)\nAverage\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nDEPICT: Calinski-Harabasz index\nRaw score\n0.88\n0.82\n-0.66\n-0.51\n-0.40\n-0.28\n0.82\n0.78\n-0.92\n-0.82\n-0.06\n-0.00\nPaired score\n0.88\n0.82\n-0.96\n-0.91\n-0.37\n-0.22\n0.79\n0.73\n-0.92\n-0.82\n-0.11\n-0.08\nPooled score\n0.88\n0.82\n-0.94\n-0.87\n-0.37\n-0.22\n0.82\n0.78\n0.44\n0.56\n0.17\n0.21\nACE\n0.88\n0.82\n-0.67\n-0.56\n0.92\n0.78\n0.82\n0.78\n0.92\n0.82\n0.57\n0.53\nDEPICT: Silhouette score (cosine distance)\nRaw score\n-0.39\n-0.33\n0.99\n0.96\n0.52\n0.39\n0.76\n0.56\n-0.43\n-0.33\n0.29\n0.25\nPaired score\n0.87\n0.78\n-0.69\n-0.56\n-0.37\n-0.22\n0.79\n0.73\n0.07\n0.11\n0.14\n0.17\nPooled score\n0.90\n0.73\n0.67\n0.51\n0.68\n0.56\n0.90\n0.82\n0.98\n0.91\n0.83\n0.71\nACE\n0.95\n0.87\n0.92\n0.82\n0.80\n0.67\n0.95\n0.87\n0.99\n0.96\n0.92\n0.84\nDEPICT: Silhouette score (euclidean distance)\nRaw score\n-0.28\n-0.24\n0.99\n0.96\n-0.20\n-0.17\n0.66\n0.51\n-0.43\n-0.33\n0.15\n0.14\nPaired score\n0.87\n0.78\n-0.64\n-0.51\n-0.37\n-0.22\n0.79\n0.73\n-0.12\n-0.02\n0.11\n0.15\nPooled score\n0.90\n0.73\n0.99\n0.96\n0.68\n0.56\n0.94\n0.87\n0.99\n0.96\n0.90\n0.81\nACE\n0.88\n0.82\n0.98\n0.91\n0.73\n0.56\n0.95\n0.87\n0.98\n0.91\n0.90\n0.81\nDEPICT: Cubic clustering criterion\nRaw score\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nPaired score\n-0.49\n-0.33\n0.99\n0.96\n0.90\n0.78\n-0.16\n-0.07\n0.98\n0.91\n0.44\n0.45\nPooled score\n-0.62\n-0.51\n0.99\n0.96\n0.38\n0.28\n-0.19\n-0.11\n0.92\n0.82\n0.29\n0.29\nACE\n-0.62\n-0.51\n0.99\n0.96\n0.38\n0.28\n-0.25\n-0.16\n0.92\n0.82\n0.28\n0.28\nDEPICT: Dunn index\nRaw score\n0.19\n0.11\n0.85\n0.73\n0.48\n0.39\n0.25\n0.20\n0.20\n0.24\n0.39\n0.34\nPaired score\n0.24\n0.20\n-0.19\n-0.11\n-0.20\n-0.06\n0.59\n0.47\n0.02\n-0.02\n0.09\n0.10\nPooled score\n0.24\n0.16\n-0.31\n-0.20\n0.60\n0.50\n0.46\n0.38\n0.22\n0.16\n0.24\n0.20\nACE\n-0.07\n-0.07\n0.09\n0.07\n0.60\n0.50\n0.53\n0.42\n0.22\n0.16\n0.28\n0.22\nDEPICT: Cindex\nRaw score\n-0.60\n-0.47\n0.61\n0.38\n0.73\n0.50\n-0.72\n-0.56\n0.85\n0.73\n0.18\n0.12\nPaired score\n0.88\n0.82\n-0.62\n-0.51\n-0.43\n-0.33\n0.78\n0.64\n0.12\n0.16\n0.14\n0.16\nPooled score\n-0.87\n-0.78\n0.99\n0.96\n0.37\n0.22\n-0.70\n-0.60\n0.92\n0.82\n0.14\n0.12\nACE\n-0.87\n-0.78\n0.99\n0.96\n0.23\n0.22\n-0.71\n-0.60\n0.92\n0.82\n0.11\n0.12\nDEPICT: SDbw index\nRaw score\n-0.83\n-0.69\n-0.51\n-0.40\n0.22\n0.11\n-0.72\n-0.56\n0.92\n0.82\n-0.19\n-0.14\nPaired score\n0.85\n0.73\n-0.38\n-0.24\n-0.37\n-0.22\n0.85\n0.78\n0.26\n0.29\n0.24\n0.27\nPooled score\n-0.85\n-0.78\n0.99\n0.96\n0.37\n0.22\n-0.71\n-0.56\n0.92\n0.82\n0.14\n0.13\nACE\n-0.85\n-0.78\n0.99\n0.96\n0.37\n0.22\n-0.74\n-0.64\n0.93\n0.87\n0.14\n0.12\nDEPICT: CDbw index\nRaw score\n-\n-\n-\n-\n-\n-\n-\n-\n-0.43\n-0.38\n-0.43\n-0.38\nPaired score\n0.84\n0.73\n-0.71\n-0.56\n-0.33\n-0.17\n0.81\n0.73\n-0.81\n-0.64\n-0.04\n0.02\nPooled score\n-0.47\n-0.42\n-0.09\n0.02\n-0.53\n-0.39\n-0.64\n-0.38\n0.85\n0.73\n-0.17\n-0.09\nACE\n-0.28\n-0.24\n0.37\n0.38\n-0.23\n-0.22\n-0.61\n-0.33\n0.90\n0.78\n0.03\n0.07\n\nDetermination of the number of clusters - Qualitative Analysis In this section, we present qualitative analysis results for determining the number of clusters using both JULE and *DEPICT*. Graphs illustrating the rank correlation between the retained spaces after the multimodality test, based on different validity indices, are provided in Figures 27 (Davies-Bouldin index), 29 (Calinski-Harabasz index), 31, and 33 (*DEPICT*: Silhouette score with euclidean distance) (*DEPICT*: Silhouette score with cosine distance) for the hyperparameter tuning task performed with *JULE* for deep clustering. Similarly, Figures 11 (Davies-Bouldin index), 13\n(Calinski-Harabasz index), 15 (Silhouette score with cosine distance), and 17 (Silhouette score with euclidean distance) present these graphs for the hyperparameter tuning task with *DEPICT*.\n\nIn each graph, spaces grouped together by a density-based clustering approach share the same color, while outlier spaces are uniformly colored in grey. Similar to the observations from the hyperparameter tuning task, we find that the grouping behavior varies depending on the chosen validity measures. However, in this task, where we generate around 10 spaces, we observe a tendency to have more cases with only one group. This suggests that the grouping behavior of embedding spaces also depends on the number of spaces included for comparison.\n\nWe employ t-SNE plots (Van der Maaten & Hinton, 2008) to visually assess the discriminative capability of embedding subspaces selected by *ACE* compared to those excluded by *ACE*. T-SNE, recognized for its ability to preserve local structure and relative distances in high-dimensional space, is utilized to project the embedding space into a 2-dimensional feature space for visualization. For the hyperparameter tuning task with *JULE*, we present this comparison based on different validity indices in Figures 20 (Davies-Bouldin index), 22 (Calinski-Harabasz index),\n26 (*DEPICT*: Silhouette score with euclidean distance), and 24 (Silhouette score with cosine distance). Similarly, for *DEPICT*, we provide comparisons in Figures 28 (Davies-Bouldin index),\n30 (Calinski-Harabasz index), 34 (*DEPICT*: Silhouette score with euclidean distance), and 32\n(Silhouette score with cosine distance). In each figure, we compare one selected space with an excluded space for each dataset. Different colors in each subfigure correspond to different true clusters. Due to space limitations, we have chosen one representative space from the retained spaces, resembling an admissible space, and one from the excluded spaces for concise comparison. If a subfigure of an excluded space is missing, it indicates that all the retained spaces have been chosen as admissible spaces by *ACE*, which can occur when the number of spaces for clustering is small. We consistently observe that the selected spaces exhibit better clustering behavior compared to the excluded spaces, albeit with a smaller difference than observed in the hyperparameter tuning task. In some scenarios, all spaces are selected with none excluded, reflecting the impact of the small number of spaces included for comparison (e.g., \u223c 10) in this experiment. This finding underscores the importance of considering the size of embedding spaces in the effectiveness of *ACE*.\n\nSelection of checkpoints In this section, we present the results of the checkpoint selection experiment. We observe that all 20 obtained embedding spaces fail to reject the null hypothesis in the Dip test, as evident in the t-SNE visualizations displayed in Figure 35. (Each subfigure is annotated with the Dip test p-value). Despite this, indicating no significant departure from unimodality, we run the rest part of *ACE* on all 20 spaces as well as the score pooling algorithm for comparison. We present the rank correlation results with NMI and ACC, employing the Silhouette score, Calinski-Harabasz index, and Davies-Bouldin index, in Table 10. The pooled scores, compared with *paired scores*, show superior performance across all reported indices in Table 10. This underscores the unreliable nature of conventional *paired scores* for evaluation, emphasizing the importance of comparing and evaluating clustering results within the same space. In this experiment, *pooled scores* exhibit slightly better performance than *ACE* scores, a reasonable outcome considering the lack of significant multimodality in the spaces and our strategy aimed at selecting and ranking spaces based on differences in quality.\n\n| Silhouette score (cosine)   |   Silhouette score (euclidean) |   Davies-Bouldin index |   Calinski-Harabasz in |\n|-----------------------------|--------------------------------|------------------------|------------------------|\n| r                           |                                |                        |                        |\n| s                           |                                |                        |                        |\n| \u03c4                           |                                |                        |                        |\n| B                           |                                |                        |                        |\n| r                           |                                |                        |                        |\n| s                           |                                |                        |                        |\n| \u03c4                           |                                |                        |                        |\n| B                           |                                |                        |                        |\n| r                           |                                |                        |                        |\n| s                           |                                |                        |                        |\n| \u03c4                           |                                |                        |                        |\n| B                           |                                |                        |                        |\n| r                           |                                |                        |                        |\n| s                           |                                |                        |                        |\n| \u03c4                           |                                |                        |                        |\n| B                           |                                |                        |                        |\n| NMI                         |                                |                        |                        |\n| Paired score                |                          -0.75 |                  -0.59 |                  -0.74 |\n| Pooled score                |                           0.43 |                   0.34 |                   0.43 |\n| ACE                         |                           0.4  |                   0.29 |                   0.4  |\n| ACC                         |                                |                        |                        |\n| Paired score                |                          -0.75 |                  -0.58 |                  -0.74 |\n| Pooled score                |                           0.43 |                   0.32 |                   0.43 |\n| ACE                         |                           0.4  |                   0.28 |                   0.4  |\n"
    },
    {
        "level": "##",
        "title": "A.6.5 Ablation Studies",
        "content": "\nIn this section, we conduct a series of ablation studies and sensitivity analyses to examine the impact of various components or factors within our algorithm on the overall evaluation performance.\n\nOur ablation studies specifically explore the effects of performing the Dip test, employing different FWERs (\u03b1) for edge inclusion in link analysis, utilizing diverse link analysis algorithms, and applying different density-based algorithms. Additionally, we introduce a comparative study that includes the outlier space\u2014an approach distinct from our existing strategy that excludes outlier space from clustering. Dip Test The Dip test serves as a filtering mechanism in our approach, targeting spaces that exhibit multimodality or clustering behavior. To assess the impact of removing the Dip test on model evaluation, specifically on the *pooled score* and *ACE* score, we conduct a thorough analysis. Tables 11 and 12 present a comparative evaluation, contrasting the performance of the *pooled score* with and without the Dip test, as well as *ACE* scores with and without the Dip test. The evaluation is based on both rank correlation with ACC and NMI, focusing on the hyperparameter tuning task. Similarly, Tables 13 and 14 extend this analysis to the task of determining the number of clusters. In our observations, we note that the application of the Dip test tends to enhance the performance of *ACE* in certain tasks, while its impact on the *pooled score* is relatively marginal. Specifically, *ACE* exhibits significant improvements\n(compared to *ACE* without the Dip test) in tasks such as hyperparameter tuning for *JULE* and DEPICT (Davies-Bouldin index). Additionally, notable improvements are observed in the task for determining the number of clusters for *JULE* (Davies-Bouldin index, Silhouette score using both euclidean and cosine distances) and *DEPICT* (Davies-Bouldin index, Silhouette score using euclidean distance). This observed enhancement in *ACE* performance can be attributed to its dependency on the quality of retained spaces. The proposed *ACE* relies on the retained spaces for voting and ranking, ultimately generating a quality score. In contrast, the *pooled score* simply averages over all retained spaces. In summary, our findings suggest that the Dip test contributes to the effectiveness of *ACE* in specific tasks, while its impact on the *pooled score* remains limited.\n\nUSPS\nYTF\nFRGC\nMNIST-test\nCMU-PIE\nUMist\nCOIL-20\nCOIL-100\nAverage\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nJULE: Calinski-Harabasz index\nPaired score\n0.17\n0.13\n0.52\n0.40\n-0.13\n-0.10\n0.49\n0.34\n-0.13\n-0.08\n0.70\n0.50\n0.53\n0.38\n0.20\n0.19\n0.29\n0.22\nPooled score (w/o. Dip test)\n0.85\n0.68\n0.91\n0.79\n0.31\n0.23\n0.82\n0.67\n0.90\n0.77\n0.63\n0.44\n0.61\n0.46\n0.91\n0.76\n0.74\n0.60\nPooled score\n0.84\n0.68\n0.91\n0.79\n0.29\n0.22\n0.82\n0.67\n0.94\n0.82\n0.81\n0.60\n0.62\n0.47\n0.89\n0.73\n0.77\n0.62\nACE (w/o .Dip test)\n0.80\n0.63\n0.90\n0.73\n0.42\n0.30\n0.86\n0.70\n0.98\n0.93\n0.71\n0.51\n0.92\n0.76\n0.92\n0.79\n0.81\n0.67\nACE\n0.80\n0.63\n0.90\n0.73\n0.39\n0.26\n0.87\n0.71\n0.98\n0.90\n0.81\n0.61\n0.60\n0.45\n0.95\n0.82\n0.79\n0.64\nJULE: Davies-Bouldin index\nPaired score\n-0.10\n-0.03\n-0.32\n-0.21\n-0.08\n-0.05\n-0.13\n-0.06\n0.26\n0.20\n0.62\n0.44\n0.61\n0.42\n0.43\n0.35\n0.16\n0.13\nPooled score (w/o. Dip test)\n-0.26\n-0.13\n-0.46\n-0.34\n0.12\n0.08\n-0.15\n-0.06\n0.92\n0.78\n-0.35\n-0.24\n-0.24\n-0.17\n-0.46\n-0.35\n-0.11\n-0.05\nPooled score\n-0.26\n-0.12\n-0.46\n-0.34\n0.11\n0.07\n-0.16\n-0.07\n0.92\n0.78\n0.30\n0.20\n-0.25\n-0.17\n-0.46\n-0.35\n-0.03\n-0.00\nACE (w/o .Dip test)\n-0.08\n-0.02\n-0.30\n-0.21\n0.22\n0.16\n0.73\n0.55\n0.03\n-0.01\n0.74\n0.54\n0.29\n0.26\n-0.49\n-0.39\n0.14\n0.11\nACE\n-0.08\n-0.02\n-0.30\n-0.21\n0.22\n0.16\n0.73\n0.55\n0.10\n0.06\n0.38\n0.27\n0.23\n0.22\n0.48\n0.33\n0.22\n0.17\nJULE: Silhouette score (cosine distance)\nPaired score\n0.28\n0.22\n0.73\n0.56\n0.09\n0.06\n0.63\n0.47\n0.50\n0.36\n0.71\n0.50\n0.68\n0.50\n0.74\n0.54\n0.54\n0.40\nPooled score (w/o. Dip test)\n0.71\n0.58\n0.93\n0.81\n0.41\n0.28\n0.79\n0.64\n0.95\n0.84\n0.58\n0.39\n0.26\n0.16\n0.69\n0.53\n0.66\n0.53\nPooled score\n0.70\n0.56\n0.93\n0.81\n0.40\n0.27\n0.79\n0.64\n0.95\n0.85\n0.77\n0.56\n0.27\n0.16\n0.68\n0.52\n0.69\n0.55\nACE (w/o .Dip test)\n0.89\n0.73\n0.93\n0.83\n0.52\n0.35\n0.81\n0.66\n0.99\n0.94\n0.83\n0.65\n0.44\n0.38\n0.91\n0.77\n0.79\n0.66\nACE\n0.89\n0.73\n0.93\n0.83\n0.52\n0.35\n0.81\n0.66\n0.99\n0.93\n0.79\n0.59\n0.44\n0.38\n0.92\n0.78\n0.79\n0.66\nJULE: Silhouette score (euclidean distance)\nPaired score\n0.27\n0.20\n0.72\n0.55\n0.04\n0.03\n0.56\n0.41\n0.42\n0.30\n0.70\n0.50\n0.64\n0.46\n0.55\n0.41\n0.49\n0.36\nPooled score (w/o. Dip test)\n0.70\n0.57\n0.90\n0.77\n0.41\n0.28\n0.78\n0.63\n0.95\n0.84\n0.64\n0.43\n0.25\n0.16\n0.71\n0.54\n0.67\n0.53\nPooled score\n0.71\n0.58\n0.90\n0.77\n0.41\n0.28\n0.78\n0.63\n0.96\n0.85\n0.79\n0.57\n0.26\n0.16\n0.70\n0.54\n0.69\n0.55\nACE (w/o .Dip test)\n0.88\n0.72\n0.89\n0.75\n0.42\n0.28\n0.81\n0.65\n0.98\n0.92\n0.88\n0.70\n0.41\n0.36\n0.91\n0.78\n0.77\n0.65\nACE\n0.88\n0.72\n0.89\n0.75\n0.42\n0.28\n0.81\n0.65\n0.98\n0.90\n0.88\n0.70\n0.41\n0.36\n0.92\n0.78\n0.77\n0.64\nDEPICT: Calinski-Harabasz index\nPaired score\n0.76\n0.57\n0.44\n0.26\n0.76\n0.57\n0.89\n0.72\n0.49\n0.44\n0.67\n0.51\nPooled score (w/o. Dip test)\n0.96\n0.84\n0.53\n0.41\n0.90\n0.77\n0.96\n0.87\n0.73\n0.59\n0.82\n0.70\nPooled score\n0.96\n0.83\n0.53\n0.41\n0.90\n0.77\n0.96\n0.87\n0.61\n0.56\n0.79\n0.69\nACE (w/o .Dip test)\n0.91\n0.77\n0.56\n0.44\n0.94\n0.82\n0.96\n0.87\n0.96\n0.88\n0.87\n0.75\nACE\n0.91\n0.77\n0.56\n0.44\n0.94\n0.82\n0.96\n0.87\n0.96\n0.87\n0.87\n0.75\nDEPICT: Davies-Bouldin index\nPaired score\n0.81\n0.59\n0.45\n0.31\n0.90\n0.74\n0.89\n0.72\n0.63\n0.59\n0.73\n0.59\nPooled score (w/o. Dip test)\n0.95\n0.84\n0.49\n0.35\n0.65\n0.50\n0.50\n0.36\n0.23\n0.06\n0.56\n0.42\nPooled score\n0.96\n0.88\n0.49\n0.35\n0.64\n0.48\n0.43\n0.32\n-0.77\n-0.61\n0.35\n0.28\nACE (w/o .Dip test)\n0.90\n0.79\n0.76\n0.58\n0.91\n0.79\n0.95\n0.83\n0.63\n0.49\n0.83\n0.70\nACE\n0.91\n0.82\n0.76\n0.58\n0.91\n0.79\n0.96\n0.87\n0.98\n0.92\n0.90\n0.80\nDEPICT: Silhouette score (cosine distance)\nPaired score\n0.81\n0.62\n0.45\n0.33\n0.90\n0.75\n0.89\n0.72\n0.77\n0.58\n0.76\n0.60\nPooled score (w/o. Dip test)\n0.96\n0.83\n0.68\n0.56\n0.94\n0.82\n0.95\n0.87\n0.95\n0.86\n0.90\n0.79\nPooled score\n0.96\n0.86\n0.68\n0.56\n0.94\n0.82\n0.97\n0.90\n0.93\n0.79\n0.90\n0.78\nACE (w/o .Dip test)\n0.97\n0.90\n0.71\n0.56\n0.94\n0.82\n0.98\n0.91\n0.95\n0.84\n0.91\n0.80\nACE\n0.97\n0.90\n0.71\n0.56\n0.94\n0.82\n0.97\n0.90\n0.94\n0.83\n0.91\n0.80\nDEPICT: Silhouette score (euclidean distance)\nPaired score\n0.73\n0.50\n0.47\n0.36\n0.79\n0.65\n0.86\n0.69\n0.59\n0.52\n0.69\n0.54\nPooled score (w/o. Dip test)\n0.96\n0.84\n0.65\n0.53\n0.94\n0.82\n0.97\n0.90\n0.95\n0.86\n0.89\n0.79\nPooled score\n0.96\n0.86\n0.65\n0.53\n0.94\n0.82\n0.97\n0.90\n0.92\n0.75\n0.89\n0.77\nACE (w/o .Dip test)\n0.92\n0.80\n0.65\n0.50\n0.95\n0.83\n0.98\n0.90\n0.95\n0.83\n0.89\n0.77\nACE\n0.97\n0.88\n0.65\n0.50\n0.95\n0.83\n0.98\n0.90\n0.94\n0.82\n0.90\n0.79\nUSPS\nYTF\nFRGC\nMNIST-test\nCMU-PIE\nUMist\nCOIL-20\nCOIL-100\nAverage\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nJULE: Calinski-Harabasz index\nPaired score\n0.04\n0.05\n0.39\n0.27\n-0.26\n-0.18\n0.31\n0.21\n-0.20\n-0.12\n0.64\n0.45\n0.57\n0.40\n0.09\n0.08\n0.20\n0.14\nPooled score (w/o. Dip test)\n0.92\n0.79\n0.78\n0.61\n0.30\n0.21\n0.91\n0.77\n0.91\n0.78\n0.65\n0.47\n0.57\n0.42\n0.91\n0.78\n0.74\n0.60\nPooled score\n0.91\n0.78\n0.78\n0.61\n0.30\n0.21\n0.91\n0.77\n0.95\n0.83\n0.81\n0.60\n0.58\n0.43\n0.90\n0.75\n0.77\n0.62\nACE (w/o .Dip test)\n0.90\n0.77\n0.73\n0.54\n0.59\n0.44\n0.95\n0.81\n0.97\n0.89\n0.67\n0.49\n0.89\n0.72\n0.88\n0.74\n0.82\n0.68\nACE\n0.90\n0.77\n0.73\n0.54\n0.49\n0.36\n0.95\n0.82\n0.97\n0.87\n0.81\n0.61\n0.57\n0.40\n0.93\n0.81\n0.79\n0.65\nJULE: Davies-Bouldin index\nPaired score\n-0.27\n-0.15\n-0.14\n-0.09\n-0.23\n-0.14\n-0.35\n-0.19\n0.20\n0.16\n0.53\n0.36\n0.63\n0.44\n0.33\n0.26\n0.09\n0.08\nPooled score (w/o. Dip test)\n-0.49\n-0.21\n-0.35\n-0.23\n0.49\n0.36\n-0.35\n-0.20\n0.89\n0.76\n-0.47\n-0.34\n-0.30\n-0.22\n-0.48\n-0.34\n-0.13\n-0.05\nPooled score\n-0.49\n-0.20\n-0.35\n-0.23\n0.48\n0.36\n-0.35\n-0.21\n0.89\n0.75\n0.17\n0.11\n-0.29\n-0.22\n-0.48\n-0.34\n-0.05\n0.00\nACE (w/o .Dip test)\n-0.30\n-0.09\n-0.07\n-0.07\n0.53\n0.38\n0.79\n0.64\n0.01\n-0.04\n0.66\n0.45\n0.27\n0.23\n-0.49\n-0.35\n0.17\n0.14\nACE\n-0.30\n-0.09\n-0.07\n-0.07\n0.53\n0.38\n0.79\n0.64\n0.07\n0.03\n0.27\n0.20\n0.21\n0.18\n0.44\n0.28\n0.24\n0.19\nJULE: Silhouette score (cosine distance)\nPaired score\n0.17\n0.14\n0.59\n0.41\n0.07\n0.06\n0.47\n0.33\n0.45\n0.33\n0.64\n0.46\n0.70\n0.51\n0.64\n0.45\n0.47\n0.34\nPooled score (w/o. Dip test)\n0.75\n0.70\n0.73\n0.55\n0.71\n0.53\n0.90\n0.73\n0.96\n0.87\n0.57\n0.38\n0.19\n0.10\n0.60\n0.44\n0.68\n0.54\nPooled score\n0.74\n0.68\n0.73\n0.55\n0.71\n0.53\n0.90\n0.73\n0.96\n0.88\n0.75\n0.55\n0.20\n0.11\n0.61\n0.44\n0.70\n0.56\nACE (w/o .Dip test)\n0.96\n0.85\n0.74\n0.55\n0.82\n0.65\n0.92\n0.78\n0.99\n0.94\n0.80\n0.61\n0.41\n0.32\n0.81\n0.65\n0.81\n0.67\nACE\n0.96\n0.85\n0.74\n0.55\n0.82\n0.65\n0.92\n0.78\n0.98\n0.92\n0.78\n0.58\n0.41\n0.32\n0.84\n0.68\n0.81\n0.67\nJULE: Silhouette score (euclidean distance)\nPaired score\n0.14\n0.12\n0.54\n0.39\n-0.08\n-0.02\n0.41\n0.27\n0.36\n0.27\n0.64\n0.46\n0.67\n0.48\n0.44\n0.31\n0.39\n0.28\nPooled score (w/o. Dip test)\n0.74\n0.68\n0.66\n0.49\n0.71\n0.53\n0.89\n0.72\n0.96\n0.87\n0.64\n0.43\n0.19\n0.10\n0.62\n0.45\n0.68\n0.53\nPooled score\n0.73\n0.67\n0.66\n0.49\n0.70\n0.53\n0.89\n0.72\n0.97\n0.88\n0.77\n0.57\n0.20\n0.11\n0.62\n0.45\n0.69\n0.55\nACE (w/o .Dip test)\n0.93\n0.78\n0.63\n0.48\n0.71\n0.53\n0.92\n0.78\n0.99\n0.94\n0.86\n0.68\n0.39\n0.30\n0.81\n0.66\n0.78\n0.64\nACE\n0.93\n0.78\n0.63\n0.48\n0.71\n0.53\n0.92\n0.78\n0.98\n0.91\n0.86\n0.68\n0.39\n0.30\n0.84\n0.68\n0.78\n0.64\n\nDEPICT: Calinski-Harabasz index\n\nPaired score\n0.56\n0.40\n0.54\n0.35\n0.76\n0.57\n0.88\n0.69\n0.48\n0.43\n0.64\n0.49\nPooled score (w/o. Dip test)\n0.94\n0.83\n0.54\n0.45\n0.92\n0.79\n0.95\n0.86\n0.74\n0.62\n0.82\n0.71\nPooled score\n0.94\n0.82\n0.54\n0.45\n0.92\n0.79\n0.95\n0.86\n0.62\n0.55\n0.79\n0.69\nACE (w/o .Dip test)\n0.82\n0.72\n0.61\n0.45\n0.91\n0.82\n0.97\n0.91\n0.98\n0.91\n0.86\n0.76\nACE\n0.82\n0.72\n0.61\n0.45\n0.91\n0.82\n0.97\n0.91\n0.96\n0.87\n0.86\n0.75\nDEPICT: Davies-Bouldin index\nPaired score\n0.61\n0.42\n0.48\n0.32\n0.92\n0.74\n0.88\n0.69\n0.62\n0.56\n0.70\n0.55\nPooled score (w/o. Dip test)\n0.93\n0.80\n0.40\n0.28\n0.65\n0.50\n0.45\n0.32\n0.24\n0.07\n0.53\n0.39\nPooled score\n0.95\n0.84\n0.40\n0.28\n0.64\n0.48\n0.38\n0.28\n-0.76\n-0.60\n0.32\n0.26\nACE (w/o .Dip test)\n0.99\n0.96\n0.65\n0.46\n0.90\n0.74\n0.99\n0.92\n0.60\n0.46\n0.82\n0.71\nACE\n0.99\n0.96\n0.65\n0.46\n0.90\n0.74\n0.99\n0.96\n0.96\n0.87\n0.90\n0.80\nDEPICT: Silhouette score (cosine distance)\nPaired score\n0.62\n0.45\n0.53\n0.42\n0.91\n0.75\n0.88\n0.69\n0.77\n0.58\n0.74\n0.58\nPooled score (w/o. Dip test)\n0.96\n0.87\n0.75\n0.59\n0.94\n0.82\n0.96\n0.88\n0.95\n0.85\n0.91\n0.80\nPooled score\n0.96\n0.87\n0.75\n0.59\n0.94\n0.82\n0.96\n0.88\n0.93\n0.76\n0.91\n0.78\nACE (w/o .Dip test)\n0.95\n0.88\n0.70\n0.54\n0.91\n0.77\n0.96\n0.90\n0.96\n0.87\n0.90\n0.79\nACE\n0.95\n0.88\n0.70\n0.54\n0.91\n0.77\n0.96\n0.88\n0.94\n0.83\n0.89\n0.78\nDEPICT: Silhouette score (euclidean distance)\nPaired score\n0.52\n0.33\n0.57\n0.45\n0.80\n0.62\n0.85\n0.65\n0.59\n0.48\n0.67\n0.51\nPooled score (w/o. Dip test)\n0.95\n0.86\n0.72\n0.57\n0.94\n0.82\n0.96\n0.88\n0.95\n0.85\n0.91\n0.80\nPooled score\n0.94\n0.84\n0.72\n0.57\n0.94\n0.82\n0.96\n0.88\n0.92\n0.75\n0.90\n0.77\nACE (w/o .Dip test)\n0.94\n0.84\n0.63\n0.49\n0.91\n0.78\n0.97\n0.91\n0.95\n0.85\n0.88\n0.77\nACE\n0.95\n0.87\n0.63\n0.49\n0.91\n0.78\n0.97\n0.91\n0.95\n0.84\n0.88\n0.78\nUSPS (10)\nYTF (41)\nFRGC (20)\nMNIST-test (10)\nCMU-PIE (68)\nUMist (20)\nCOIL-20 (20)\nCOIL-100 (100)\nAverage\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nJULE: Calinski-Harabasz index\nPaired score\n0.65 (10)\n0.64 (10)\n0.1 (50)\n0.06 (50)\n-0.93 (15)\n-0.83 (15)\n0.64 (10)\n0.6 (10)\n-0.03 (20)\n-0.02 (20)\n-0.13 (5)\n-0.07 (5)\n0.76 (15)\n0.71 (15)\n0.74 (80)\n0.56 (80)\n0.22\n0.21\nPooled score (w/o. Dip test)\n0.55 (10)\n0.6 (10)\n0.9 (50)\n0.78 (50)\n-0.87 (15)\n-0.72 (15)\n0.64 (10)\n0.6 (10)\n0.88 (70)\n0.73 (70)\n-0.14 (5)\n-0.11 (5)\n0.74 (15)\n0.64 (15)\n0.72 (80)\n0.64 (80)\n0.43\n0.40\nPooled score\n0.65 (10)\n0.64 (10)\n0.9 (50)\n0.78 (50)\n-0.87 (15)\n-0.72 (15)\n0.64 (10)\n0.6 (10)\n0.9 (70)\n0.73 (70)\n-0.14 (5)\n-0.11 (5)\n0.74 (15)\n0.64 (15)\n0.72 (80)\n0.64 (80)\n0.44\n0.40\nACE (w/o .Dip test)\n0.65 (10)\n0.64 (10)\n0.93 (50)\n0.83 (50)\n-0.72 (15)\n-0.67 (15)\n0.64 (10)\n0.6 (10)\n0.88 (70)\n0.73 (70)\n-0.13 (5)\n-0.07 (5)\n0.74 (15)\n0.64 (15)\n0.79 (80)\n0.69 (80)\n0.47\n0.42\nACE\n0.65 (10)\n0.64 (10)\n0.93 (50)\n0.83 (50)\n-0.72 (15)\n-0.67 (15)\n0.64 (10)\n0.6 (10)\n0.88 (70)\n0.73 (70)\n-0.14 (5)\n-0.11 (5)\n0.74 (15)\n0.64 (15)\n0.79 (80)\n0.69 (80)\n0.47\n0.42\nJULE: Davies-Bouldin index\nPaired score\n0.54 (15)\n0.38 (15)\n0.15 (50)\n0.17 (50)\n0.85 (45)\n0.67 (45)\n0.43 (10)\n0.29 (10)\n0.78 (100)\n0.56 (100)\n-0.08 (45)\n0.02 (45)\n-0.26 (40)\n-0.14 (40)\n-0.9 (20)\n-0.78 (20)\n0.19\n0.15\nPooled score (w/o. Dip test)\n0.88 (15)\n0.73 (15)\n0.83 (50)\n0.67 (50)\n0.82 (40)\n0.61 (40)\n0.81 (10)\n0.64 (10)\n0.82 (90)\n0.64 (90)\n0.12 (50)\n0.11 (50)\n-0.67 (50)\n-0.5 (50)\n-0.92 (20)\n-0.82 (20)\n0.34\n0.26\nPooled score\n0.98 (15)\n0.91 (15)\n0.83 (50)\n0.67 (50)\n0.82 (40)\n0.61 (40)\n0.79 (10)\n0.6 (10)\n0.82 (90)\n0.64 (90)\n-0.21 (45)\n-0.02 (45)\n-0.76 (50)\n-0.57 (50)\n-0.92 (20)\n-0.82 (20)\n0.29\n0.25\nACE (w/o .Dip test)\n0.06 (30)\n0.07 (30)\n0.83 (50)\n0.67 (50)\n0.87 (40)\n0.72 (40)\n0.65 (25)\n0.51 (25)\n0.99 (70)\n0.96 (70)\n0.12 (50)\n0.11 (50)\n-0.67 (50)\n-0.5 (50)\n-0.92 (20)\n-0.82 (20)\n0.24\n0.22\nACE\n0.98 (15)\n0.91 (15)\n0.83 (50)\n0.67 (50)\n0.87 (40)\n0.72 (40)\n0.79 (10)\n0.6 (10)\n0.85 (90)\n0.69 (90)\n-0.21 (45)\n-0.02 (45)\n-0.69 (50)\n-0.57 (50)\n-0.94 (20)\n-0.82 (20)\n0.31\n0.27\nJULE: Silhouette score (cosine distance)\nPaired score\n0.99 (10)\n0.96 (10)\n0.3 (50)\n0.22 (50)\n0.72 (25)\n0.61 (25)\n0.87 (10)\n0.69 (10)\n0.98 (70)\n0.91 (70)\n-0.07 (45)\n0.07 (45)\n0.52 (25)\n0.36 (25)\n0.39 (200)\n0.2 (200)\n0.59\n0.50\nPooled score (w/o. Dip test)\n0.98 (10)\n0.91 (10)\n0.98 (50)\n0.94 (50)\n0.68 (45)\n0.56 (45)\n0.93 (10)\n0.82 (10)\n0.98 (70)\n0.91 (70)\n0.21 (45)\n0.16 (45)\n0.36 (25)\n0.21 (25)\n0.47 (200)\n0.33 (200)\n0.70\n0.60\nPooled score\n0.95 (10)\n0.87 (10)\n0.98 (50)\n0.94 (50)\n0.68 (45)\n0.56 (45)\n0.96 (10)\n0.87 (10)\n0.98 (70)\n0.91 (70)\n-0.07 (45)\n-0.02 (45)\n0.71 (20)\n0.57 (20)\n0.41 (200)\n0.24 (200)\n0.70\n0.62\nACE (w/o .Dip test)\n0.92 (10)\n0.82 (10)\n0.98 (50)\n0.94 (50)\n0.7 (45)\n0.61 (45)\n0.99 (10)\n0.96 (10)\n0.98 (70)\n0.91 (70)\n-0.48 (5)\n-0.38 (5)\n-0.24 (45)\n-0.14 (45)\n0.47 (200)\n0.33 (200)\n0.54\n0.51\nACE\n0.95 (10)\n0.87 (10)\n0.98 (50)\n0.94 (50)\n0.7 (45)\n0.61 (45)\n0.96 (10)\n0.87 (10)\n0.98 (70)\n0.91 (70)\n-0.07 (45)\n-0.02 (45)\n0.74 (20)\n0.5 (20)\n0.46 (180)\n0.33 (180)\n0.71\n0.63\nPaired score\n0.85 (10)\n0.73 (10)\n0.33 (50)\n0.28 (50)\n0.72 (25)\n0.61 (25)\n0.88 (10)\n0.69 (10)\n0.96 (80)\n0.87 (80)\n0.07 (45)\n0.16 (45)\n0.55 (25)\n0.43 (25)\n0.44 (200)\n0.29 (200)\n0.60\n0.51\nPooled score (w/o. Dip test)\n0.98 (10)\n0.91 (10)\n0.97 (50)\n0.89 (50)\n0.68 (45)\n0.56 (45)\n0.93 (10)\n0.82 (10)\n0.98 (70)\n0.91 (70)\n0.21 (45)\n0.16 (45)\n0.36 (25)\n0.21 (25)\n0.47 (200)\n0.33 (200)\n0.70\n0.60\nPooled score\n0.95 (10)\n0.87 (10)\n0.97 (50)\n0.89 (50)\n0.68 (45)\n0.56 (45)\n0.95 (10)\n0.82 (10)\n0.98 (70)\n0.91 (70)\n0.14 (45)\n0.11 (45)\n0.76 (25)\n0.57 (25)\n0.47 (200)\n0.33 (200)\n0.74\n0.63\nACE (w/o .Dip test)\n0.79 (10)\n0.73 (10)\n0.98 (50)\n0.94 (50)\n0.78 (45)\n0.67 (45)\n0.92 (10)\n0.82 (10)\n0.99 (70)\n0.96 (70)\n-0.69 (5)\n-0.51 (5)\n0.24 (25)\n0.14 (25)\n0.43 (160)\n0.29 (160)\n0.55\n0.50\nACE\n0.95 (10)\n0.87 (10)\n0.98 (50)\n0.94 (50)\n0.78 (45)\n0.67 (45)\n0.95 (10)\n0.82 (10)\n0.98 (70)\n0.91 (70)\n0.14 (45)\n0.11 (45)\n0.71 (25)\n0.43 (25)\n0.47 (200)\n0.33 (200)\n0.74\n0.64\nDEPICT: Calinski-Harabasz index\nPaired score\n0.46 (5)\n0.6 (5)\n-0.99 (5)\n-0.96 (5)\n-0.85 (10)\n-0.72 (10)\n0.44 (5)\n0.56 (5)\n-0.92 (10)\n-0.82 (10)\n-0.37\n-0.27\nPooled score (w/o. Dip test)\n0.46 (5)\n0.6 (5)\n-0.98 (5)\n-0.91 (5)\n-0.85 (10)\n-0.72 (10)\n0.46 (5)\n0.6 (5)\n0.44 (10)\n0.56 (10)\n-0.09\n0.03\nPooled score\n0.46 (5)\n0.6 (5)\n-0.98 (5)\n-0.91 (5)\n-0.85 (10)\n-0.72 (10)\n0.46 (5)\n0.6 (5)\n0.44 (10)\n0.56 (10)\n-0.09\n0.03\nACE (w/o .Dip test)\n0.46 (5)\n0.6 (5)\n-0.66 (5)\n-0.51 (5)\n0.77 (30)\n0.61 (30)\n0.44 (5)\n0.56 (5)\n0.92 (80)\n0.82 (80)\n0.39\n0.42\nACE\n0.46 (5)\n0.6 (5)\n-0.66 (5)\n-0.51 (5)\n0.77 (30)\n0.61 (30)\n0.46 (5)\n0.6 (5)\n0.92 (80)\n0.82 (80)\n0.39\n0.42\nDEPICT: Davies-Bouldin index\nPaired score\n0.46 (5)\n0.6 (5)\n-0.78 (5)\n-0.64 (5)\n-0.85 (10)\n-0.72 (10)\n0.44 (5)\n0.56 (5)\n-0.1 (10)\n0.02 (10)\n-0.17\n-0.04\nPooled score (w/o. Dip test)\n0.7 (15)\n0.64 (15)\n0.88 (50)\n0.73 (50)\n-0.13 (20)\n-0.17 (20)\n0.94 (10)\n0.82 (10)\n0.92 (100)\n0.82 (100)\n0.66\n0.57\nPooled score\n0.6 (15)\n0.51 (15)\n0.88 (50)\n0.73 (50)\n-0.13 (20)\n-0.17 (20)\n0.74 (10)\n0.64 (10)\n0.92 (100)\n0.82 (100)\n0.60\n0.51\nACE (w/o .Dip test)\n0.94 (15)\n0.82 (15)\n0.95 (50)\n0.87 (50)\n0.77 (35)\n0.67 (35)\n0.93 (15)\n0.78 (15)\n0.96 (70)\n0.91 (70)\n0.91\n0.81\nACE\n0.62 (10)\n0.6 (10)\n0.95 (50)\n0.87 (50)\n0.77 (35)\n0.67 (35)\n0.78 (10)\n0.69 (10)\n0.96 (70)\n0.91 (70)\n0.82\n0.75\nDEPICT: Silhouette score (cosine distance)\nPaired score\n0.44 (5)\n0.56 (5)\n-0.7 (5)\n-0.6 (5)\n-0.85 (10)\n-0.72 (10)\n0.44 (5)\n0.56 (5)\n0.07 (10)\n0.11 (10)\n-0.12\n-0.02\nPooled score (w/o. Dip test)\n0.89 (15)\n0.78 (15)\n0.61 (40)\n0.47 (40)\n0.07 (25)\n0.06 (25)\n0.85 (10)\n0.78 (10)\n0.98 (80)\n0.91 (80)\n0.68\n0.60\nPooled score\n0.6 (15)\n0.51 (15)\n0.61 (40)\n0.47 (40)\n0.07 (25)\n0.06 (25)\n0.71 (10)\n0.64 (10)\n0.98 (80)\n0.91 (80)\n0.59\n0.52\nACE (w/o .Dip test)\n0.83 (25)\n0.69 (25)\n0.87 (40)\n0.78 (40)\n0.93 (35)\n0.83 (35)\n0.92 (10)\n0.82 (10)\n0.99 (80)\n0.96 (80)\n0.91\n0.82\nACE\n0.65 (15)\n0.64 (15)\n0.87 (40)\n0.78 (40)\n0.93 (35)\n0.83 (35)\n0.85 (10)\n0.78 (10)\n0.99 (80)\n0.96 (80)\n0.86\n0.80\nDEPICT: Silhouette score (euclidean distance)\nPaired score\n0.44 (5)\n0.56 (5)\n-0.61 (5)\n-0.47 (5)\n-0.85 (10)\n-0.72 (10)\n0.44 (5)\n0.56 (5)\n-0.12 (10)\n-0.02 (10)\n-0.14\n-0.02\nPooled score (w/o. Dip test)\n0.74 (15)\n0.64 (15)\n0.98 (50)\n0.91 (50)\n0.07 (25)\n0.06 (25)\n0.81 (10)\n0.73 (10)\n0.99 (80)\n0.96 (80)\n0.72\n0.66\nPooled score\n0.6 (15)\n0.51 (15)\n0.98 (50)\n0.91 (50)\n0.07 (25)\n0.06 (25)\n0.73 (10)\n0.69 (10)\n0.99 (80)\n0.96 (80)\n0.67\n0.63\nACE (w/o .Dip test)\n0.65 (15)\n0.64 (15)\n0.94 (40)\n0.87 (40)\n0.02 (25)\n0.06 (25)\n0.9 (15)\n0.78 (15)\n0.98 (80)\n0.91 (80)\n0.70\n0.65\nACE\n0.46 (5)\n0.6 (5)\n0.94 (40)\n0.87 (40)\n0.02 (25)\n0.06 (25)\n0.85 (10)\n0.78 (10)\n0.98 (80)\n0.91 (80)\n0.65\n0.64\nUSPS (10)\nYTF (41)\nFRGC (20)\nMNIST-test (10)\nCMU-PIE (68)\nUMist (20)\nCOIL-20 (20)\nCOIL-100 (100)\nAverage\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nJULE: Calinski-Harabasz index\nPaired score\n0.84\n0.73\n0.03\n-0.06\n-0.49\n-0.31\n0.61\n0.56\n-0.09\n-0.07\n-0.04\n0.07\n0.74\n0.64\n0.60\n0.51\n0.27\n0.26\nPooled score (w/o. Dip test)\n0.78\n0.69\n0.88\n0.78\n-0.37\n-0.20\n0.61\n0.56\n0.83\n0.69\n-0.07\n0.02\n0.76\n0.71\n0.56\n0.51\n0.50\n0.47\nPooled score\n0.84\n0.73\n0.88\n0.78\n-0.37\n-0.20\n0.61\n0.56\n0.85\n0.69\n-0.07\n0.02\n0.76\n0.71\n0.56\n0.51\n0.51\n0.48\nACE (w/o .Dip test)\n0.84\n0.73\n0.92\n0.83\n-0.11\n-0.03\n0.61\n0.56\n0.83\n0.69\n-0.04\n0.07\n0.76\n0.71\n0.65\n0.56\n0.56\n0.52\nACE\n0.84\n0.73\n0.92\n0.83\n-0.11\n-0.03\n0.61\n0.56\n0.83\n0.69\n-0.07\n0.02\n0.76\n0.71\n0.65\n0.56\n0.55\n0.51\nJULE: Davies-Bouldin index\nPaired score\n0.39\n0.29\n0.10\n0.06\n0.37\n0.25\n0.49\n0.33\n0.83\n0.60\n-0.28\n-0.29\n-0.29\n-0.21\n-0.87\n-0.73\n0.09\n0.04\nPooled score (w/o. Dip test)\n0.77\n0.56\n0.80\n0.67\n0.71\n0.54\n0.84\n0.69\n0.85\n0.69\n-0.06\n-0.20\n-0.69\n-0.57\n-0.79\n-0.69\n0.30\n0.21\nPooled score\n0.89\n0.73\n0.80\n0.67\n0.71\n0.54\n0.83\n0.64\n0.85\n0.69\n-0.42\n-0.33\n-0.79\n-0.64\n-0.79\n-0.69\n0.26\n0.20\nACE (w/o .Dip test)\n-0.15\n-0.11\n0.80\n0.67\n0.60\n0.42\n0.67\n0.56\n1.00\n1.00\n-0.06\n-0.20\n-0.69\n-0.57\n-0.79\n-0.69\n0.17\n0.13\nACE\n0.89\n0.73\n0.80\n0.67\n0.60\n0.42\n0.83\n0.64\n0.88\n0.73\n-0.42\n-0.33\n-0.71\n-0.64\n-0.82\n-0.69\n0.26\n0.19\nJULE: Silhouette score (cosine distance)\nPaired score\n0.89\n0.78\n0.27\n0.22\n0.21\n0.09\n0.81\n0.64\n0.99\n0.96\n-0.26\n-0.24\n0.55\n0.43\n0.52\n0.33\n0.50\n0.40\nPooled score (w/o. Dip test)\n0.88\n0.73\n0.98\n0.94\n0.61\n0.48\n0.90\n0.78\n0.99\n0.96\n0.04\n-0.07\n0.38\n0.29\n0.59\n0.47\n0.67\n0.57\nPooled score\n0.95\n0.87\n0.98\n0.94\n0.61\n0.48\n0.94\n0.82\n0.99\n0.96\n-0.32\n-0.24\n0.67\n0.50\n0.54\n0.38\n0.67\n0.59\nACE (w/o .Dip test)\n0.96\n0.91\n0.98\n0.94\n0.64\n0.54\n0.98\n0.91\n0.99\n0.96\n-0.76\n-0.60\n-0.21\n-0.07\n0.59\n0.47\n0.52\n0.51\nACE\n0.95\n0.87\n0.98\n0.94\n0.64\n0.54\n0.94\n0.82\n0.99\n0.96\n-0.32\n-0.24\n0.76\n0.57\n0.60\n0.47\n0.69\n0.61\nJULE: Silhouette score (euclidean distance)\nPaired score\n0.93\n0.82\n0.30\n0.28\n0.21\n0.09\n0.82\n0.64\n0.98\n0.91\n-0.13\n-0.16\n0.52\n0.36\n0.55\n0.42\n0.52\n0.42\nPooled score (w/o. Dip test)\n0.88\n0.73\n0.97\n0.89\n0.61\n0.48\n0.90\n0.78\n0.99\n0.96\n0.04\n-0.07\n0.33\n0.14\n0.59\n0.47\n0.66\n0.55\nPooled score\n0.95\n0.87\n0.97\n0.89\n0.61\n0.48\n0.92\n0.78\n0.99\n0.96\n-0.03\n-0.11\n0.74\n0.50\n0.59\n0.47\n0.72\n0.60\nACE (w/o .Dip test)\n0.90\n0.82\n0.98\n0.94\n0.57\n0.48\n0.89\n0.78\n1.00\n1.00\n-0.89\n-0.73\n0.31\n0.21\n0.56\n0.42\n0.54\n0.49\nACE\n0.95\n0.87\n0.98\n0.94\n0.57\n0.48\n0.92\n0.78\n0.99\n0.96\n-0.03\n-0.11\n0.74\n0.50\n0.59\n0.47\n0.71\n0.61\nDEPICT: Calinski-Harabasz index\nPaired score\n0.88\n0.82\n-0.96\n-0.91\n-0.37\n-0.22\n0.79\n0.73\n-0.92\n-0.82\n-0.11\n-0.08\nPooled score (w/o. Dip test)\n0.88\n0.82\n-0.94\n-0.87\n-0.37\n-0.22\n0.82\n0.78\n0.44\n0.56\n0.17\n0.21\nPooled score\n0.88\n0.82\n-0.94\n-0.87\n-0.37\n-0.22\n0.82\n0.78\n0.44\n0.56\n0.17\n0.21\nACE (w/o .Dip test)\n0.88\n0.82\n-0.67\n-0.56\n0.92\n0.78\n0.81\n0.73\n0.92\n0.82\n0.57\n0.52\nACE\n0.88\n0.82\n-0.67\n-0.56\n0.92\n0.78\n0.82\n0.78\n0.92\n0.82\n0.57\n0.53\nDEPICT: Davies-Bouldin index\nPaired score\n0.88\n0.82\n-0.77\n-0.60\n-0.37\n-0.22\n0.79\n0.73\n-0.10\n0.02\n0.09\n0.15\nPooled score (w/o. Dip test)\n0.48\n0.42\n0.90\n0.78\n0.47\n0.33\n0.85\n0.73\n0.92\n0.82\n0.72\n0.62\nPooled score\n0.90\n0.73\n0.90\n0.78\n0.47\n0.33\n0.88\n0.82\n0.92\n0.82\n0.81\n0.70\nACE (w/o .Dip test)\n0.83\n0.69\n0.96\n0.91\n0.92\n0.83\n0.84\n0.69\n0.96\n0.91\n0.90\n0.81\nACE\n0.93\n0.82\n0.96\n0.91\n0.92\n0.83\n0.93\n0.87\n0.96\n0.91\n0.94\n0.87\nDEPICT: Silhouette score (cosine distance)\nPaired score\n0.87\n0.78\n-0.69\n-0.56\n-0.37\n-0.22\n0.79\n0.73\n0.07\n0.11\n0.14\n0.17\nPooled score (w/o. Dip test)\n0.85\n0.73\n0.67\n0.51\n0.68\n0.56\n0.95\n0.87\n0.98\n0.91\n0.83\n0.72\nPooled score\n0.90\n0.73\n0.67\n0.51\n0.68\n0.56\n0.90\n0.82\n0.98\n0.91\n0.83\n0.71\nACE (w/o .Dip test)\n0.64\n0.47\n0.92\n0.82\n0.80\n0.67\n0.96\n0.91\n0.99\n0.96\n0.86\n0.76\nACE\n0.95\n0.87\n0.92\n0.82\n0.80\n0.67\n0.95\n0.87\n0.99\n0.96\n0.92\n0.84\nDEPICT: Silhouette score (euclidean distance)\nPaired score\n0.87\n0.78\n-0.64\n-0.51\n-0.37\n-0.22\n0.79\n0.73\n-0.12\n-0.02\n0.11\n0.15\nPooled score (w/o. Dip test)\n0.93\n0.87\n0.99\n0.96\n0.68\n0.56\n0.96\n0.91\n0.99\n0.96\n0.91\n0.85\nPooled score\n0.90\n0.73\n0.99\n0.96\n0.68\n0.56\n0.94\n0.87\n0.99\n0.96\n0.90\n0.81\nACE (w/o .Dip test)\n0.95\n0.87\n0.98\n0.91\n0.73\n0.56\n0.95\n0.87\n0.98\n0.91\n0.92\n0.82\nACE\n0.88\n0.82\n0.98\n0.91\n0.73\n0.56\n0.95\n0.87\n0.98\n0.91\n0.90\n0.81\n\nDifferent \u03b1\nIn this section, we delve into the impact of different family-wise error rates\n(\u03b1) for edge inclusion in link analysis.\n\nIn Algorithm 1, a multiple testing procedure (the Holm\u2013Bonferroni method applied in this paper) FWER \u03b1 is employed to include edges with significant rank correlation for link analysis. In addition to the experiments using \u03b1 = 0.1, as reported in the main text, we conduct experiments with \u03b1 = 0.05, indicating a more stringent criterion for edge inclusion, as well as including all edges without edge filtering. The comparative study for the hyperparameter tuning task is presented in Tables 15 and 17, while the results for the task of determining the number of clusters are reported in Tables 16 and 18. Across most cases, we observe that *ACE* with \u03b1 = 0.1 and \u03b1 = 0.05 yields similar performance, highlighting the robustness of *ACE* to the choice of \u03b1 for edge inclusion. In the majority of cases, including all edges also produces very similar performance. However, in some instances, such as DEPICT\n(Davies-Bouldin index), including all edges can result in a significantly lower correlation. This emphasizes the effects of applying a multiple testing procedure to include only significantly rank-correlated edges for link analysis.\n\nUSPS\nYTF\nFRGC\nMNIST-test\nCMU-PIE\nUMist\nCOIL-20\nCOIL-100\nAverage\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nJULE: Calinski-Harabasz index\nPaired score\n0.17\n0.13\n0.52\n0.40\n-0.13\n-0.10\n0.49\n0.34\n-0.13\n-0.08\n0.70\n0.50\n0.53\n0.38\n0.20\n0.19\n0.29\n0.22\nACE (include all edges)\n0.80\n0.63\n0.90\n0.73\n0.39\n0.26\n0.87\n0.71\n0.98\n0.90\n0.81\n0.61\n0.60\n0.45\n0.95\n0.82\n0.79\n0.64\nACE (\u03b1 = 0.1)\n0.80\n0.63\n0.90\n0.73\n0.39\n0.26\n0.87\n0.71\n0.98\n0.90\n0.81\n0.61\n0.60\n0.45\n0.95\n0.82\n0.79\n0.64\nACE (\u03b1 = 0.05)\n0.80\n0.63\n0.90\n0.73\n0.39\n0.26\n0.87\n0.71\n0.98\n0.90\n0.81\n0.61\n0.60\n0.45\n0.95\n0.82\n0.79\n0.64\nJULE: Davies-Bouldin index\nPaired score\n-0.10\n-0.03\n-0.32\n-0.21\n-0.08\n-0.05\n-0.13\n-0.06\n0.26\n0.20\n0.62\n0.44\n0.61\n0.42\n0.43\n0.35\n0.16\n0.13\nACE (include all edges)\n-0.08\n-0.02\n-0.30\n-0.21\n0.22\n0.16\n0.73\n0.55\n0.10\n0.06\n0.36\n0.25\n0.23\n0.22\n0.54\n0.38\n0.23\n0.17\nACE (\u03b1 = 0.1)\n-0.08\n-0.02\n-0.30\n-0.21\n0.22\n0.16\n0.73\n0.55\n0.10\n0.06\n0.38\n0.27\n0.23\n0.22\n0.48\n0.33\n0.22\n0.17\nACE (\u03b1 = 0.05)\n-0.08\n-0.02\n-0.30\n-0.21\n0.22\n0.16\n0.73\n0.55\n0.10\n0.06\n0.30\n0.20\n0.23\n0.22\n0.48\n0.33\n0.21\n0.16\nJULE: Silhouette score (cosine distance)\nPaired score\n0.28\n0.22\n0.73\n0.56\n0.09\n0.06\n0.63\n0.47\n0.50\n0.36\n0.71\n0.50\n0.68\n0.50\n0.74\n0.54\n0.54\n0.40\nACE (include all edges)\n0.89\n0.73\n0.93\n0.83\n0.52\n0.35\n0.81\n0.66\n0.99\n0.93\n0.79\n0.59\n0.44\n0.38\n0.92\n0.78\n0.79\n0.66\nACE (\u03b1 = 0.1)\n0.89\n0.73\n0.93\n0.83\n0.52\n0.35\n0.81\n0.66\n0.99\n0.93\n0.79\n0.59\n0.44\n0.38\n0.92\n0.78\n0.79\n0.66\nACE (\u03b1 = 0.05)\n0.89\n0.73\n0.93\n0.83\n0.52\n0.35\n0.81\n0.66\n0.99\n0.93\n0.80\n0.59\n0.44\n0.38\n0.92\n0.78\n0.79\n0.66\nJULE: Silhouette score (euclidean distance)\nPaired score\n0.27\n0.20\n0.72\n0.55\n0.04\n0.03\n0.56\n0.41\n0.42\n0.30\n0.70\n0.50\n0.64\n0.46\n0.55\n0.41\n0.49\n0.36\nACE (include all edges)\n0.88\n0.72\n0.89\n0.75\n0.42\n0.28\n0.81\n0.65\n0.98\n0.90\n0.88\n0.70\n0.41\n0.36\n0.92\n0.78\n0.77\n0.64\nACE (\u03b1 = 0.1)\n0.88\n0.72\n0.89\n0.75\n0.42\n0.28\n0.81\n0.65\n0.98\n0.90\n0.88\n0.70\n0.41\n0.36\n0.92\n0.78\n0.77\n0.64\nACE (\u03b1 = 0.05)\n0.88\n0.72\n0.89\n0.75\n0.42\n0.28\n0.81\n0.65\n0.98\n0.90\n0.88\n0.70\n0.41\n0.36\n0.92\n0.78\n0.77\n0.64\nDEPICT: Calinski-Harabasz index\nPaired score\n0.76\n0.57\n0.44\n0.26\n0.76\n0.57\n0.89\n0.72\n0.49\n0.44\n0.67\n0.51\nACE (include all edges)\n0.91\n0.77\n0.56\n0.44\n0.94\n0.82\n0.96\n0.87\n0.96\n0.87\n0.87\n0.75\nACE (\u03b1 = 0.1)\n0.91\n0.77\n0.56\n0.44\n0.94\n0.82\n0.96\n0.87\n0.96\n0.87\n0.87\n0.75\nACE (\u03b1 = 0.05)\n0.91\n0.77\n0.56\n0.44\n0.94\n0.82\n0.96\n0.87\n0.95\n0.84\n0.87\n0.75\nDEPICT: Davies-Bouldin index\nPaired score\n0.81\n0.59\n0.45\n0.31\n0.90\n0.74\n0.89\n0.72\n0.63\n0.59\n0.73\n0.59\nACE (include all edges)\n0.91\n0.82\n0.76\n0.58\n0.89\n0.75\n0.96\n0.87\n0.98\n0.92\n0.90\n0.79\nACE (\u03b1 = 0.1)\n0.91\n0.82\n0.76\n0.58\n0.91\n0.79\n0.96\n0.87\n0.98\n0.92\n0.90\n0.80\nACE (\u03b1 = 0.05)\n0.91\n0.82\n0.76\n0.58\n0.91\n0.79\n0.96\n0.87\n0.98\n0.92\n0.90\n0.80\nDEPICT: Silhouette score (cosine distance)\nPaired score\n0.81\n0.62\n0.45\n0.33\n0.90\n0.75\n0.89\n0.72\n0.77\n0.58\n0.76\n0.60\nACE (include all edges)\n0.97\n0.90\n0.56\n0.45\n0.94\n0.82\n0.97\n0.90\n0.94\n0.83\n0.88\n0.78\nACE (\u03b1 = 0.1)\n0.97\n0.90\n0.71\n0.56\n0.94\n0.82\n0.97\n0.90\n0.94\n0.83\n0.91\n0.80\nACE (\u03b1 = 0.05)\n0.97\n0.90\n0.71\n0.56\n0.94\n0.82\n0.97\n0.90\n0.94\n0.83\n0.91\n0.80\nDEPICT: Silhouette score (euclidean distance)\nPaired score\n0.73\n0.50\n0.47\n0.36\n0.79\n0.65\n0.86\n0.69\n0.59\n0.52\n0.69\n0.54\nACE (include all edges)\n0.97\n0.88\n0.65\n0.50\n0.95\n0.83\n0.98\n0.90\n0.94\n0.82\n0.90\n0.79\nACE (\u03b1 = 0.1)\n0.97\n0.88\n0.65\n0.50\n0.95\n0.83\n0.98\n0.90\n0.94\n0.82\n0.90\n0.79\nACE (\u03b1 = 0.05)\n0.97\n0.88\n0.67\n0.52\n0.95\n0.83\n0.98\n0.90\n0.94\n0.82\n0.90\n0.79\nUSPS (10)\nYTF (41)\nFRGC (20)\nMNIST-test (10)\nCMU-PIE (68)\nUMist (20)\nCOIL-20 (20)\nCOIL-100 (100)\nAverage\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nJULE: Calinski-Harabasz index\nPaired score\n0.65 (10)\n0.64 (10)\n0.1 (50)\n0.06 (50)\n-0.93 (15)\n-0.83 (15)\n0.64 (10)\n0.6 (10)\n-0.03 (20)\n-0.02 (20)\n-0.13 (5)\n-0.07 (5)\n0.76 (15)\n0.71 (15)\n0.74 (80)\n0.56 (80)\n0.22\n0.21\nACE (include all edges)\n0.65 (10)\n0.64 (10)\n0.93 (50)\n0.83 (50)\n-0.72 (15)\n-0.67 (15)\n0.64 (10)\n0.6 (10)\n0.88 (70)\n0.73 (70)\n-0.14 (5)\n-0.11 (5)\n0.74 (15)\n0.64 (15)\n0.79 (80)\n0.69 (80)\n0.47\n0.42\nACE (\u03b1 = 0.1)\n0.65 (10)\n0.64 (10)\n0.93 (50)\n0.83 (50)\n-0.72 (15)\n-0.67 (15)\n0.64 (10)\n0.6 (10)\n0.88 (70)\n0.73 (70)\n-0.14 (5)\n-0.11 (5)\n0.74 (15)\n0.64 (15)\n0.79 (80)\n0.69 (80)\n0.47\n0.42\nACE (\u03b1 = 0.05)\n0.65 (10)\n0.64 (10)\n0.93 (50)\n0.83 (50)\n-0.72 (15)\n-0.67 (15)\n0.64 (10)\n0.6 (10)\n0.88 (70)\n0.73 (70)\n-0.14 (5)\n-0.11 (5)\n0.74 (15)\n0.64 (15)\n0.79 (80)\n0.69 (80)\n0.47\n0.42\nJULE: Davies-Bouldin index\nPaired score\n0.54 (15)\n0.38 (15)\n0.15 (50)\n0.17 (50)\n0.85 (45)\n0.67 (45)\n0.43 (10)\n0.29 (10)\n0.78 (100)\n0.56 (100)\n-0.08 (45)\n0.02 (45)\n-0.26 (40)\n-0.14 (40)\n-0.9 (20)\n-0.78 (20)\n0.19\n0.15\nACE (include all edges)\n0.98 (15)\n0.91 (15)\n0.83 (50)\n0.67 (50)\n0.83 (40)\n0.67 (40)\n0.81 (10)\n0.64 (10)\n0.85 (90)\n0.69 (90)\n-0.33 (45)\n-0.11 (45)\n-0.83 (50)\n-0.71 (50)\n-0.94 (20)\n-0.82 (20)\n0.28\n0.24\nACE (\u03b1 = 0.1)\n0.98 (15)\n0.91 (15)\n0.83 (50)\n0.67 (50)\n0.87 (40)\n0.72 (40)\n0.79 (10)\n0.6 (10)\n0.85 (90)\n0.69 (90)\n-0.21 (45)\n-0.02 (45)\n-0.69 (50)\n-0.57 (50)\n-0.94 (20)\n-0.82 (20)\n0.31\n0.27\nACE (\u03b1 = 0.05)\n0.72 (15)\n0.64 (15)\n0.92 (50)\n0.78 (50)\n0.87 (40)\n0.72 (40)\n0.79 (10)\n0.6 (10)\n0.85 (90)\n0.69 (90)\n-0.49 (50)\n-0.38 (50)\n-0.69 (50)\n-0.57 (50)\n-0.94 (20)\n-0.82 (20)\n0.25\n0.21\nJULE: Silhouette score (cosine distance)\nPaired score\n0.99 (10)\n0.96 (10)\n0.3 (50)\n0.22 (50)\n0.72 (25)\n0.61 (25)\n0.87 (10)\n0.69 (10)\n0.98 (70)\n0.91 (70)\n-0.07 (45)\n0.07 (45)\n0.52 (25)\n0.36 (25)\n0.39 (200)\n0.2 (200)\n0.59\n0.50\nACE (include all edges)\n0.95 (10)\n0.87 (10)\n0.98 (50)\n0.94 (50)\n0.68 (45)\n0.56 (45)\n0.96 (10)\n0.87 (10)\n0.98 (70)\n0.91 (70)\n-0.22 (45)\n-0.16 (45)\n0.76 (20)\n0.57 (20)\n0.46 (180)\n0.33 (180)\n0.69\n0.61\nACE (\u03b1 = 0.1)\n0.95 (10)\n0.87 (10)\n0.98 (50)\n0.94 (50)\n0.7 (45)\n0.61 (45)\n0.96 (10)\n0.87 (10)\n0.98 (70)\n0.91 (70)\n-0.07 (45)\n-0.02 (45)\n0.74 (20)\n0.5 (20)\n0.46 (180)\n0.33 (180)\n0.71\n0.63\nACE (\u03b1 = 0.05)\n0.95 (10)\n0.87 (10)\n0.98 (50)\n0.94 (50)\n0.83 (45)\n0.72 (45)\n0.96 (10)\n0.87 (10)\n0.98 (70)\n0.91 (70)\n-0.07 (45)\n-0.02 (45)\n0.74 (20)\n0.5 (20)\n0.46 (180)\n0.33 (180)\n0.73\n0.64\nJULE: Silhouette score (euclidean distance)\nPaired score\n0.85 (10)\n0.73 (10)\n0.33 (50)\n0.28 (50)\n0.72 (25)\n0.61 (25)\n0.88 (10)\n0.69 (10)\n0.96 (80)\n0.87 (80)\n0.07 (45)\n0.16 (45)\n0.55 (25)\n0.43 (25)\n0.44 (200)\n0.29 (200)\n0.60\n0.51\nACE (include all edges)\n0.95 (10)\n0.87 (10)\n0.98 (50)\n0.94 (50)\n0.78 (45)\n0.67 (45)\n0.95 (10)\n0.82 (10)\n0.98 (70)\n0.91 (70)\n0.14 (45)\n0.11 (45)\n0.76 (25)\n0.57 (25)\n0.47 (200)\n0.33 (200)\n0.75\n0.65\nACE (\u03b1 = 0.1)\n0.95 (10)\n0.87 (10)\n0.98 (50)\n0.94 (50)\n0.78 (45)\n0.67 (45)\n0.95 (10)\n0.82 (10)\n0.98 (70)\n0.91 (70)\n0.14 (45)\n0.11 (45)\n0.71 (25)\n0.43 (25)\n0.47 (200)\n0.33 (200)\n0.74\n0.64\nACE (\u03b1 = 0.05)\n0.95 (10)\n0.87 (10)\n0.98 (50)\n0.94 (50)\n0.78 (45)\n0.67 (45)\n0.95 (10)\n0.82 (10)\n0.98 (70)\n0.91 (70)\n0.14 (45)\n0.11 (45)\n0.71 (25)\n0.43 (25)\n0.47 (200)\n0.33 (200)\n0.74\n0.64\nDEPICT: Calinski-Harabasz index\nPaired score\n0.46 (5)\n0.6 (5)\n-0.99 (5)\n-0.96 (5)\n-0.85 (10)\n-0.72 (10)\n0.44 (5)\n0.56 (5)\n-0.92 (10)\n-0.82 (10)\n-0.37\n-0.27\nACE (include all edges)\n0.46 (5)\n0.6 (5)\n-0.65 (5)\n-0.47 (5)\n-0.75 (10)\n-0.56 (10)\n0.46 (5)\n0.6 (5)\n0.72 (80)\n0.69 (80)\n0.05\n0.17\nACE (\u03b1 = 0.1)\n0.46 (5)\n0.6 (5)\n-0.66 (5)\n-0.51 (5)\n0.77 (30)\n0.61 (30)\n0.46 (5)\n0.6 (5)\n0.92 (80)\n0.82 (80)\n0.39\n0.42\nACE (\u03b1 = 0.05)\n0.46 (5)\n0.6 (5)\n-0.66 (5)\n-0.51 (5)\n0.87 (35)\n0.72 (35)\n0.46 (5)\n0.6 (5)\n0.92 (80)\n0.82 (80)\n0.41\n0.45\nDEPICT: Davies-Bouldin index\nPaired score\n0.46 (5)\n0.6 (5)\n-0.78 (5)\n-0.64 (5)\n-0.85 (10)\n-0.72 (10)\n0.44 (5)\n0.56 (5)\n-0.1 (10)\n0.02 (10)\n-0.17\n-0.04\nACE (include all edges)\n0.61 (15)\n0.56 (15)\n0.96 (50)\n0.91 (50)\n0.88 (35)\n0.78 (35)\n0.87 (10)\n0.78 (10)\n0.95 (80)\n0.87 (80)\n0.85\n0.78\nACE (\u03b1 = 0.1)\n0.62 (10)\n0.6 (10)\n0.95 (50)\n0.87 (50)\n0.77 (35)\n0.67 (35)\n0.78 (10)\n0.69 (10)\n0.96 (70)\n0.91 (70)\n0.82\n0.75\nACE (\u03b1 = 0.05)\n0.62 (10)\n0.6 (10)\n0.96 (50)\n0.91 (50)\n0.77 (35)\n0.67 (35)\n0.87 (10)\n0.78 (10)\n1.0 (80)\n1.0 (80)\n0.84\n0.79\nDEPICT: Silhouette score (cosine distance)\nPaired score\n0.44 (5)\n0.56 (5)\n-0.7 (5)\n-0.6 (5)\n-0.85 (10)\n-0.72 (10)\n0.44 (5)\n0.56 (5)\n0.07 (10)\n0.11 (10)\n-0.12\n-0.02\nACE (include all edges)\n0.64 (15)\n0.6 (15)\n0.82 (40)\n0.73 (40)\n0.93 (35)\n0.83 (35)\n0.93 (10)\n0.82 (10)\n0.98 (80)\n0.91 (80)\n0.86\n0.78\nACE (\u03b1 = 0.1)\n0.65 (15)\n0.64 (15)\n0.87 (40)\n0.78 (40)\n0.93 (35)\n0.83 (35)\n0.85 (10)\n0.78 (10)\n0.99 (80)\n0.96 (80)\n0.86\n0.80\nACE (\u03b1 = 0.05)\n0.65 (15)\n0.64 (15)\n0.87 (40)\n0.78 (40)\n0.93 (35)\n0.83 (35)\n0.85 (10)\n0.78 (10)\n0.99 (80)\n0.96 (80)\n0.86\n0.80\nDEPICT: Silhouette score (euclidean distance)\nPaired score\n0.44 (5)\n0.56 (5)\n-0.61 (5)\n-0.47 (5)\n-0.85 (10)\n-0.72 (10)\n0.44 (5)\n0.56 (5)\n-0.12 (10)\n-0.02 (10)\n-0.14\n-0.02\nACE (include all edges)\n0.64 (15)\n0.6 (15)\n0.94 (40)\n0.87 (40)\n0.3 (25)\n0.22 (25)\n0.87 (10)\n0.78 (10)\n0.99 (80)\n0.96 (80)\n0.75\n0.69\nACE (\u03b1 = 0.1)\n0.46 (5)\n0.6 (5)\n0.94 (40)\n0.87 (40)\n0.02 (25)\n0.06 (25)\n0.85 (10)\n0.78 (10)\n0.98 (80)\n0.91 (80)\n0.65\n0.64\nACE (\u03b1 = 0.05)\n0.46 (5)\n0.6 (5)\n0.94 (40)\n0.87 (40)\n0.18 (25)\n0.17 (25)\n0.85 (10)\n0.78 (10)\n0.99 (80)\n0.96 (80)\n0.68\n0.68\nUSPS\nYTF\nFRGC\nMNIST-test\nCMU-PIE\nUMist\nCOIL-20\nCOIL-100\nAverage\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nJULE: Calinski-Harabasz index\nPaired score\n0.04\n0.05\n0.39\n0.27\n-0.26\n-0.18\n0.31\n0.21\n-0.20\n-0.12\n0.64\n0.45\n0.57\n0.40\n0.09\n0.08\n0.20\n0.14\nACE (include all edges)\n0.90\n0.77\n0.73\n0.54\n0.49\n0.36\n0.95\n0.82\n0.97\n0.87\n0.81\n0.61\n0.57\n0.40\n0.93\n0.81\n0.79\n0.65\nACE (\u03b1 = 0.1)\n0.90\n0.77\n0.73\n0.54\n0.49\n0.36\n0.95\n0.82\n0.97\n0.87\n0.81\n0.61\n0.57\n0.40\n0.93\n0.81\n0.79\n0.65\nACE (\u03b1 = 0.05)\n0.90\n0.77\n0.73\n0.54\n0.49\n0.36\n0.95\n0.82\n0.97\n0.87\n0.81\n0.61\n0.57\n0.40\n0.93\n0.81\n0.79\n0.65\nJULE: Davies-Bouldin index\nPaired score\n-0.27\n-0.15\n-0.14\n-0.09\n-0.23\n-0.14\n-0.35\n-0.19\n0.20\n0.16\n0.53\n0.36\n0.63\n0.44\n0.33\n0.26\n0.09\n0.08\nACE (include all edges)\n-0.30\n-0.09\n-0.07\n-0.07\n0.53\n0.38\n0.79\n0.64\n0.07\n0.03\n0.24\n0.17\n0.21\n0.18\n0.49\n0.33\n0.24\n0.20\nACE (\u03b1 = 0.1)\n-0.30\n-0.09\n-0.07\n-0.07\n0.53\n0.38\n0.79\n0.64\n0.07\n0.03\n0.27\n0.20\n0.21\n0.18\n0.44\n0.28\n0.24\n0.19\nACE (\u03b1 = 0.05)\n-0.30\n-0.09\n-0.07\n-0.07\n0.53\n0.38\n0.79\n0.64\n0.07\n0.03\n0.17\n0.11\n0.21\n0.18\n0.44\n0.28\n0.23\n0.18\nJULE: Silhouette score (cosine distance)\nPaired score\n0.17\n0.14\n0.59\n0.41\n0.07\n0.06\n0.47\n0.33\n0.45\n0.33\n0.64\n0.46\n0.70\n0.51\n0.64\n0.45\n0.47\n0.34\nACE (include all edges)\n0.96\n0.85\n0.74\n0.55\n0.82\n0.65\n0.92\n0.78\n0.98\n0.92\n0.78\n0.58\n0.41\n0.32\n0.84\n0.68\n0.81\n0.67\nACE (\u03b1 = 0.1)\n0.96\n0.85\n0.74\n0.55\n0.82\n0.65\n0.92\n0.78\n0.98\n0.92\n0.78\n0.58\n0.41\n0.32\n0.84\n0.68\n0.81\n0.67\nACE (\u03b1 = 0.05)\n0.96\n0.85\n0.74\n0.55\n0.82\n0.65\n0.92\n0.78\n0.98\n0.92\n0.78\n0.57\n0.41\n0.32\n0.84\n0.68\n0.81\n0.66\nJULE: Silhouette score (euclidean distance)\nPaired score\n0.14\n0.12\n0.54\n0.39\n-0.08\n-0.02\n0.41\n0.27\n0.36\n0.27\n0.64\n0.46\n0.67\n0.48\n0.44\n0.31\n0.39\n0.28\nACE (include all edges)\n0.93\n0.78\n0.63\n0.48\n0.71\n0.53\n0.92\n0.78\n0.98\n0.91\n0.86\n0.68\n0.39\n0.30\n0.84\n0.68\n0.78\n0.64\nACE (\u03b1 = 0.1)\n0.93\n0.78\n0.63\n0.48\n0.71\n0.53\n0.92\n0.78\n0.98\n0.91\n0.86\n0.68\n0.39\n0.30\n0.84\n0.68\n0.78\n0.64\nACE (\u03b1 = 0.05)\n0.93\n0.78\n0.63\n0.48\n0.71\n0.53\n0.92\n0.78\n0.98\n0.91\n0.86\n0.68\n0.39\n0.30\n0.84\n0.68\n0.78\n0.64\nDEPICT: Calinski-Harabasz index\nPaired score\n0.56\n0.40\n0.54\n0.35\n0.76\n0.57\n0.88\n0.69\n0.48\n0.43\n0.64\n0.49\nACE (include all edges)\n0.82\n0.72\n0.61\n0.45\n0.91\n0.82\n0.97\n0.91\n0.96\n0.87\n0.86\n0.75\nACE (\u03b1 = 0.1)\n0.82\n0.72\n0.61\n0.45\n0.91\n0.82\n0.97\n0.91\n0.96\n0.87\n0.86\n0.75\nACE (\u03b1 = 0.05)\n0.82\n0.72\n0.61\n0.45\n0.91\n0.82\n0.97\n0.91\n0.96\n0.87\n0.86\n0.75\nDEPICT: Davies-Bouldin index\nPaired score\n0.61\n0.42\n0.48\n0.32\n0.92\n0.74\n0.88\n0.69\n0.62\n0.56\n0.70\n0.55\nACE (include all edges)\n0.99\n0.96\n0.65\n0.46\n0.88\n0.72\n0.99\n0.96\n0.96\n0.87\n0.89\n0.80\nACE (\u03b1 = 0.1)\n0.99\n0.96\n0.65\n0.46\n0.90\n0.74\n0.99\n0.96\n0.96\n0.87\n0.90\n0.80\nACE (\u03b1 = 0.05)\n0.99\n0.96\n0.65\n0.46\n0.90\n0.74\n0.99\n0.96\n0.96\n0.87\n0.90\n0.80\nDEPICT: Silhouette score (cosine distance)\nPaired score\n0.62\n0.45\n0.53\n0.42\n0.91\n0.75\n0.88\n0.69\n0.77\n0.58\n0.74\n0.58\nACE (include all edges)\n0.95\n0.88\n0.60\n0.44\n0.91\n0.77\n0.96\n0.88\n0.94\n0.83\n0.87\n0.76\nACE (\u03b1 = 0.1)\n0.95\n0.88\n0.70\n0.54\n0.91\n0.77\n0.96\n0.88\n0.94\n0.83\n0.89\n0.78\nACE (\u03b1 = 0.05)\n0.95\n0.88\n0.70\n0.54\n0.91\n0.77\n0.96\n0.88\n0.94\n0.83\n0.89\n0.78\nDEPICT: Silhouette score (euclidean distance)\nPaired score\n0.52\n0.33\n0.57\n0.45\n0.80\n0.62\n0.85\n0.65\n0.59\n0.48\n0.67\n0.51\nACE (include all edges)\n0.95\n0.87\n0.63\n0.49\n0.91\n0.78\n0.97\n0.91\n0.95\n0.84\n0.88\n0.78\nACE (\u03b1 = 0.1)\n0.95\n0.87\n0.63\n0.49\n0.91\n0.78\n0.97\n0.91\n0.95\n0.84\n0.88\n0.78\nACE (\u03b1 = 0.05)\n0.95\n0.87\n0.64\n0.50\n0.91\n0.78\n0.97\n0.91\n0.95\n0.84\n0.88\n0.78\nUSPS (10)\nYTF (41)\nFRGC (20)\nMNIST-test (10)\nCMU-PIE (68)\nUMist (20)\nCOIL-20 (20)\nCOIL-100 (100)\nAverage\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nJULE: Calinski-Harabasz index\nPaired score\n0.84\n0.73\n0.03\n-0.06\n-0.49\n-0.31\n0.61\n0.56\n-0.09\n-0.07\n-0.04\n0.07\n0.74\n0.64\n0.60\n0.51\n0.27\n0.26\nACE (include all edges)\n0.84\n0.73\n0.92\n0.83\n-0.11\n-0.03\n0.61\n0.56\n0.83\n0.69\n-0.07\n0.02\n0.76\n0.71\n0.65\n0.56\n0.55\n0.51\nACE (\u03b1 = 0.1)\n0.84\n0.73\n0.92\n0.83\n-0.11\n-0.03\n0.61\n0.56\n0.83\n0.69\n-0.07\n0.02\n0.76\n0.71\n0.65\n0.56\n0.55\n0.51\nACE (\u03b1 = 0.05)\n0.84\n0.73\n0.92\n0.83\n-0.11\n-0.03\n0.61\n0.56\n0.83\n0.69\n-0.07\n0.02\n0.76\n0.71\n0.65\n0.56\n0.55\n0.51\nJULE: Davies-Bouldin index\nPaired score\n0.39\n0.29\n0.10\n0.06\n0.37\n0.25\n0.49\n0.33\n0.83\n0.60\n-0.28\n-0.29\n-0.29\n-0.21\n-0.87\n-0.73\n0.09\n0.04\nACE (include all edges)\n0.89\n0.73\n0.80\n0.67\n0.63\n0.48\n0.84\n0.69\n0.88\n0.73\n-0.58\n-0.42\n-0.86\n-0.79\n-0.82\n-0.69\n0.22\n0.18\nACE (\u03b1 = 0.1)\n0.89\n0.73\n0.80\n0.67\n0.60\n0.42\n0.83\n0.64\n0.88\n0.73\n-0.42\n-0.33\n-0.71\n-0.64\n-0.82\n-0.69\n0.26\n0.19\nACE (\u03b1 = 0.05)\n0.89\n0.73\n0.90\n0.78\n0.60\n0.42\n0.83\n0.64\n0.88\n0.73\n-0.83\n-0.69\n-0.71\n-0.64\n-0.82\n-0.69\n0.22\n0.16\nJULE: Silhouette score (cosine distance)\nPaired score\n0.89\n0.78\n0.27\n0.22\n0.21\n0.09\n0.81\n0.64\n0.99\n0.96\n-0.26\n-0.24\n0.55\n0.43\n0.52\n0.33\n0.50\n0.40\nACE (include all edges)\n0.95\n0.87\n0.98\n0.94\n0.61\n0.48\n0.94\n0.82\n0.99\n0.96\n-0.60\n-0.38\n0.79\n0.64\n0.60\n0.47\n0.66\n0.60\nACE (\u03b1 = 0.1)\n0.95\n0.87\n0.98\n0.94\n0.64\n0.54\n0.94\n0.82\n0.99\n0.96\n-0.32\n-0.24\n0.76\n0.57\n0.60\n0.47\n0.69\n0.61\nACE (\u03b1 = 0.05)\n0.95\n0.87\n0.98\n0.94\n0.54\n0.42\n0.94\n0.82\n0.99\n0.96\n-0.32\n-0.24\n0.76\n0.57\n0.60\n0.47\n0.68\n0.60\nJULE: Silhouette score (euclidean distance)\nPaired score\n0.93\n0.82\n0.30\n0.28\n0.21\n0.09\n0.82\n0.64\n0.98\n0.91\n-0.13\n-0.16\n0.52\n0.36\n0.55\n0.42\n0.52\n0.42\nACE (include all edges)\n0.95\n0.87\n0.98\n0.94\n0.57\n0.48\n0.92\n0.78\n0.99\n0.96\n-0.03\n-0.11\n0.74\n0.50\n0.59\n0.47\n0.71\n0.61\nACE (\u03b1 = 0.1)\n0.95\n0.87\n0.98\n0.94\n0.57\n0.48\n0.92\n0.78\n0.99\n0.96\n-0.03\n-0.11\n0.74\n0.50\n0.59\n0.47\n0.71\n0.61\nACE (\u03b1 = 0.05)\n0.95\n0.87\n0.98\n0.94\n0.57\n0.48\n0.92\n0.78\n0.99\n0.96\n-0.03\n-0.11\n0.74\n0.50\n0.59\n0.47\n0.71\n0.61\nDEPICT: Calinski-Harabasz index\nPaired score\n0.88\n0.82\n-0.96\n-0.91\n-0.37\n-0.22\n0.79\n0.73\n-0.92\n-0.82\n-0.11\n-0.08\nACE (include all edges)\n0.88\n0.82\n-0.66\n-0.51\n-0.13\n-0.06\n0.82\n0.78\n0.72\n0.69\n0.32\n0.34\nACE (\u03b1 = 0.1)\n0.88\n0.82\n-0.67\n-0.56\n0.92\n0.78\n0.82\n0.78\n0.92\n0.82\n0.57\n0.53\nACE (\u03b1 = 0.05)\n0.88\n0.82\n-0.67\n-0.56\n0.83\n0.67\n0.82\n0.78\n0.92\n0.82\n0.55\n0.51\nDEPICT: Davies-Bouldin index\nPaired score\n0.88\n0.82\n-0.77\n-0.60\n-0.37\n-0.22\n0.79\n0.73\n-0.10\n0.02\n0.09\n0.15\nACE (include all edges)\n0.92\n0.78\n0.99\n0.96\n0.87\n0.72\n0.89\n0.78\n0.95\n0.87\n0.92\n0.82\nACE (\u03b1 = 0.1)\n0.93\n0.82\n0.96\n0.91\n0.92\n0.83\n0.93\n0.87\n0.96\n0.91\n0.94\n0.87\nACE (\u03b1 = 0.05)\n0.93\n0.82\n0.99\n0.96\n0.92\n0.83\n0.89\n0.78\n1.00\n1.00\n0.94\n0.88\nDEPICT: Silhouette score (cosine distance)\nPaired score\n0.87\n0.78\n-0.69\n-0.56\n-0.37\n-0.22\n0.79\n0.73\n0.07\n0.11\n0.14\n0.17\nACE (include all edges)\n0.94\n0.82\n0.87\n0.78\n0.80\n0.67\n0.90\n0.82\n0.98\n0.91\n0.90\n0.80\nACE (\u03b1 = 0.1)\n0.95\n0.87\n0.92\n0.82\n0.80\n0.67\n0.95\n0.87\n0.99\n0.96\n0.92\n0.84\nACE (\u03b1 = 0.05)\n0.95\n0.87\n0.92\n0.82\n0.80\n0.67\n0.95\n0.87\n0.99\n0.96\n0.92\n0.84\nDEPICT: Silhouette score (euclidean distance)\nPaired score\n0.87\n0.78\n-0.64\n-0.51\n-0.37\n-0.22\n0.79\n0.73\n-0.12\n-0.02\n0.11\n0.15\nACE (include all edges)\n0.94\n0.82\n0.98\n0.91\n0.88\n0.72\n0.89\n0.78\n0.99\n0.96\n0.94\n0.84\nACE (\u03b1 = 0.1)\n0.88\n0.82\n0.98\n0.91\n0.73\n0.56\n0.95\n0.87\n0.98\n0.91\n0.90\n0.81\nACE (\u03b1 = 0.05)\n0.88\n0.82\n0.98\n0.91\n0.83\n0.67\n0.95\n0.87\n0.99\n0.96\n0.93\n0.84\n\nHDBSCAN vs. DBSCAN\nIn Algorithm 1, we employ a density-based clustering approach to group embedding spaces based on their rank correlation. Density-based methods are advantageous as they do not necessitate prior knowledge of the number of groups and can identify outlier spaces with low rank correlation. In the main text, we present the results using HDBSCAN, a densitybased clustering algorithm that requires minimal parameter tuning compared to alternatives like DBSCAN. In this section, we extend our exploration by conducting additional experiments with DBSCAN. Specifically, we vary the critical parameter eps, which plays a pivotal role in DBSCAN, setting it to 0.1 and 0.2 respectively. These results are compared with HDBSCAN, as reported in the main text. Tables 19 and 20 showcase the evaluation performance for the hyperparameter tuning task, while Tables 21 and 22 present the results for determining the number of clusters. Our observations reveal that, in certain cases (e.g., *JULE* for hyperparameter tuning), DBSCAN\ncan even yield higher correlations with NMI and ACC. Conversely, in other scenarios, HDBSCAN outperforms (e.g., *JULE* for determining the number of clusters). Considering the advantage of not needing to fine-tune the parameter eps, we opt for HDBSCAN as the grouping method and report its performance in the main text.\n\nUSPS\nYTF\nFRGC\nMNIST-test\nCMU-PIE\nUMist\nCOIL-20\nCOIL-100\nAverage\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nJULE: Calinski-Harabasz index\nPaired score\n0.17\n0.13\n0.52\n0.40\n-0.13\n-0.10\n0.49\n0.34\n-0.13\n-0.08\n0.70\n0.50\n0.53\n0.38\n0.20\n0.19\n0.29\n0.22\nACE (DBSCANeps=0.1)\n0.74\n0.59\n0.88\n0.70\n0.37\n0.25\n0.87\n0.71\n0.96\n0.85\n0.88\n0.68\n0.93\n0.78\n0.95\n0.82\n0.82\n0.67\nACE (DBSCANeps=0.2)\n0.74\n0.59\n0.71\n0.54\n0.08\n0.04\n0.87\n0.71\n0.96\n0.85\n0.87\n0.68\n0.92\n0.76\n0.94\n0.80\n0.76\n0.62\nACE (*HDBSCAN*)\n0.80\n0.63\n0.90\n0.73\n0.39\n0.26\n0.87\n0.71\n0.98\n0.90\n0.81\n0.61\n0.60\n0.45\n0.95\n0.82\n0.79\n0.64\nJULE: Davies-Bouldin index\nPaired score\n-0.10\n-0.03\n-0.32\n-0.21\n-0.08\n-0.05\n-0.13\n-0.06\n0.26\n0.20\n0.62\n0.44\n0.61\n0.42\n0.43\n0.35\n0.16\n0.13\nACE (DBSCANeps=0.1)\n-0.14\n-0.07\n-0.57\n-0.40\n0.48\n0.32\n0.73\n0.55\n0.96\n0.87\n0.59\n0.41\n0.29\n0.26\n-0.48\n-0.34\n0.23\n0.20\nACE (DBSCANeps=0.2)\n0.01\n0.05\n-0.54\n-0.39\n0.22\n0.16\n0.73\n0.55\n0.96\n0.87\n0.59\n0.41\n0.26\n0.25\n-0.41\n-0.29\n0.23\n0.20\nACE (*HDBSCAN*)\n-0.08\n-0.02\n-0.30\n-0.21\n0.22\n0.16\n0.73\n0.55\n0.10\n0.06\n0.38\n0.27\n0.23\n0.22\n0.48\n0.33\n0.22\n0.17\nJULE: Silhouette score (cosine distance)\nPaired score\n0.28\n0.22\n0.73\n0.56\n0.09\n0.06\n0.63\n0.47\n0.50\n0.36\n0.71\n0.50\n0.68\n0.50\n0.74\n0.54\n0.54\n0.40\nACE (DBSCANeps=0.1)\n0.89\n0.73\n0.92\n0.80\n0.58\n0.40\n0.81\n0.66\n0.57\n0.49\n0.87\n0.70\n0.92\n0.78\n0.92\n0.78\n0.81\n0.67\nACE (DBSCANeps=0.2)\n0.89\n0.73\n0.92\n0.80\n0.52\n0.35\n0.81\n0.66\n0.97\n0.90\n0.88\n0.70\n0.44\n0.38\n0.92\n0.78\n0.79\n0.66\nACE (*HDBSCAN*)\n0.89\n0.73\n0.93\n0.83\n0.52\n0.35\n0.81\n0.66\n0.99\n0.93\n0.79\n0.59\n0.44\n0.38\n0.92\n0.78\n0.79\n0.66\nJULE: Silhouette score (euclidean distance)\nPaired score\n0.27\n0.20\n0.72\n0.55\n0.04\n0.03\n0.56\n0.41\n0.42\n0.30\n0.70\n0.50\n0.64\n0.46\n0.55\n0.41\n0.49\n0.36\nACE (DBSCANeps=0.1)\n0.88\n0.72\n0.90\n0.77\n0.58\n0.41\n0.81\n0.65\n0.99\n0.93\n0.88\n0.70\n0.92\n0.77\n0.91\n0.78\n0.86\n0.71\nACE (DBSCANeps=0.2)\n0.88\n0.72\n0.90\n0.77\n0.53\n0.36\n0.81\n0.65\n0.99\n0.93\n0.89\n0.70\n0.41\n0.36\n0.92\n0.78\n0.79\n0.66\nACE (*HDBSCAN*)\n0.88\n0.72\n0.89\n0.75\n0.42\n0.28\n0.81\n0.65\n0.98\n0.90\n0.88\n0.70\n0.41\n0.36\n0.92\n0.78\n0.77\n0.64\nDEPICT: Calinski-Harabasz index\nPaired score\n0.76\n0.57\n0.44\n0.26\n0.76\n0.57\n0.89\n0.72\n0.49\n0.44\n0.67\n0.51\nACE (DBSCANeps=0.1)\n0.91\n0.77\n0.58\n0.44\n0.94\n0.82\n0.96\n0.87\n0.97\n0.90\n0.87\n0.76\nACE (DBSCANeps=0.2)\n0.91\n0.77\n0.67\n0.54\n0.91\n0.79\n0.96\n0.87\n0.96\n0.87\n0.88\n0.77\nACE (*HDBSCAN*)\n0.91\n0.77\n0.56\n0.44\n0.94\n0.82\n0.96\n0.87\n0.96\n0.87\n0.87\n0.75\nDEPICT: Davies-Bouldin index\nPaired score\n0.81\n0.59\n0.45\n0.31\n0.90\n0.74\n0.89\n0.72\n0.63\n0.59\n0.73\n0.59\nACE (DBSCANeps=0.1)\n0.90\n0.79\n0.57\n0.42\n0.92\n0.80\n0.95\n0.83\n0.99\n0.95\n0.87\n0.76\nACE (DBSCANeps=0.2)\n0.95\n0.86\n0.54\n0.39\n0.91\n0.79\n0.95\n0.83\n0.98\n0.92\n0.87\n0.76\nACE (*HDBSCAN*)\n0.91\n0.82\n0.76\n0.58\n0.91\n0.79\n0.96\n0.87\n0.98\n0.92\n0.90\n0.80\nDEPICT: Silhouette score (cosine distance)\nPaired score\n0.81\n0.62\n0.45\n0.33\n0.90\n0.75\n0.89\n0.72\n0.77\n0.58\n0.76\n0.60\nACE (DBSCANeps=0.1)\n0.97\n0.90\n0.59\n0.48\n0.95\n0.83\n0.98\n0.91\n0.94\n0.84\n0.89\n0.79\nACE (DBSCANeps=0.2)\n0.96\n0.87\n0.62\n0.49\n0.95\n0.83\n0.98\n0.91\n0.94\n0.83\n0.89\n0.79\nACE (*HDBSCAN*)\n0.97\n0.90\n0.71\n0.56\n0.94\n0.82\n0.97\n0.90\n0.94\n0.83\n0.91\n0.80\nDEPICT: Silhouette score (euclidean distance)\nPaired score\n0.73\n0.50\n0.47\n0.36\n0.79\n0.65\n0.86\n0.69\n0.59\n0.52\n0.69\n0.54\nACE (DBSCANeps=0.1)\n0.97\n0.88\n0.58\n0.45\n0.95\n0.83\n0.98\n0.90\n0.94\n0.82\n0.88\n0.78\nACE (DBSCANeps=0.2)\n0.97\n0.88\n0.62\n0.48\n0.95\n0.84\n0.98\n0.90\n0.94\n0.82\n0.89\n0.78\nACE (*HDBSCAN*)\n0.97\n0.88\n0.65\n0.50\n0.95\n0.83\n0.98\n0.90\n0.94\n0.82\n0.90\n0.79\nUSPS\nYTF\nFRGC\nMNIST-test\nCMU-PIE\nUMist\nCOIL-20\nCOIL-100\nAverage\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nJULE: Calinski-Harabasz index\nPaired score\n0.04\n0.05\n0.39\n0.27\n-0.26\n-0.18\n0.31\n0.21\n-0.20\n-0.12\n0.64\n0.45\n0.57\n0.40\n0.09\n0.08\n0.20\n0.14\nACE (DBSCANeps=0.1)\n0.71\n0.58\n0.74\n0.55\n0.49\n0.38\n0.95\n0.82\n0.94\n0.82\n0.88\n0.69\n0.90\n0.74\n0.93\n0.81\n0.82\n0.67\nACE (DBSCANeps=0.2)\n0.71\n0.58\n0.61\n0.46\n0.13\n0.09\n0.95\n0.82\n0.94\n0.82\n0.87\n0.69\n0.89\n0.72\n0.92\n0.79\n0.75\n0.62\nACE (*HDBSCAN*)\n0.90\n0.77\n0.73\n0.54\n0.49\n0.36\n0.95\n0.82\n0.97\n0.87\n0.81\n0.61\n0.57\n0.40\n0.93\n0.81\n0.79\n0.65\nJULE: Davies-Bouldin index\nPaired score\n-0.27\n-0.15\n-0.14\n-0.09\n-0.23\n-0.14\n-0.35\n-0.19\n0.20\n0.16\n0.53\n0.36\n0.63\n0.44\n0.33\n0.26\n0.09\n0.08\nACE (DBSCANeps=0.1)\n-0.36\n-0.14\n-0.43\n-0.30\n0.83\n0.64\n0.79\n0.64\n0.95\n0.85\n0.50\n0.36\n0.27\n0.23\n-0.46\n-0.32\n0.26\n0.24\nACE (DBSCANeps=0.2)\n-0.28\n-0.12\n-0.42\n-0.29\n0.53\n0.38\n0.79\n0.64\n0.95\n0.85\n0.50\n0.36\n0.23\n0.21\n-0.42\n-0.29\n0.23\n0.22\nACE (*HDBSCAN*)\n-0.30\n-0.09\n-0.07\n-0.07\n0.53\n0.38\n0.79\n0.64\n0.07\n0.03\n0.27\n0.20\n0.21\n0.18\n0.44\n0.28\n0.24\n0.19\nJULE: Silhouette score (cosine distance)\nPaired score\n0.17\n0.14\n0.59\n0.41\n0.07\n0.06\n0.47\n0.33\n0.45\n0.33\n0.64\n0.46\n0.70\n0.51\n0.64\n0.45\n0.47\n0.34\nACE (DBSCANeps=0.1)\n0.96\n0.85\n0.73\n0.55\n0.88\n0.69\n0.92\n0.78\n0.58\n0.52\n0.85\n0.67\n0.90\n0.72\n0.85\n0.68\n0.83\n0.68\nACE (DBSCANeps=0.2)\n0.96\n0.85\n0.73\n0.55\n0.82\n0.65\n0.92\n0.78\n0.98\n0.90\n0.87\n0.68\n0.41\n0.32\n0.84\n0.68\n0.82\n0.67\nACE (*HDBSCAN*)\n0.96\n0.85\n0.74\n0.55\n0.82\n0.65\n0.92\n0.78\n0.98\n0.92\n0.78\n0.58\n0.41\n0.32\n0.84\n0.68\n0.81\n0.67\nJULE: Silhouette score (euclidean distance)\nPaired score\n0.14\n0.12\n0.54\n0.39\n-0.08\n-0.02\n0.41\n0.27\n0.36\n0.27\n0.64\n0.46\n0.67\n0.48\n0.44\n0.31\n0.39\n0.28\nACE (DBSCANeps=0.1)\n0.93\n0.78\n0.66\n0.49\n0.88\n0.68\n0.92\n0.78\n0.99\n0.93\n0.86\n0.68\n0.89\n0.71\n0.82\n0.67\n0.87\n0.71\nACE (DBSCANeps=0.2)\n0.93\n0.78\n0.66\n0.49\n0.82\n0.63\n0.92\n0.78\n0.99\n0.93\n0.88\n0.69\n0.39\n0.30\n0.84\n0.68\n0.80\n0.66\nACE (*HDBSCAN*)\n0.93\n0.78\n0.63\n0.48\n0.71\n0.53\n0.92\n0.78\n0.98\n0.91\n0.86\n0.68\n0.39\n0.30\n0.84\n0.68\n0.78\n0.64\nDEPICT: Calinski-Harabasz index\nPaired score\n0.56\n0.40\n0.54\n0.35\n0.76\n0.57\n0.88\n0.69\n0.48\n0.43\n0.64\n0.49\nACE (DBSCANeps=0.1)\n0.82\n0.72\n0.53\n0.40\n0.91\n0.82\n0.95\n0.86\n0.98\n0.92\n0.84\n0.74\nACE (DBSCANeps=0.2)\n0.82\n0.72\n0.59\n0.45\n0.93\n0.82\n0.95\n0.86\n0.96\n0.87\n0.85\n0.74\nACE (*HDBSCAN*)\n0.82\n0.72\n0.61\n0.45\n0.91\n0.82\n0.97\n0.91\n0.96\n0.87\n0.86\n0.75\nDEPICT: Davies-Bouldin index\nPaired score\n0.61\n0.42\n0.48\n0.32\n0.92\n0.74\n0.88\n0.69\n0.62\n0.56\n0.70\n0.55\nACE (DBSCANeps=0.1)\n0.99\n0.96\n0.50\n0.39\n0.90\n0.75\n0.99\n0.92\n0.97\n0.89\n0.87\n0.78\nACE (DBSCANeps=0.2)\n0.96\n0.87\n0.44\n0.32\n0.91\n0.77\n0.99\n0.92\n0.96\n0.87\n0.85\n0.75\nACE (*HDBSCAN*)\n0.99\n0.96\n0.65\n0.46\n0.90\n0.74\n0.99\n0.96\n0.96\n0.87\n0.90\n0.80\nDEPICT: Silhouette score (cosine distance)\nPaired score\n0.62\n0.45\n0.53\n0.42\n0.91\n0.75\n0.88\n0.69\n0.77\n0.58\n0.74\n0.58\nACE (DBSCANeps=0.1)\n0.95\n0.88\n0.55\n0.44\n0.94\n0.80\n0.96\n0.90\n0.95\n0.84\n0.87\n0.77\nACE (DBSCANeps=0.2)\n0.96\n0.88\n0.59\n0.45\n0.94\n0.80\n0.96\n0.90\n0.94\n0.83\n0.88\n0.77\nACE (*HDBSCAN*)\n0.95\n0.88\n0.70\n0.54\n0.91\n0.77\n0.96\n0.88\n0.94\n0.83\n0.89\n0.78\nDEPICT: Silhouette score (euclidean distance)\nPaired score\n0.52\n0.33\n0.57\n0.45\n0.80\n0.62\n0.85\n0.65\n0.59\n0.48\n0.67\n0.51\nACE (DBSCANeps=0.1)\n0.95\n0.87\n0.56\n0.44\n0.94\n0.80\n0.97\n0.91\n0.95\n0.84\n0.87\n0.77\nACE (DBSCANeps=0.2)\n0.95\n0.87\n0.60\n0.46\n0.94\n0.82\n0.97\n0.91\n0.95\n0.84\n0.88\n0.78\nACE (*HDBSCAN*)\n0.95\n0.87\n0.63\n0.49\n0.91\n0.78\n0.97\n0.91\n0.95\n0.84\n0.88\n0.78\nUSPS (10)\nYTF (41)\nFRGC (20)\nMNIST-test (10)\nCMU-PIE (68)\nUMist (20)\nCOIL-20 (20)\nCOIL-100 (100)\nAverage\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nJULE: Calinski-Harabasz index\nPaired score\n0.65 (10)\n0.64 (10)\n0.1 (50)\n0.06 (50)\n-0.93 (15)\n-0.83 (15)\n0.64 (10)\n0.6 (10)\n-0.03 (20)\n-0.02 (20)\n-0.13 (5)\n-0.07 (5)\n0.76 (15)\n0.71 (15)\n0.74 (80)\n0.56 (80)\n0.22\n0.21\nACE (DBSCANeps=0.1)\n0.65 (10)\n0.64 (10)\n0.93 (50)\n0.83 (50)\n-0.87 (15)\n-0.72 (15)\n0.64 (10)\n0.6 (10)\n0.88 (70)\n0.73 (70)\n-0.13 (5)\n-0.07 (5)\n0.74 (15)\n0.64 (15)\n0.72 (80)\n0.64 (80)\n0.45\n0.41\nACE (DBSCANeps=0.2)\n0.65 (10)\n0.64 (10)\n0.3 (20)\n0.17 (20)\n-0.87 (15)\n-0.72 (15)\n0.64 (10)\n0.6 (10)\n0.88 (70)\n0.73 (70)\n-0.14 (5)\n-0.11 (5)\n0.74 (15)\n0.64 (15)\n0.72 (80)\n0.64 (80)\n0.36\n0.32\nACE (*HDBSCAN*)\n0.65 (10)\n0.64 (10)\n0.93 (50)\n0.83 (50)\n-0.72 (15)\n-0.67 (15)\n0.64 (10)\n0.6 (10)\n0.88 (70)\n0.73 (70)\n-0.14 (5)\n-0.11 (5)\n0.74 (15)\n0.64 (15)\n0.79 (80)\n0.69 (80)\n0.47\n0.42\nJULE: Davies-Bouldin index\nPaired score\n0.54 (15)\n0.38 (15)\n0.15 (50)\n0.17 (50)\n0.85 (45)\n0.67 (45)\n0.43 (10)\n0.29 (10)\n0.78 (100)\n0.56 (100)\n-0.08 (45)\n0.02 (45)\n-0.26 (40)\n-0.14 (40)\n-0.9 (20)\n-0.78 (20)\n0.19\n0.15\nACE (DBSCANeps=0.1)\n0.73 (10)\n0.69 (10)\n0.92 (50)\n0.78 (50)\n0.87 (40)\n0.72 (40)\n0.65 (25)\n0.51 (25)\n0.85 (90)\n0.69 (90)\n-0.6 (5)\n-0.47 (5)\n-0.67 (50)\n-0.5 (50)\n-0.95 (20)\n-0.87 (20)\n0.22\n0.19\nACE (DBSCANeps=0.2)\n0.73 (10)\n0.69 (10)\n0.32 (20)\n0.17 (20)\n0.87 (40)\n0.72 (40)\n0.65 (25)\n0.51 (25)\n0.82 (90)\n0.64 (90)\n-0.49 (50)\n-0.38 (50)\n-0.67 (50)\n-0.5 (50)\n-0.94 (20)\n-0.82 (20)\n0.16\n0.13\nACE (*HDBSCAN*)\n0.98 (15)\n0.91 (15)\n0.83 (50)\n0.67 (50)\n0.87 (40)\n0.72 (40)\n0.79 (10)\n0.6 (10)\n0.85 (90)\n0.69 (90)\n-0.21 (45)\n-0.02 (45)\n-0.69 (50)\n-0.57 (50)\n-0.94 (20)\n-0.82 (20)\n0.31\n0.27\nJULE: Silhouette score (cosine distance)\nPaired score\n0.99 (10)\n0.96 (10)\n0.3 (50)\n0.22 (50)\n0.72 (25)\n0.61 (25)\n0.87 (10)\n0.69 (10)\n0.98 (70)\n0.91 (70)\n-0.07 (45)\n0.07 (45)\n0.52 (25)\n0.36 (25)\n0.39 (200)\n0.2 (200)\n0.59\n0.50\nACE (DBSCANeps=0.1)\n0.92 (10)\n0.82 (10)\n0.98 (50)\n0.94 (50)\n0.88 (45)\n0.78 (45)\n0.98 (10)\n0.91 (10)\n0.98 (70)\n0.91 (70)\n-0.48 (5)\n-0.38 (5)\n0.69 (20)\n0.43 (20)\n0.46 (180)\n0.33 (180)\n0.68\n0.59\nACE (DBSCANeps=0.2)\n0.92 (10)\n0.82 (10)\n0.78 (50)\n0.67 (50)\n0.7 (45)\n0.61 (45)\n0.96 (10)\n0.87 (10)\n0.98 (70)\n0.91 (70)\n-0.48 (5)\n-0.38 (5)\n0.69 (20)\n0.43 (20)\n0.46 (180)\n0.33 (180)\n0.63\n0.53\nACE (*HDBSCAN*)\n0.95 (10)\n0.87 (10)\n0.98 (50)\n0.94 (50)\n0.7 (45)\n0.61 (45)\n0.96 (10)\n0.87 (10)\n0.98 (70)\n0.91 (70)\n-0.07 (45)\n-0.02 (45)\n0.74 (20)\n0.5 (20)\n0.46 (180)\n0.33 (180)\n0.71\n0.63\nJULE: Silhouette score (euclidean distance)\nPaired score\n0.85 (10)\n0.73 (10)\n0.33 (50)\n0.28 (50)\n0.72 (25)\n0.61 (25)\n0.88 (10)\n0.69 (10)\n0.96 (80)\n0.87 (80)\n0.07 (45)\n0.16 (45)\n0.55 (25)\n0.43 (25)\n0.44 (200)\n0.29 (200)\n0.60\n0.51\nACE (DBSCANeps=0.1)\n0.79 (10)\n0.73 (10)\n0.98 (50)\n0.94 (50)\n0.83 (45)\n0.72 (45)\n0.92 (10)\n0.82 (10)\n0.98 (70)\n0.91 (70)\n-0.69 (5)\n-0.51 (5)\n0.71 (25)\n0.43 (25)\n0.47 (200)\n0.33 (200)\n0.62\n0.55\nACE (DBSCANeps=0.2)\n0.79 (10)\n0.73 (10)\n0.98 (50)\n0.94 (50)\n0.65 (45)\n0.56 (45)\n0.92 (10)\n0.82 (10)\n0.98 (70)\n0.91 (70)\n-0.69 (5)\n-0.51 (5)\n0.71 (25)\n0.43 (25)\n0.47 (200)\n0.33 (200)\n0.60\n0.53\nACE (*HDBSCAN*)\n0.95 (10)\n0.87 (10)\n0.98 (50)\n0.94 (50)\n0.78 (45)\n0.67 (45)\n0.95 (10)\n0.82 (10)\n0.98 (70)\n0.91 (70)\n0.14 (45)\n0.11 (45)\n0.71 (25)\n0.43 (25)\n0.47 (200)\n0.33 (200)\n0.74\n0.64\nPaired score\n0.46 (5)\n0.6 (5)\n-0.99 (5)\n-0.96 (5)\n-0.85 (10)\n-0.72 (10)\n0.44 (5)\n0.56 (5)\n-0.92 (10)\n-0.82 (10)\n-0.37\n-0.27\nACE (DBSCANeps=0.1)\n0.46 (5)\n0.6 (5)\n0.88 (35)\n0.73 (35)\n0.97 (35)\n0.89 (35)\n0.95 (10)\n0.87 (10)\n0.95 (80)\n0.87 (80)\n0.84\n0.79\nACE (DBSCANeps=0.2)\n0.46 (5)\n0.6 (5)\n0.84 (40)\n0.69 (40)\n0.22 (20)\n0.11 (20)\n0.95 (10)\n0.87 (10)\n0.95 (80)\n0.87 (80)\n0.68\n0.63\nACE (*HDBSCAN*)\n0.46 (5)\n0.6 (5)\n-0.66 (5)\n-0.51 (5)\n0.77 (30)\n0.61 (30)\n0.46 (5)\n0.6 (5)\n0.92 (80)\n0.82 (80)\n0.39\n0.42\nDEPICT: Davies-Bouldin index\nPaired score\n0.46 (5)\n0.6 (5)\n-0.78 (5)\n-0.64 (5)\n-0.85 (10)\n-0.72 (10)\n0.44 (5)\n0.56 (5)\n-0.1 (10)\n0.02 (10)\n-0.17\n-0.04\nACE (DBSCANeps=0.1)\n0.62 (10)\n0.6 (10)\n0.99 (50)\n0.96 (50)\n0.68 (35)\n0.61 (35)\n0.9 (15)\n0.73 (15)\n0.87 (70)\n0.78 (70)\n0.81\n0.74\nACE (DBSCANeps=0.2)\n0.62 (10)\n0.6 (10)\n0.95 (50)\n0.87 (50)\n0.68 (35)\n0.61 (35)\n0.93 (10)\n0.82 (10)\n0.64 (50)\n0.47 (50)\n0.76\n0.67\nACE (*HDBSCAN*)\n0.62 (10)\n0.6 (10)\n0.95 (50)\n0.87 (50)\n0.77 (35)\n0.67 (35)\n0.78 (10)\n0.69 (10)\n0.96 (70)\n0.91 (70)\n0.82\n0.75\nDEPICT: Silhouette score (cosine distance)\nPaired score\n0.44 (5)\n0.56 (5)\n-0.7 (5)\n-0.6 (5)\n-0.85 (10)\n-0.72 (10)\n0.44 (5)\n0.56 (5)\n0.07 (10)\n0.11 (10)\n-0.12\n-0.02\nACE (DBSCANeps=0.1)\n0.46 (5)\n0.6 (5)\n0.99 (50)\n0.96 (50)\n0.83 (35)\n0.72 (35)\n0.85 (10)\n0.78 (10)\n1.0 (80)\n1.0 (80)\n0.83\n0.81\nACE (DBSCANeps=0.2)\n0.46 (5)\n0.6 (5)\n0.87 (40)\n0.78 (40)\n0.93 (35)\n0.83 (35)\n0.73 (10)\n0.69 (10)\n0.72 (50)\n0.56 (50)\n0.74\n0.69\nACE (*HDBSCAN*)\n0.65 (15)\n0.64 (15)\n0.87 (40)\n0.78 (40)\n0.93 (35)\n0.83 (35)\n0.85 (10)\n0.78 (10)\n0.99 (80)\n0.96 (80)\n0.86\n0.80\nPaired score\n0.44 (5)\n0.56 (5)\n-0.61 (5)\n-0.47 (5)\n-0.85 (10)\n-0.72 (10)\n0.44 (5)\n0.56 (5)\n-0.12 (10)\n-0.02 (10)\n-0.14\n-0.02\nACE (DBSCANeps=0.1)\n0.46 (5)\n0.6 (5)\n0.94 (40)\n0.87 (40)\n0.77 (35)\n0.67 (35)\n0.73 (10)\n0.69 (10)\n0.98 (80)\n0.91 (80)\n0.78\n0.75\nACE (DBSCANeps=0.2)\n0.46 (5)\n0.6 (5)\n0.94 (40)\n0.87 (40)\n0.45 (30)\n0.39 (30)\n0.73 (10)\n0.69 (10)\n0.98 (80)\n0.91 (80)\n0.71\n0.69\nACE (*HDBSCAN*)\n0.46 (5)\n0.6 (5)\n0.94 (40)\n0.87 (40)\n0.02 (25)\n0.06 (25)\n0.85 (10)\n0.78 (10)\n0.98 (80)\n0.91 (80)\n0.65\n0.64\nUSPS (10)\nYTF (41)\nFRGC (20)\nMNIST-test (10)\nCMU-PIE (68)\nUMist (20)\nCOIL-20 (20)\nCOIL-100 (100)\nAverage\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nJULE: Calinski-Harabasz index\nPaired score\n0.84\n0.73\n0.03\n-0.06\n-0.49\n-0.31\n0.61\n0.56\n-0.09\n-0.07\n-0.04\n0.07\n0.74\n0.64\n0.60\n0.51\n0.27\n0.26\nACE (DBSCANeps=0.1)\n0.84\n0.73\n0.92\n0.83\n-0.37\n-0.20\n0.61\n0.56\n0.83\n0.69\n-0.04\n0.07\n0.76\n0.71\n0.56\n0.51\n0.51\n0.49\nACE (DBSCANeps=0.2)\n0.84\n0.73\n0.17\n0.06\n-0.37\n-0.20\n0.61\n0.56\n0.83\n0.69\n-0.07\n0.02\n0.76\n0.71\n0.56\n0.51\n0.42\n0.39\nACE (*HDBSCAN*)\n0.84\n0.73\n0.92\n0.83\n-0.11\n-0.03\n0.61\n0.56\n0.83\n0.69\n-0.07\n0.02\n0.76\n0.71\n0.65\n0.56\n0.55\n0.51\nJULE: Davies-Bouldin index\nPaired score\n0.39\n0.29\n0.10\n0.06\n0.37\n0.25\n0.49\n0.33\n0.83\n0.60\n-0.28\n-0.29\n-0.29\n-0.21\n-0.87\n-0.73\n0.09\n0.04\nACE (DBSCANeps=0.1)\n0.90\n0.78\n0.90\n0.78\n0.60\n0.42\n0.67\n0.56\n0.88\n0.73\n-0.89\n-0.78\n-0.71\n-0.57\n-0.83\n-0.73\n0.19\n0.15\nACE (DBSCANeps=0.2)\n0.90\n0.78\n0.30\n0.17\n0.60\n0.42\n0.67\n0.56\n0.85\n0.69\n-0.83\n-0.69\n-0.71\n-0.57\n-0.82\n-0.69\n0.12\n0.08\nACE (*HDBSCAN*)\n0.89\n0.73\n0.80\n0.67\n0.60\n0.42\n0.83\n0.64\n0.88\n0.73\n-0.42\n-0.33\n-0.71\n-0.64\n-0.82\n-0.69\n0.26\n0.19\nJULE: Silhouette score (cosine distance)\nPaired score\n0.89\n0.78\n0.27\n0.22\n0.21\n0.09\n0.81\n0.64\n0.99\n0.96\n-0.26\n-0.24\n0.55\n0.43\n0.52\n0.33\n0.50\n0.40\nACE (DBSCANeps=0.1)\n0.96\n0.91\n0.98\n0.94\n0.46\n0.37\n0.96\n0.87\n0.99\n0.96\n-0.76\n-0.60\n0.71\n0.50\n0.60\n0.47\n0.61\n0.55\nACE (DBSCANeps=0.2)\n0.96\n0.91\n0.70\n0.56\n0.64\n0.54\n0.94\n0.82\n0.99\n0.96\n-0.76\n-0.60\n0.71\n0.50\n0.60\n0.47\n0.60\n0.52\nACE (*HDBSCAN*)\n0.95\n0.87\n0.98\n0.94\n0.64\n0.54\n0.94\n0.82\n0.99\n0.96\n-0.32\n-0.24\n0.76\n0.57\n0.60\n0.47\n0.69\n0.61\nJULE: Silhouette score (euclidean distance)\nPaired score\n0.93\n0.82\n0.30\n0.28\n0.21\n0.09\n0.82\n0.64\n0.98\n0.91\n-0.13\n-0.16\n0.52\n0.36\n0.55\n0.42\n0.52\n0.42\nACE (DBSCANeps=0.1)\n0.90\n0.82\n0.98\n0.94\n0.54\n0.42\n0.89\n0.78\n0.99\n0.96\n-0.89\n-0.73\n0.74\n0.50\n0.59\n0.47\n0.59\n0.52\nACE (DBSCANeps=0.2)\n0.90\n0.82\n0.98\n0.94\n0.60\n0.48\n0.89\n0.78\n0.99\n0.96\n-0.89\n-0.73\n0.74\n0.50\n0.59\n0.47\n0.60\n0.53\nACE (*HDBSCAN*)\n0.95\n0.87\n0.98\n0.94\n0.57\n0.48\n0.92\n0.78\n0.99\n0.96\n-0.03\n-0.11\n0.74\n0.50\n0.59\n0.47\n0.71\n0.61\nDEPICT: Calinski-Harabasz index\nPaired score\n0.88\n0.82\n-0.96\n-0.91\n-0.37\n-0.22\n0.79\n0.73\n-0.92\n-0.82\n-0.11\n-0.08\nACE (DBSCANeps=0.1)\n0.88\n0.82\n0.90\n0.78\n0.73\n0.61\n0.94\n0.87\n0.95\n0.87\n0.88\n0.79\nACE (DBSCANeps=0.2)\n0.88\n0.82\n0.88\n0.73\n0.70\n0.61\n0.94\n0.87\n0.95\n0.87\n0.87\n0.78\nACE (*HDBSCAN*)\n0.88\n0.82\n-0.67\n-0.56\n0.92\n0.78\n0.82\n0.78\n0.92\n0.82\n0.57\n0.53\nDEPICT: Davies-Bouldin index\nPaired score\n0.88\n0.82\n-0.77\n-0.60\n-0.37\n-0.22\n0.79\n0.73\n-0.10\n0.02\n0.09\n0.15\nACE (DBSCANeps=0.1)\n0.93\n0.82\n1.00\n1.00\n0.90\n0.78\n0.88\n0.73\n0.87\n0.78\n0.91\n0.82\nACE (DBSCANeps=0.2)\n0.93\n0.82\n0.96\n0.91\n0.90\n0.78\n0.90\n0.82\n0.64\n0.47\n0.87\n0.76\nACE (*HDBSCAN*)\n0.93\n0.82\n0.96\n0.91\n0.92\n0.83\n0.93\n0.87\n0.96\n0.91\n0.94\n0.87\nDEPICT: Silhouette score (cosine distance)\nPaired score\n0.87\n0.78\n-0.69\n-0.56\n-0.37\n-0.22\n0.79\n0.73\n0.07\n0.11\n0.14\n0.17\nACE (DBSCANeps=0.1)\n0.88\n0.82\n1.00\n1.00\n0.88\n0.78\n0.95\n0.87\n1.00\n1.00\n0.94\n0.89\nACE (DBSCANeps=0.2)\n0.88\n0.82\n0.92\n0.82\n0.80\n0.67\n0.94\n0.87\n0.72\n0.56\n0.85\n0.75\nACE (*HDBSCAN*)\n0.95\n0.87\n0.92\n0.82\n0.80\n0.67\n0.95\n0.87\n0.99\n0.96\n0.92\n0.84\nDEPICT: Silhouette score (euclidean distance)\nPaired score\n0.87\n0.78\n-0.64\n-0.51\n-0.37\n-0.22\n0.79\n0.73\n-0.12\n-0.02\n0.11\n0.15\nACE (DBSCANeps=0.1)\n0.88\n0.82\n0.98\n0.91\n0.92\n0.83\n0.94\n0.87\n0.98\n0.91\n0.94\n0.87\nACE (DBSCANeps=0.2)\n0.88\n0.82\n0.98\n0.91\n0.97\n0.89\n0.94\n0.87\n0.98\n0.91\n0.95\n0.88\nACE (*HDBSCAN*)\n0.88\n0.82\n0.98\n0.91\n0.73\n0.56\n0.95\n0.87\n0.98\n0.91\n0.90\n0.81\n\nPageRank vs. HITS\nACE incorporates link analysis to score and rank each space within the selected group embedding spaces based on its linkage in the group. Two popular link algorithms introduced in Appendix A.5.3 are *HITS* and *PageRank*. In our main text, we chose PageRank as it considers both incoming and outgoing links simultaneously, while *HITS* considers them separately. We conducted experiments with both algorithms to compare their performance. For HITS, we utilized the authority value as the weight, considering its focus on incoming links.\n\nIn cases where the algorithm failed to converge, we assigned equal weights to all spaces. In Tables 23 and 24, we present the comparative performance for hyperparameter tuning, and in Tables 25 and 26, we report the performance for determining the number of clusters. Throughout the experiments, we observed that these two algorithms produced very similar performances, and in some cases, *PageRank* yielded higher correlation, such as for *JULE* (Silhouette score with euclidean distance) and *DEPICT* (Davies-Bouldin index) when determining the number of clusters. Generally, *PageRank* demonstrated slightly better performance than *HITS*.\n\nUSPS\nYTF\nFRGC\nMNIST-test\nCMU-PIE\nUMist\nCOIL-20\nCOIL-100\nAverage\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nJULE: Calinski-Harabasz index\nPaired score\n0.17\n0.13\n0.52\n0.40\n-0.13\n-0.10\n0.49\n0.34\n-0.13\n-0.08\n0.70\n0.50\n0.53\n0.38\n0.20\n0.19\n0.29\n0.22\nACE (HITS)\n0.80\n0.63\n0.90\n0.73\n0.39\n0.26\n0.87\n0.71\n0.98\n0.90\n0.82\n0.62\n0.60\n0.46\n0.95\n0.82\n0.79\n0.64\nACE (PR)\n0.80\n0.63\n0.90\n0.73\n0.39\n0.26\n0.87\n0.71\n0.98\n0.90\n0.81\n0.61\n0.60\n0.45\n0.95\n0.82\n0.79\n0.64\nJULE: Davies-Bouldin index\nPaired score\n-0.10\n-0.03\n-0.32\n-0.21\n-0.08\n-0.05\n-0.13\n-0.06\n0.26\n0.20\n0.62\n0.44\n0.61\n0.42\n0.43\n0.35\n0.16\n0.13\nACE (HITS)\n-0.08\n-0.02\n-0.30\n-0.21\n0.21\n0.15\n0.73\n0.55\n0.10\n0.06\n0.46\n0.34\n0.23\n0.22\n0.62\n0.44\n0.25\n0.19\nACE (PR)\n-0.08\n-0.02\n-0.30\n-0.21\n0.22\n0.16\n0.73\n0.55\n0.10\n0.06\n0.38\n0.27\n0.23\n0.22\n0.48\n0.33\n0.22\n0.17\nJULE: Silhouette score (cosine distance)\nPaired score\n0.28\n0.22\n0.73\n0.56\n0.09\n0.06\n0.63\n0.47\n0.50\n0.36\n0.71\n0.50\n0.68\n0.50\n0.74\n0.54\n0.54\n0.40\nACE (HITS)\n0.89\n0.73\n0.93\n0.83\n0.52\n0.35\n0.81\n0.66\n0.99\n0.93\n0.80\n0.60\n0.44\n0.38\n0.92\n0.78\n0.79\n0.66\nACE (PR)\n0.89\n0.73\n0.93\n0.83\n0.52\n0.35\n0.81\n0.66\n0.99\n0.93\n0.79\n0.59\n0.44\n0.38\n0.92\n0.78\n0.79\n0.66\nJULE: Silhouette score (euclidean distance)\nPaired score\n0.27\n0.20\n0.72\n0.55\n0.04\n0.03\n0.56\n0.41\n0.42\n0.30\n0.70\n0.50\n0.64\n0.46\n0.55\n0.41\n0.49\n0.36\nACE (HITS)\n0.88\n0.72\n0.89\n0.75\n0.42\n0.28\n0.81\n0.65\n0.97\n0.88\n0.88\n0.70\n0.41\n0.36\n0.92\n0.78\n0.77\n0.64\nACE (PR)\n0.88\n0.72\n0.89\n0.75\n0.42\n0.28\n0.81\n0.65\n0.98\n0.90\n0.88\n0.70\n0.41\n0.36\n0.92\n0.78\n0.77\n0.64\nDEPICT: Calinski-Harabasz index\nPaired score\n0.76\n0.57\n0.44\n0.26\n0.76\n0.57\n0.89\n0.72\n0.49\n0.44\n0.67\n0.51\nACE (HITS)\n0.91\n0.77\n0.56\n0.44\n0.94\n0.82\n0.96\n0.87\n0.96\n0.87\n0.87\n0.75\nACE (PR)\n0.91\n0.77\n0.56\n0.44\n0.94\n0.82\n0.96\n0.87\n0.96\n0.87\n0.87\n0.75\nDEPICT: Davies-Bouldin index\nPaired score\n0.81\n0.59\n0.45\n0.31\n0.90\n0.74\n0.89\n0.72\n0.63\n0.59\n0.73\n0.59\nACE (HITS)\n0.91\n0.82\n0.64\n0.52\n0.92\n0.80\n0.96\n0.87\n0.98\n0.92\n0.88\n0.79\nACE (PR)\n0.91\n0.82\n0.76\n0.58\n0.91\n0.79\n0.96\n0.87\n0.98\n0.92\n0.90\n0.80\nDEPICT: Silhouette score (cosine distance)\nPaired score\n0.81\n0.62\n0.45\n0.33\n0.90\n0.75\n0.89\n0.72\n0.77\n0.58\n0.76\n0.60\nACE (HITS)\n0.97\n0.90\n0.71\n0.56\n0.94\n0.82\n0.97\n0.90\n0.94\n0.83\n0.91\n0.80\nACE (PR)\n0.97\n0.90\n0.71\n0.56\n0.94\n0.82\n0.97\n0.90\n0.94\n0.83\n0.91\n0.80\nDEPICT: Silhouette score (euclidean distance)\nPaired score\n0.73\n0.50\n0.47\n0.36\n0.79\n0.65\n0.86\n0.69\n0.59\n0.52\n0.69\n0.54\nACE (HITS)\n0.97\n0.88\n0.62\n0.49\n0.95\n0.83\n0.98\n0.90\n0.94\n0.82\n0.89\n0.78\nACE (PR)\n0.97\n0.88\n0.65\n0.50\n0.95\n0.83\n0.98\n0.90\n0.94\n0.82\n0.90\n0.79\nUSPS\nYTF\nFRGC\nMNIST-test\nCMU-PIE\nUMist\nCOIL-20\nCOIL-100\nAverage\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nJULE: Calinski-Harabasz index\nPaired score\n0.04\n0.05\n0.39\n0.27\n-0.26\n-0.18\n0.31\n0.21\n-0.20\n-0.12\n0.64\n0.45\n0.57\n0.40\n0.09\n0.08\n0.20\n0.14\nACE (HITS)\n0.90\n0.77\n0.73\n0.54\n0.49\n0.36\n0.95\n0.82\n0.97\n0.87\n0.82\n0.62\n0.58\n0.40\n0.93\n0.81\n0.80\n0.65\nACE (PR)\n0.90\n0.77\n0.73\n0.54\n0.49\n0.36\n0.95\n0.82\n0.97\n0.87\n0.81\n0.61\n0.57\n0.40\n0.93\n0.81\n0.79\n0.65\nJULE: Davies-Bouldin index\nPaired score\n-0.27\n-0.15\n-0.14\n-0.09\n-0.23\n-0.14\n-0.35\n-0.19\n0.20\n0.16\n0.53\n0.36\n0.63\n0.44\n0.33\n0.26\n0.09\n0.08\nACE (HITS)\n-0.31\n-0.10\n-0.07\n-0.07\n0.52\n0.38\n0.79\n0.64\n0.07\n0.03\n0.36\n0.25\n0.20\n0.18\n0.56\n0.38\n0.27\n0.21\nACE (PR)\n-0.30\n-0.09\n-0.07\n-0.07\n0.53\n0.38\n0.79\n0.64\n0.07\n0.03\n0.27\n0.20\n0.21\n0.18\n0.44\n0.28\n0.24\n0.19\nJULE: Silhouette score (cosine distance)\nPaired score\n0.17\n0.14\n0.59\n0.41\n0.07\n0.06\n0.47\n0.33\n0.45\n0.33\n0.64\n0.46\n0.70\n0.51\n0.64\n0.45\n0.47\n0.34\nACE (HITS)\n0.96\n0.85\n0.74\n0.55\n0.82\n0.65\n0.92\n0.78\n0.98\n0.92\n0.79\n0.58\n0.41\n0.32\n0.84\n0.68\n0.81\n0.67\nACE (PR)\n0.96\n0.85\n0.74\n0.55\n0.82\n0.65\n0.92\n0.78\n0.98\n0.92\n0.78\n0.58\n0.41\n0.32\n0.84\n0.68\n0.81\n0.67\nJULE: Silhouette score (euclidean distance)\nPaired score\n0.14\n0.12\n0.54\n0.39\n-0.08\n-0.02\n0.41\n0.27\n0.36\n0.27\n0.64\n0.46\n0.67\n0.48\n0.44\n0.31\n0.39\n0.28\nACE (HITS)\n0.93\n0.78\n0.63\n0.48\n0.71\n0.53\n0.92\n0.78\n0.98\n0.90\n0.86\n0.68\n0.39\n0.30\n0.84\n0.68\n0.78\n0.64\nACE (PR)\n0.93\n0.78\n0.63\n0.48\n0.71\n0.53\n0.92\n0.78\n0.98\n0.91\n0.86\n0.68\n0.39\n0.30\n0.84\n0.68\n0.78\n0.64\nDEPICT: Calinski-Harabasz index\nPaired score\n0.56\n0.40\n0.54\n0.35\n0.76\n0.57\n0.88\n0.69\n0.48\n0.43\n0.64\n0.49\nACE (HITS)\n0.82\n0.72\n0.61\n0.45\n0.91\n0.82\n0.97\n0.91\n0.96\n0.87\n0.86\n0.75\nACE (PR)\n0.82\n0.72\n0.61\n0.45\n0.91\n0.82\n0.97\n0.91\n0.96\n0.87\n0.86\n0.75\nDEPICT: Davies-Bouldin index\nPaired score\n0.61\n0.42\n0.48\n0.32\n0.92\n0.74\n0.88\n0.69\n0.62\n0.56\n0.70\n0.55\nACE (HITS)\n0.99\n0.96\n0.52\n0.37\n0.90\n0.75\n0.99\n0.96\n0.96\n0.87\n0.87\n0.78\nACE (PR)\n0.99\n0.96\n0.65\n0.46\n0.90\n0.74\n0.99\n0.96\n0.96\n0.87\n0.90\n0.80\nDEPICT: Silhouette score (cosine distance)\nPaired score\n0.62\n0.45\n0.53\n0.42\n0.91\n0.75\n0.88\n0.69\n0.77\n0.58\n0.74\n0.58\nACE (HITS)\n0.95\n0.88\n0.70\n0.54\n0.91\n0.77\n0.96\n0.88\n0.94\n0.83\n0.89\n0.78\nACE (PR)\n0.95\n0.88\n0.70\n0.54\n0.91\n0.77\n0.96\n0.88\n0.94\n0.83\n0.89\n0.78\nDEPICT: Silhouette score (euclidean distance)\nPaired score\n0.52\n0.33\n0.57\n0.45\n0.80\n0.62\n0.85\n0.65\n0.59\n0.48\n0.67\n0.51\nACE (HITS)\n0.95\n0.87\n0.61\n0.48\n0.91\n0.78\n0.97\n0.91\n0.95\n0.84\n0.88\n0.77\nACE (PR)\n0.95\n0.87\n0.63\n0.49\n0.91\n0.78\n0.97\n0.91\n0.95\n0.84\n0.88\n0.78\nUSPS (10)\nYTF (41)\nFRGC (20)\nMNIST-test (10)\nCMU-PIE (68)\nUMist (20)\nCOIL-20 (20)\nCOIL-100 (100)\nAverage\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nJULE: Calinski-Harabasz index\nPaired score\n0.65 (10)\n0.64 (10)\n0.1 (50)\n0.06 (50)\n-0.93 (15)\n-0.83 (15)\n0.64 (10)\n0.6 (10)\n-0.03 (20)\n-0.02 (20)\n-0.13 (5)\n-0.07 (5)\n0.76 (15)\n0.71 (15)\n0.74 (80)\n0.56 (80)\n0.22\n0.21\nACE (HITS)\n0.65 (10)\n0.64 (10)\n0.93 (50)\n0.83 (50)\n0.03 (15)\n0.0 (15)\n0.64 (10)\n0.6 (10)\n0.88 (70)\n0.73 (70)\n-0.14 (5)\n-0.11 (5)\n0.74 (15)\n0.64 (15)\n0.79 (80)\n0.69 (80)\n0.56\n0.50\nACE (PR)\n0.65 (10)\n0.64 (10)\n0.93 (50)\n0.83 (50)\n-0.72 (15)\n-0.67 (15)\n0.64 (10)\n0.6 (10)\n0.88 (70)\n0.73 (70)\n-0.14 (5)\n-0.11 (5)\n0.74 (15)\n0.64 (15)\n0.79 (80)\n0.69 (80)\n0.47\n0.42\nJULE: Davies-Bouldin index\nPaired score\n0.54 (15)\n0.38 (15)\n0.15 (50)\n0.17 (50)\n0.85 (45)\n0.67 (45)\n0.43 (10)\n0.29 (10)\n0.78 (100)\n0.56 (100)\n-0.08 (45)\n0.02 (45)\n-0.26 (40)\n-0.14 (40)\n-0.9 (20)\n-0.78 (20)\n0.19\n0.15\nACE (HITS)\n0.73 (10)\n0.69 (10)\n0.92 (50)\n0.78 (50)\n0.87 (40)\n0.72 (40)\n0.65 (25)\n0.51 (25)\n0.85 (90)\n0.69 (90)\n-0.44 (50)\n-0.24 (50)\n-0.67 (50)\n-0.5 (50)\n-0.94 (20)\n-0.82 (20)\n0.25\n0.23\nACE (PR)\n0.98 (15)\n0.91 (15)\n0.83 (50)\n0.67 (50)\n0.87 (40)\n0.72 (40)\n0.79 (10)\n0.6 (10)\n0.85 (90)\n0.69 (90)\n-0.21 (45)\n-0.02 (45)\n-0.69 (50)\n-0.57 (50)\n-0.94 (20)\n-0.82 (20)\n0.31\n0.27\nJULE: Silhouette score (cosine distance)\nPaired score\n0.99 (10)\n0.96 (10)\n0.3 (50)\n0.22 (50)\n0.72 (25)\n0.61 (25)\n0.87 (10)\n0.69 (10)\n0.98 (70)\n0.91 (70)\n-0.07 (45)\n0.07 (45)\n0.52 (25)\n0.36 (25)\n0.39 (200)\n0.2 (200)\n0.59\n0.50\nACE (HITS)\n0.95 (10)\n0.87 (10)\n0.98 (50)\n0.94 (50)\n0.62 (40)\n0.5 (40)\n0.96 (10)\n0.87 (10)\n0.98 (70)\n0.91 (70)\n-0.16 (45)\n-0.07 (45)\n0.67 (20)\n0.36 (20)\n0.46 (180)\n0.33 (180)\n0.68\n0.59\nACE (PR)\n0.95 (10)\n0.87 (10)\n0.98 (50)\n0.94 (50)\n0.7 (45)\n0.61 (45)\n0.96 (10)\n0.87 (10)\n0.98 (70)\n0.91 (70)\n-0.07 (45)\n-0.02 (45)\n0.74 (20)\n0.5 (20)\n0.46 (180)\n0.33 (180)\n0.71\n0.63\nJULE: Silhouette score (euclidean distance)\nPaired score\n0.85 (10)\n0.73 (10)\n0.33 (50)\n0.28 (50)\n0.72 (25)\n0.61 (25)\n0.88 (10)\n0.69 (10)\n0.96 (80)\n0.87 (80)\n0.07 (45)\n0.16 (45)\n0.55 (25)\n0.43 (25)\n0.44 (200)\n0.29 (200)\n0.60\n0.51\nACE (HITS)\n0.95 (10)\n0.87 (10)\n0.98 (50)\n0.94 (50)\n0.7 (45)\n0.61 (45)\n0.95 (10)\n0.82 (10)\n0.98 (70)\n0.91 (70)\n-0.62 (5)\n-0.42 (5)\n0.71 (25)\n0.43 (25)\n0.47 (200)\n0.33 (200)\n0.64\n0.56\nACE (PR)\n0.95 (10)\n0.87 (10)\n0.98 (50)\n0.94 (50)\n0.78 (45)\n0.67 (45)\n0.95 (10)\n0.82 (10)\n0.98 (70)\n0.91 (70)\n0.14 (45)\n0.11 (45)\n0.71 (25)\n0.43 (25)\n0.47 (200)\n0.33 (200)\n0.74\n0.64\nDEPICT: Calinski-Harabasz index\nPaired score\n0.46 (5)\n0.6 (5)\n-0.99 (5)\n-0.96 (5)\n-0.85 (10)\n-0.72 (10)\n0.44 (5)\n0.56 (5)\n-0.92 (10)\n-0.82 (10)\n-0.37\n-0.27\nACE (HITS)\n0.46 (5)\n0.6 (5)\n-0.61 (5)\n-0.56 (5)\n0.82 (30)\n0.72 (30)\n0.95 (10)\n0.87 (10)\n0.95 (80)\n0.87 (80)\n0.51\n0.50\nACE (PR)\n0.46 (5)\n0.6 (5)\n-0.66 (5)\n-0.51 (5)\n0.77 (30)\n0.61 (30)\n0.46 (5)\n0.6 (5)\n0.92 (80)\n0.82 (80)\n0.39\n0.42\nDEPICT: Davies-Bouldin index\nPaired score\n0.46 (5)\n0.6 (5)\n-0.78 (5)\n-0.64 (5)\n-0.85 (10)\n-0.72 (10)\n0.44 (5)\n0.56 (5)\n-0.1 (10)\n0.02 (10)\n-0.17\n-0.04\nACE (HITS)\n0.27 (15)\n0.33 (15)\n0.95 (50)\n0.87 (50)\n0.53 (35)\n0.44 (35)\n0.78 (10)\n0.69 (10)\n1.0 (80)\n1.0 (80)\n0.71\n0.67\nACE (PR)\n0.62 (10)\n0.6 (10)\n0.95 (50)\n0.87 (50)\n0.77 (35)\n0.67 (35)\n0.78 (10)\n0.69 (10)\n0.96 (70)\n0.91 (70)\n0.82\n0.75\nDEPICT: Silhouette score (cosine distance)\nPaired score\n0.44 (5)\n0.56 (5)\n-0.7 (5)\n-0.6 (5)\n-0.85 (10)\n-0.72 (10)\n0.44 (5)\n0.56 (5)\n0.07 (10)\n0.11 (10)\n-0.12\n-0.02\nACE (HITS)\n0.46 (5)\n0.6 (5)\n0.87 (40)\n0.78 (40)\n0.93 (35)\n0.83 (35)\n0.85 (10)\n0.78 (10)\n0.99 (80)\n0.96 (80)\n0.82\n0.79\nACE (PR)\n0.65 (15)\n0.64 (15)\n0.87 (40)\n0.78 (40)\n0.93 (35)\n0.83 (35)\n0.85 (10)\n0.78 (10)\n0.99 (80)\n0.96 (80)\n0.86\n0.80\nDEPICT: Silhouette score (euclidean distance)\nPaired score\n0.44 (5)\n0.56 (5)\n-0.61 (5)\n-0.47 (5)\n-0.85 (10)\n-0.72 (10)\n0.44 (5)\n0.56 (5)\n-0.12 (10)\n-0.02 (10)\n-0.14\n-0.02\nACE (HITS)\n0.46 (5)\n0.6 (5)\n0.94 (40)\n0.87 (40)\n0.35 (30)\n0.28 (30)\n0.85 (10)\n0.78 (10)\n0.98 (80)\n0.91 (80)\n0.72\n0.69\nACE (PR)\n0.46 (5)\n0.6 (5)\n0.94 (40)\n0.87 (40)\n0.02 (25)\n0.06 (25)\n0.85 (10)\n0.78 (10)\n0.98 (80)\n0.91 (80)\n0.65\n0.64\nUSPS (10)\nYTF (41)\nFRGC (20)\nMNIST-test (10)\nCMU-PIE (68)\nUMist (20)\nCOIL-20 (20)\nCOIL-100 (100)\nAverage\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nJULE: Calinski-Harabasz index\nPaired score\n0.84\n0.73\n0.03\n-0.06\n-0.49\n-0.31\n0.61\n0.56\n-0.09\n-0.07\n-0.04\n0.07\n0.74\n0.64\n0.60\n0.51\n0.27\n0.26\nACE (HITS)\n0.84\n0.73\n0.92\n0.83\n-0.07\n-0.03\n0.61\n0.56\n0.83\n0.69\n-0.07\n0.02\n0.76\n0.71\n0.65\n0.56\n0.56\n0.51\nACE (PR)\n0.84\n0.73\n0.92\n0.83\n-0.11\n-0.03\n0.61\n0.56\n0.83\n0.69\n-0.07\n0.02\n0.76\n0.71\n0.65\n0.56\n0.55\n0.51\nJULE: Davies-Bouldin index\nPaired score\n0.39\n0.29\n0.10\n0.06\n0.37\n0.25\n0.49\n0.33\n0.83\n0.60\n-0.28\n-0.29\n-0.29\n-0.21\n-0.87\n-0.73\n0.09\n0.04\nACE (HITS)\n0.90\n0.78\n0.90\n0.78\n0.60\n0.42\n0.67\n0.56\n0.88\n0.73\n-0.71\n-0.56\n-0.76\n-0.57\n-0.82\n-0.69\n0.21\n0.18\nACE (PR)\n0.89\n0.73\n0.80\n0.67\n0.60\n0.42\n0.83\n0.64\n0.88\n0.73\n-0.42\n-0.33\n-0.71\n-0.64\n-0.82\n-0.69\n0.26\n0.19\nJULE: Silhouette score (cosine distance)\nPaired score\n0.89\n0.78\n0.27\n0.22\n0.21\n0.09\n0.81\n0.64\n0.99\n0.96\n-0.26\n-0.24\n0.55\n0.43\n0.52\n0.33\n0.50\n0.40\nACE (HITS)\n0.95\n0.87\n0.98\n0.94\n0.73\n0.65\n0.94\n0.82\n0.99\n0.96\n-0.33\n-0.29\n0.69\n0.43\n0.60\n0.47\n0.69\n0.61\nACE (PR)\n0.95\n0.87\n0.98\n0.94\n0.64\n0.54\n0.94\n0.82\n0.99\n0.96\n-0.32\n-0.24\n0.76\n0.57\n0.60\n0.47\n0.69\n0.61\nJULE: Silhouette score (euclidean distance)\nPaired score\n0.93\n0.82\n0.30\n0.28\n0.21\n0.09\n0.82\n0.64\n0.98\n0.91\n-0.13\n-0.16\n0.52\n0.36\n0.55\n0.42\n0.52\n0.42\nACE (HITS)\n0.95\n0.87\n0.98\n0.94\n0.55\n0.42\n0.92\n0.78\n0.99\n0.96\n-0.77\n-0.64\n0.74\n0.50\n0.59\n0.47\n0.62\n0.54\nACE (PR)\n0.95\n0.87\n0.98\n0.94\n0.57\n0.48\n0.92\n0.78\n0.99\n0.96\n-0.03\n-0.11\n0.74\n0.50\n0.59\n0.47\n0.71\n0.61\nDEPICT: Calinski-Harabasz index\nPaired score\n0.88\n0.82\n-0.96\n-0.91\n-0.37\n-0.22\n0.79\n0.73\n-0.92\n-0.82\n-0.11\n-0.08\nACE (HITS)\n0.88\n0.82\n-0.62\n-0.60\n0.87\n0.78\n0.94\n0.87\n0.95\n0.87\n0.60\n0.55\nACE (PR)\n0.88\n0.82\n-0.67\n-0.56\n0.92\n0.78\n0.82\n0.78\n0.92\n0.82\n0.57\n0.53\nDEPICT: Davies-Bouldin index\nPaired score\n0.88\n0.82\n-0.77\n-0.60\n-0.37\n-0.22\n0.79\n0.73\n-0.10\n0.02\n0.09\n0.15\nACE (HITS)\n0.08\n0.11\n0.96\n0.91\n0.87\n0.72\n0.93\n0.87\n1.00\n1.00\n0.77\n0.72\nACE (PR)\n0.93\n0.82\n0.96\n0.91\n0.92\n0.83\n0.93\n0.87\n0.96\n0.91\n0.94\n0.87\nDEPICT: Silhouette score (cosine distance)\nPaired score\n0.87\n0.78\n-0.69\n-0.56\n-0.37\n-0.22\n0.79\n0.73\n0.07\n0.11\n0.14\n0.17\nACE (HITS)\n0.88\n0.82\n0.92\n0.82\n0.80\n0.67\n0.95\n0.87\n0.99\n0.96\n0.91\n0.83\nACE (PR)\n0.95\n0.87\n0.92\n0.82\n0.80\n0.67\n0.95\n0.87\n0.99\n0.96\n0.92\n0.84\nDEPICT: Silhouette score (euclidean distance)\nPaired score\n0.87\n0.78\n-0.64\n-0.51\n-0.37\n-0.22\n0.79\n0.73\n-0.12\n-0.02\n0.11\n0.15\nACE (HITS)\n0.88\n0.82\n0.98\n0.91\n0.80\n0.67\n0.95\n0.87\n0.98\n0.91\n0.92\n0.84\nACE (PR)\n0.88\n0.82\n0.98\n0.91\n0.73\n0.56\n0.95\n0.87\n0.98\n0.91\n0.90\n0.81\n\nOutlier space (rank uncorrelated space)\nIn Algorithm 1 and 2, we exclude outlier spaces in the first phase of the stage-wise clustering algorithm, treating them as rank uncorrelated spaces, for ensemble analysis. However, a challenge arises when there are insufficient admissible spaces among all the embedding spaces from deep clustering models, typically due to a limited number of clustering models for comparison. In cases where M is not large enough, leading to too few admissible spaces, these spaces may be incorrectly classified as outliers in the first phase of our stage-wise grouping strategy. The current version of *ACE* cannot handle scenarios where identified admissible spaces are considered outliers.\n\nTo address this issue, we identify the \"rank uncorrelated\" space Z*outlier*\u2217 with the largest average score and compare {\u03c0(\u03c1m|Goutlier\u2217)}M\nm=1 with {\u03c0(\u03c1m|Gs\u2217)}M\nm=1 we obtained in Algorithm\n1. If G*outlier*\u2217 exceeds Gs\u2217 in terms of the average score, we conduct a paired t-test to ensure that Gs\u2217 is unlikely to surpass G*outlier*\u2217, as we apply a more stringent criterion to outlier spaces. This approach can mitigate the issue arising from too few admissible spaces, yet it concurrently elevates variance by introducing singleton subgroups of spaces. These subgroups lack rank correlation with other spaces in the final calculation, potentially leading to fluctuations, decreasing performance in other cases.\n\nUnfortunately, finding a uniform solution for both edge cases is challenging. In this section, we implement an alternative version of *ACE* that incorporates outlier spaces identified in the first grouping stage to compare with the *ACE* presented in the main text. Tables 27 and 28 report comparative performance for hyperparameter tuning, and Tables 29 and 30 report comparisons for determining the number of clusters.\n\nFrom the comparison, we observe that across most cases, *ACE* and *ACE* (with Z*outlier*)\ngenerate similar performance, suggesting that these two edge cases do not occur frequently. Both strategies outperform the application of *paired scores*, indicating that both proposed strategies can surpass the use of paired embedding spaces to calculate the validity index. In some cases, such as COIL-20 for hyperparameter tuning with *JULE* (Davies-Bouldin index), where ACE underperforms *paired scores*, the consideration of outlier space in *ACE* significantly improves performance. Upon closer inspection, we found the poor performance of *ACE* in this case was caused by only a few admissible spaces included in the comparison, suggesting that this alternative strategy can somewhat remedy the proposed strategy in certain edge cases.\n\nUSPS\nYTF\nFRGC\nMNIST-test\nCMU-PIE\nUMist\nCOIL-20\nCOIL-100\nAverage\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nJULE: Calinski-Harabasz index\nPaired score\n0.17\n0.13\n0.52\n0.40\n-0.13\n-0.10\n0.49\n0.34\n-0.13\n-0.08\n0.70\n0.50\n0.53\n0.38\n0.20\n0.19\n0.29\n0.22\nACE (with Z*outlier*)\n0.81\n0.64\n0.71\n0.54\n0.08\n0.04\n0.87\n0.71\n0.98\n0.90\n0.81\n0.61\n0.71\n0.54\n0.61\n0.47\n0.70\n0.56\nACE\n0.80\n0.63\n0.90\n0.73\n0.39\n0.26\n0.87\n0.71\n0.98\n0.90\n0.81\n0.61\n0.60\n0.45\n0.95\n0.82\n0.79\n0.64\nJULE: Davies-Bouldin index\nPaired score\n-0.10\n-0.03\n-0.32\n-0.21\n-0.08\n-0.05\n-0.13\n-0.06\n0.26\n0.20\n0.62\n0.44\n0.61\n0.42\n0.43\n0.35\n0.16\n0.13\nACE (with Z*outlier*)\n0.01\n0.05\n-0.30\n-0.21\n0.22\n0.16\n0.73\n0.55\n0.83\n0.67\n0.38\n0.27\n0.86\n0.66\n0.48\n0.33\n0.40\n0.31\nACE\n-0.08\n-0.02\n-0.30\n-0.21\n0.22\n0.16\n0.73\n0.55\n0.10\n0.06\n0.38\n0.27\n0.23\n0.22\n0.48\n0.33\n0.22\n0.17\nJULE: Silhouette score (cosine distance)\nPaired score\n0.28\n0.22\n0.73\n0.56\n0.09\n0.06\n0.63\n0.47\n0.50\n0.36\n0.71\n0.50\n0.68\n0.50\n0.74\n0.54\n0.54\n0.40\nACE (with Z*outlier*)\n0.89\n0.73\n0.93\n0.83\n0.52\n0.35\n0.81\n0.66\n0.99\n0.93\n0.79\n0.59\n0.44\n0.38\n0.92\n0.78\n0.79\n0.66\nACE\n0.89\n0.73\n0.93\n0.83\n0.52\n0.35\n0.81\n0.66\n0.99\n0.93\n0.79\n0.59\n0.44\n0.38\n0.92\n0.78\n0.79\n0.66\nJULE: Silhouette score (euclidean distance)\nPaired score\n0.27\n0.20\n0.72\n0.55\n0.04\n0.03\n0.56\n0.41\n0.42\n0.30\n0.70\n0.50\n0.64\n0.46\n0.55\n0.41\n0.49\n0.36\nACE (with Z*outlier*)\n0.88\n0.72\n0.89\n0.75\n0.53\n0.36\n0.81\n0.65\n0.52\n0.44\n0.88\n0.70\n0.41\n0.36\n0.92\n0.78\n0.73\n0.60\nACE\n0.88\n0.72\n0.89\n0.75\n0.42\n0.28\n0.81\n0.65\n0.98\n0.90\n0.88\n0.70\n0.41\n0.36\n0.92\n0.78\n0.77\n0.64\nDEPICT: Calinski-Harabasz index\nPaired score\n0.76\n0.57\n0.44\n0.26\n0.76\n0.57\n0.89\n0.72\n0.49\n0.44\n0.67\n0.51\nACE (with Z*outlier*)\n0.91\n0.77\n0.56\n0.44\n0.94\n0.82\n0.96\n0.87\n0.96\n0.87\n0.87\n0.75\nACE\n0.91\n0.77\n0.56\n0.44\n0.94\n0.82\n0.96\n0.87\n0.96\n0.87\n0.87\n0.75\nDEPICT: Davies-Bouldin index\nPaired score\n0.81\n0.59\n0.45\n0.31\n0.90\n0.74\n0.89\n0.72\n0.63\n0.59\n0.73\n0.59\nACE (with Z*outlier*)\n0.91\n0.82\n0.76\n0.58\n0.91\n0.79\n0.96\n0.87\n0.98\n0.92\n0.90\n0.80\nACE\n0.91\n0.82\n0.76\n0.58\n0.91\n0.79\n0.96\n0.87\n0.98\n0.92\n0.90\n0.80\nDEPICT: Silhouette score (cosine distance)\nPaired score\n0.81\n0.62\n0.45\n0.33\n0.90\n0.75\n0.89\n0.72\n0.77\n0.58\n0.76\n0.60\nACE (with Z*outlier*)\n0.97\n0.90\n0.71\n0.56\n0.94\n0.82\n0.97\n0.90\n0.94\n0.83\n0.91\n0.80\nACE\n0.97\n0.90\n0.71\n0.56\n0.94\n0.82\n0.97\n0.90\n0.94\n0.83\n0.91\n0.80\nDEPICT: Silhouette score (euclidean distance)\nPaired score\n0.73\n0.50\n0.47\n0.36\n0.79\n0.65\n0.86\n0.69\n0.59\n0.52\n0.69\n0.54\nACE (with Z*outlier*)\n0.97\n0.88\n0.65\n0.50\n0.95\n0.83\n0.98\n0.90\n0.94\n0.82\n0.90\n0.79\nACE\n0.97\n0.88\n0.65\n0.50\n0.95\n0.83\n0.98\n0.90\n0.94\n0.82\n0.90\n0.79\nUSPS\nYTF\nFRGC\nMNIST-test\nCMU-PIE\nUMist\nCOIL-20\nCOIL-100\nAverage\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nJULE: Calinski-Harabasz index\nPaired score\n0.04\n0.05\n0.39\n0.27\n-0.26\n-0.18\n0.31\n0.21\n-0.20\n-0.12\n0.64\n0.45\n0.57\n0.40\n0.09\n0.08\n0.20\n0.14\nACE (with Z*outlier*)\n0.85\n0.70\n0.61\n0.46\n0.13\n0.09\n0.95\n0.82\n0.97\n0.87\n0.81\n0.61\n0.68\n0.51\n0.59\n0.46\n0.70\n0.57\nACE\n0.90\n0.77\n0.73\n0.54\n0.49\n0.36\n0.95\n0.82\n0.97\n0.87\n0.81\n0.61\n0.57\n0.40\n0.93\n0.81\n0.79\n0.65\nJULE: Davies-Bouldin index\nPaired score\n-0.27\n-0.15\n-0.14\n-0.09\n-0.23\n-0.14\n-0.35\n-0.19\n0.20\n0.16\n0.53\n0.36\n0.63\n0.44\n0.33\n0.26\n0.09\n0.08\nACE (with Z*outlier*)\n-0.28\n-0.12\n-0.07\n-0.07\n0.53\n0.38\n0.79\n0.64\n0.78\n0.62\n0.27\n0.20\n0.84\n0.64\n0.44\n0.28\n0.41\n0.32\nACE\n-0.30\n-0.09\n-0.07\n-0.07\n0.53\n0.38\n0.79\n0.64\n0.07\n0.03\n0.27\n0.20\n0.21\n0.18\n0.44\n0.28\n0.24\n0.19\nJULE: Silhouette score (cosine distance)\nPaired score\n0.17\n0.14\n0.59\n0.41\n0.07\n0.06\n0.47\n0.33\n0.45\n0.33\n0.64\n0.46\n0.70\n0.51\n0.64\n0.45\n0.47\n0.34\nACE (with Z*outlier*)\n0.96\n0.85\n0.74\n0.55\n0.82\n0.65\n0.92\n0.78\n0.98\n0.92\n0.78\n0.58\n0.41\n0.32\n0.84\n0.68\n0.81\n0.67\nACE\n0.96\n0.85\n0.74\n0.55\n0.82\n0.65\n0.92\n0.78\n0.98\n0.92\n0.78\n0.58\n0.41\n0.32\n0.84\n0.68\n0.81\n0.67\nJULE: Silhouette score (euclidean distance)\nPaired score\n0.14\n0.12\n0.54\n0.39\n-0.08\n-0.02\n0.41\n0.27\n0.36\n0.27\n0.64\n0.46\n0.67\n0.48\n0.44\n0.31\n0.39\n0.28\nACE (with Z*outlier*)\n0.93\n0.78\n0.63\n0.48\n0.82\n0.63\n0.92\n0.78\n0.54\n0.47\n0.86\n0.68\n0.39\n0.30\n0.84\n0.68\n0.74\n0.60\nACE\n0.93\n0.78\n0.63\n0.48\n0.71\n0.53\n0.92\n0.78\n0.98\n0.91\n0.86\n0.68\n0.39\n0.30\n0.84\n0.68\n0.78\n0.64\nDEPICT: Calinski-Harabasz index\nPaired score\n0.56\n0.40\n0.54\n0.35\n0.76\n0.57\n0.88\n0.69\n0.48\n0.43\n0.64\n0.49\nACE (with Z*outlier*)\n0.82\n0.72\n0.61\n0.45\n0.91\n0.82\n0.97\n0.91\n0.96\n0.87\n0.86\n0.75\nACE\n0.82\n0.72\n0.61\n0.45\n0.91\n0.82\n0.97\n0.91\n0.96\n0.87\n0.86\n0.75\nDEPICT: Davies-Bouldin index\nPaired score\n0.61\n0.42\n0.48\n0.32\n0.92\n0.74\n0.88\n0.69\n0.62\n0.56\n0.70\n0.55\nACE (with Z*outlier*)\n0.99\n0.96\n0.65\n0.46\n0.90\n0.74\n0.99\n0.96\n0.96\n0.87\n0.90\n0.80\nACE\n0.99\n0.96\n0.65\n0.46\n0.90\n0.74\n0.99\n0.96\n0.96\n0.87\n0.90\n0.80\nDEPICT: Silhouette score (cosine distance)\nPaired score\n0.62\n0.45\n0.53\n0.42\n0.91\n0.75\n0.88\n0.69\n0.77\n0.58\n0.74\n0.58\nACE (with Z*outlier*)\n0.95\n0.88\n0.70\n0.54\n0.91\n0.77\n0.96\n0.88\n0.94\n0.83\n0.89\n0.78\nACE\n0.95\n0.88\n0.70\n0.54\n0.91\n0.77\n0.96\n0.88\n0.94\n0.83\n0.89\n0.78\nDEPICT: Silhouette score (euclidean distance)\nPaired score\n0.52\n0.33\n0.57\n0.45\n0.80\n0.62\n0.85\n0.65\n0.59\n0.48\n0.67\n0.51\nACE (with Z*outlier*)\n0.95\n0.87\n0.63\n0.49\n0.91\n0.78\n0.97\n0.91\n0.95\n0.84\n0.88\n0.78\nACE\n0.95\n0.87\n0.63\n0.49\n0.91\n0.78\n0.97\n0.91\n0.95\n0.84\n0.88\n0.78\nUSPS (10)\nYTF (41)\nFRGC (20)\nMNIST-test (10)\nCMU-PIE (68)\nUMist (20)\nCOIL-20 (20)\nCOIL-100 (100)\nAverage\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nJULE: Calinski-Harabasz index\nPaired score\n0.65 (10)\n0.64 (10)\n0.1 (50)\n0.06 (50)\n-0.93 (15)\n-0.83 (15)\n0.64 (10)\n0.6 (10)\n-0.03 (20)\n-0.02 (20)\n-0.13 (5)\n-0.07 (5)\n0.76 (15)\n0.71 (15)\n0.74 (80)\n0.56 (80)\n0.22\n0.21\nACE (with Z*outlier*)\n0.65 (10)\n0.64 (10)\n0.93 (50)\n0.83 (50)\n-0.93 (10)\n-0.83 (10)\n0.64 (10)\n0.6 (10)\n0.14 (20)\n0.16 (20)\n-0.14 (5)\n-0.11 (5)\n0.74 (15)\n0.64 (15)\n0.79 (80)\n0.69 (80)\n0.35\n0.33\nACE\n0.65 (10)\n0.64 (10)\n0.93 (50)\n0.83 (50)\n-0.72 (15)\n-0.67 (15)\n0.64 (10)\n0.6 (10)\n0.88 (70)\n0.73 (70)\n-0.14 (5)\n-0.11 (5)\n0.74 (15)\n0.64 (15)\n0.79 (80)\n0.69 (80)\n0.47\n0.42\nJULE: Davies-Bouldin index\nPaired score\n0.54 (15)\n0.38 (15)\n0.15 (50)\n0.17 (50)\n0.85 (45)\n0.67 (45)\n0.43 (10)\n0.29 (10)\n0.78 (100)\n0.56 (100)\n-0.08 (45)\n0.02 (45)\n-0.26 (40)\n-0.14 (40)\n-0.9 (20)\n-0.78 (20)\n0.19\n0.15\nACE (with Z*outlier*)\n0.98 (15)\n0.91 (15)\n0.83 (50)\n0.67 (50)\n0.87 (40)\n0.72 (40)\n0.79 (10)\n0.6 (10)\n0.85 (90)\n0.69 (90)\n-0.21 (45)\n-0.02 (45)\n-0.69 (50)\n-0.57 (50)\n-0.94 (20)\n-0.82 (20)\n0.31\n0.27\nACE\n0.98 (15)\n0.91 (15)\n0.83 (50)\n0.67 (50)\n0.87 (40)\n0.72 (40)\n0.79 (10)\n0.6 (10)\n0.85 (90)\n0.69 (90)\n-0.21 (45)\n-0.02 (45)\n-0.69 (50)\n-0.57 (50)\n-0.94 (20)\n-0.82 (20)\n0.31\n0.27\nJULE: Silhouette score (cosine distance)\nPaired score\n0.99 (10)\n0.96 (10)\n0.3 (50)\n0.22 (50)\n0.72 (25)\n0.61 (25)\n0.87 (10)\n0.69 (10)\n0.98 (70)\n0.91 (70)\n-0.07 (45)\n0.07 (45)\n0.52 (25)\n0.36 (25)\n0.39 (200)\n0.2 (200)\n0.59\n0.50\nACE (with Z*outlier*)\n0.95 (10)\n0.87 (10)\n0.98 (50)\n0.94 (50)\n0.7 (45)\n0.61 (45)\n0.96 (10)\n0.87 (10)\n0.95 (90)\n0.87 (90)\n-0.07 (45)\n-0.02 (45)\n0.74 (20)\n0.5 (20)\n0.43 (160)\n0.29 (160)\n0.70\n0.62\nACE\n0.95 (10)\n0.87 (10)\n0.98 (50)\n0.94 (50)\n0.7 (45)\n0.61 (45)\n0.96 (10)\n0.87 (10)\n0.98 (70)\n0.91 (70)\n-0.07 (45)\n-0.02 (45)\n0.74 (20)\n0.5 (20)\n0.46 (180)\n0.33 (180)\n0.71\n0.63\nJULE: Silhouette score (euclidean distance)\nPaired score\n0.85 (10)\n0.73 (10)\n0.33 (50)\n0.28 (50)\n0.72 (25)\n0.61 (25)\n0.88 (10)\n0.69 (10)\n0.96 (80)\n0.87 (80)\n0.07 (45)\n0.16 (45)\n0.55 (25)\n0.43 (25)\n0.44 (200)\n0.29 (200)\n0.60\n0.51\nACE (with Z*outlier*)\n0.95 (10)\n0.87 (10)\n0.98 (50)\n0.94 (50)\n0.78 (45)\n0.67 (45)\n0.95 (10)\n0.82 (10)\n0.95 (90)\n0.87 (90)\n0.14 (45)\n0.11 (45)\n0.71 (25)\n0.43 (25)\n0.47 (200)\n0.33 (200)\n0.74\n0.63\nACE\n0.95 (10)\n0.87 (10)\n0.98 (50)\n0.94 (50)\n0.78 (45)\n0.67 (45)\n0.95 (10)\n0.82 (10)\n0.98 (70)\n0.91 (70)\n0.14 (45)\n0.11 (45)\n0.71 (25)\n0.43 (25)\n0.47 (200)\n0.33 (200)\n0.74\n0.64\nDEPICT: Calinski-Harabasz index\nPaired score\n0.46 (5)\n0.6 (5)\n-0.99 (5)\n-0.96 (5)\n-0.85 (10)\n-0.72 (10)\n0.44 (5)\n0.56 (5)\n-0.92 (10)\n-0.82 (10)\n-0.37\n-0.27\nACE (with Z*outlier*)\n0.46 (5)\n0.6 (5)\n-0.66 (5)\n-0.51 (5)\n0.77 (30)\n0.61 (30)\n0.46 (5)\n0.6 (5)\n0.92 (80)\n0.82 (80)\n0.39\n0.42\nACE\n0.46 (5)\n0.6 (5)\n-0.66 (5)\n-0.51 (5)\n0.77 (30)\n0.61 (30)\n0.46 (5)\n0.6 (5)\n0.92 (80)\n0.82 (80)\n0.39\n0.42\nDEPICT: Davies-Bouldin index\nPaired score\n0.46 (5)\n0.6 (5)\n-0.78 (5)\n-0.64 (5)\n-0.85 (10)\n-0.72 (10)\n0.44 (5)\n0.56 (5)\n-0.1 (10)\n0.02 (10)\n-0.17\n-0.04\nACE (with Z*outlier*)\n0.62 (10)\n0.6 (10)\n0.95 (50)\n0.87 (50)\n0.77 (35)\n0.67 (35)\n0.78 (10)\n0.69 (10)\n0.96 (70)\n0.91 (70)\n0.82\n0.75\nACE\n0.62 (10)\n0.6 (10)\n0.95 (50)\n0.87 (50)\n0.77 (35)\n0.67 (35)\n0.78 (10)\n0.69 (10)\n0.96 (70)\n0.91 (70)\n0.82\n0.75\nDEPICT: Silhouette score (cosine distance)\nPaired score\n0.44 (5)\n0.56 (5)\n-0.7 (5)\n-0.6 (5)\n-0.85 (10)\n-0.72 (10)\n0.44 (5)\n0.56 (5)\n0.07 (10)\n0.11 (10)\n-0.12\n-0.02\nACE (with Z*outlier*)\n0.65 (15)\n0.64 (15)\n0.87 (40)\n0.78 (40)\n0.93 (35)\n0.83 (35)\n0.85 (10)\n0.78 (10)\n0.99 (80)\n0.96 (80)\n0.86\n0.80\nACE\n0.65 (15)\n0.64 (15)\n0.87 (40)\n0.78 (40)\n0.93 (35)\n0.83 (35)\n0.85 (10)\n0.78 (10)\n0.99 (80)\n0.96 (80)\n0.86\n0.80\nDEPICT: Silhouette score (euclidean distance)\nPaired score\n0.44 (5)\n0.56 (5)\n-0.61 (5)\n-0.47 (5)\n-0.85 (10)\n-0.72 (10)\n0.44 (5)\n0.56 (5)\n-0.12 (10)\n-0.02 (10)\n-0.14\n-0.02\nACE (with Z*outlier*)\n0.46 (5)\n0.6 (5)\n0.94 (40)\n0.87 (40)\n0.02 (25)\n0.06 (25)\n0.85 (10)\n0.78 (10)\n0.98 (80)\n0.91 (80)\n0.65\n0.64\nACE\n0.46 (5)\n0.6 (5)\n0.94 (40)\n0.87 (40)\n0.02 (25)\n0.06 (25)\n0.85 (10)\n0.78 (10)\n0.98 (80)\n0.91 (80)\n0.65\n0.64\nUSPS (10)\nYTF (41)\nFRGC (20)\nMNIST-test (10)\nCMU-PIE (68)\nUMist (20)\nCOIL-20 (20)\nCOIL-100 (100)\nAverage\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nrs\n\u03c4B\nJULE: Calinski-Harabasz index\nPaired score\n0.84\n0.73\n0.03\n-0.06\n-0.49\n-0.31\n0.61\n0.56\n-0.09\n-0.07\n-0.04\n0.07\n0.74\n0.64\n0.60\n0.51\n0.27\n0.26\nACE (with Z*outlier*)\n0.84\n0.73\n0.92\n0.83\n-0.59\n-0.42\n0.61\n0.56\n0.08\n0.11\n-0.07\n0.02\n0.76\n0.71\n0.65\n0.56\n0.40\n0.39\nACE\n0.84\n0.73\n0.92\n0.83\n-0.11\n-0.03\n0.61\n0.56\n0.83\n0.69\n-0.07\n0.02\n0.76\n0.71\n0.65\n0.56\n0.55\n0.51\nJULE: Davies-Bouldin index\nPaired score\n0.39\n0.29\n0.10\n0.06\n0.37\n0.25\n0.49\n0.33\n0.83\n0.60\n-0.28\n-0.29\n-0.29\n-0.21\n-0.87\n-0.73\n0.09\n0.04\nACE (with Z*outlier*)\n0.89\n0.73\n0.80\n0.67\n0.60\n0.42\n0.83\n0.64\n0.88\n0.73\n-0.42\n-0.33\n-0.71\n-0.64\n-0.82\n-0.69\n0.26\n0.19\nACE\n0.89\n0.73\n0.80\n0.67\n0.60\n0.42\n0.83\n0.64\n0.88\n0.73\n-0.42\n-0.33\n-0.71\n-0.64\n-0.82\n-0.69\n0.26\n0.19\nJULE: Silhouette score (cosine distance)\nPaired score\n0.89\n0.78\n0.27\n0.22\n0.21\n0.09\n0.81\n0.64\n0.99\n0.96\n-0.26\n-0.24\n0.55\n0.43\n0.52\n0.33\n0.50\n0.40\nACE (with Z*outlier*)\n0.95\n0.87\n0.98\n0.94\n0.64\n0.54\n0.94\n0.82\n0.96\n0.91\n-0.32\n-0.24\n0.76\n0.57\n0.56\n0.42\n0.69\n0.60\nACE\n0.95\n0.87\n0.98\n0.94\n0.64\n0.54\n0.94\n0.82\n0.99\n0.96\n-0.32\n-0.24\n0.76\n0.57\n0.60\n0.47\n0.69\n0.61\nJULE: Silhouette score (euclidean distance)\nPaired score\n0.93\n0.82\n0.30\n0.28\n0.21\n0.09\n0.82\n0.64\n0.98\n0.91\n-0.13\n-0.16\n0.52\n0.36\n0.55\n0.42\n0.52\n0.42\nACE (with Z*outlier*)\n0.95\n0.87\n0.98\n0.94\n0.57\n0.48\n0.92\n0.78\n0.96\n0.91\n-0.03\n-0.11\n0.74\n0.50\n0.59\n0.47\n0.71\n0.60\nACE\n0.95\n0.87\n0.98\n0.94\n0.57\n0.48\n0.92\n0.78\n0.99\n0.96\n-0.03\n-0.11\n0.74\n0.50\n0.59\n0.47\n0.71\n0.61\nDEPICT: Calinski-Harabasz index\nPaired score\n0.88\n0.82\n-0.96\n-0.91\n-0.37\n-0.22\n0.79\n0.73\n-0.92\n-0.82\n-0.11\n-0.08\nACE (with Z*outlier*)\n0.88\n0.82\n-0.67\n-0.56\n0.92\n0.78\n0.82\n0.78\n0.92\n0.82\n0.57\n0.53\nACE\n0.88\n0.82\n-0.67\n-0.56\n0.92\n0.78\n0.82\n0.78\n0.92\n0.82\n0.57\n0.53\nDEPICT: Davies-Bouldin index\nPaired score\n0.88\n0.82\n-0.77\n-0.60\n-0.37\n-0.22\n0.79\n0.73\n-0.10\n0.02\n0.09\n0.15\nACE (with Z*outlier*)\n0.93\n0.82\n0.96\n0.91\n0.92\n0.83\n0.93\n0.87\n0.96\n0.91\n0.94\n0.87\nACE\n0.93\n0.82\n0.96\n0.91\n0.92\n0.83\n0.93\n0.87\n0.96\n0.91\n0.94\n0.87\nDEPICT: Silhouette score (cosine distance)\nPaired score\n0.87\n0.78\n-0.69\n-0.56\n-0.37\n-0.22\n0.79\n0.73\n0.07\n0.11\n0.14\n0.17\nACE (with Z*outlier*)\n0.95\n0.87\n0.92\n0.82\n0.80\n0.67\n0.95\n0.87\n0.99\n0.96\n0.92\n0.84\nACE\n0.95\n0.87\n0.92\n0.82\n0.80\n0.67\n0.95\n0.87\n0.99\n0.96\n0.92\n0.84\nDEPICT: Silhouette score (euclidean distance)\nPaired score\n0.87\n0.78\n-0.64\n-0.51\n-0.37\n-0.22\n0.79\n0.73\n-0.12\n-0.02\n0.11\n0.15\nACE (with Z*outlier*)\n0.88\n0.82\n0.98\n0.91\n0.73\n0.56\n0.95\n0.87\n0.98\n0.91\n0.90\n0.81\nACE\n0.88\n0.82\n0.98\n0.91\n0.73\n0.56\n0.95\n0.87\n0.98\n0.91\n0.90\n0.81\n"
    }
]