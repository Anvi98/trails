[
    {
        "level": "#",
        "title": "Information Rates Of Successive Interference Cancellation For Optical Fiber",
        "content": "\nAlex J\u00a8ager and Gerhard Kramer, Fellow, IEEE\n\n  Abstract\u2014Successive interference cancellation (SIC) is used\nto approach the achievable information rates (AIRs) of joint\ndetection and decoding for long-haul optical fiber links. The\nAIRs of memoryless ring constellations are compared to those of\ncircularly symmetric complex Gaussian modulation for surrogate\nchannel models with correlated phase noise. Simulations are\nperformed for 1000 km of standard single-mode fiber with ideal\nRaman amplification. In this setup, 32 rings and 16 SIC-\nstages with Gaussian message-passing receivers achieve the AIR\npeaks of previous work. The computational complexity scales in\nproportion to the number of SIC-stages, where one stage has the\ncomplexity of separate detection and decoding.\n\n  Index Terms\u2014Belief propagation, nonlinearity mitigation, op-\ntical fiber communication, phase noise, successive interference\ncancellation.\n"
    },
    {
        "level": "##",
        "title": "I. Introduction",
        "content": "\n  Estimating the capacity of optical fiber is difficult be-\ncause of the interactions of frequency-dependent attenuation,\ndispersion, and Kerr non-linearity [1]. A standard approach\ncomputes achievable information rates (AIRs) by simulating\ntransmission and having the receiver process its signal via\nsurrogate, or mismatched, models. The closer the surrogate\nmodel is to the actual model, as measured by an informational\ndivergence, the higher the computed AIR. One, therefore, often\nhas a trade-off between AIR and computational complexity.\n  For example, two useful surrogate models are a memoryless\nadditive white Gaussian noise (AWGN) channel whose covari-\nance and pseudo-covariance may depend on the channel input\namplitude [1, Sec. X.C] and an AWGN channel with correlated\nphase noise with large memory [2], [3], [4]; see also [5],\n[6], [7], [8]. A memoryless model suggests practical receiver\nalgorithms with a posteriori probability (APP) processing. The\nmodels with memory improve the AIR, but it is less clear how\nto build practical receivers. In particular, the receivers in [6],\n[7], [9], [8], [10] use particle filters to compute joint detection\nand decoding (JDD) rates, but it is not obvious how to convert\nsuch structures into practical systems.\n  Two classic methods to approach JDD performance combine\nseparate detection and decoding (SDD) with either turbo\nprocessing or successive interference cancellation (SIC). The\nformer approach was applied to Wiener phase noise channels\n[11], [12], [13] and fiber-optic systems [14], [15]. This method\nhas the disadvantage of requiring receiver iterations and a\n\ndedicated code design to maximize the rates. We instead use\nthe SIC structure outlined in [1, Sec. XII] for which off-the-\nshelf codes and classic multi-level coded modulation approach\ncapacity; see also [16], [17], [18] and recently [19], [20].\n  This paper is organized as follows. Sec. II introduces nota-\ntion, the channel model, and the correlated phase and additive\nnoise (CPAN) surrogate model of [9]. Sec. III and Sec. IV\npropose SIC receivers for circularly symmetric complex Gaus-\nsian (CSCG) modulation and ring constellations, respectively.\nThe receivers use belief propagation and approximate message\npassing. For a sufficient number of SIC-stages, the receiver\nachieves, and even surpasses, the AIR for JDD predicted in\n[9]. Sec. V concludes the paper and suggests future work on\nimplementations.\n"
    },
    {
        "level": "##",
        "title": "Ii. Preliminaries A. Notation",
        "content": "\nRandom variables are written in uppercase, such as X, and their realizations in lowercase, such as x. Random vectors are written with bold letters, such as X, and their realizations as x. We write a(x) \u221d b(x) if there exists a constant c for which a(x) = c b(x). A Gaussian X with mean \u00b5 and variance \u03c32\nhas probability density function (pdf)\n\n2 (x \u2212 \u00b5)2 \u03c32 2\u03c0\u03c32 exp \ufffd \u22121 \ufffd . (1) N \ufffd x; \u00b5, \u03c32\ufffd = 1 \u221a\nSimilarly, a complex Gaussian X with mean \u00b5, variance \u03c32 =\nE\n\ufffd\n|X \u2212 \u00b5|2\ufffd\nand pseudo-variance p2 = E\n\ufffd\n(X \u2212 \u00b5)2\ufffd\nhas pdf\n\nNC \ufffd x; \u00b5, \u03c32, p2\ufffd = 1 \u03c0 \ufffd \u03c32 \ufffd \u03c32 \ufffd \u03c32 \u2212 |p|4 exp \u22121 \ufffd \ufffd\ufffd \ufffd\u22121 \ufffd (x \u2212 \u00b5) (x \u2212 \u00b5)\u2217 2[(x \u2212 \u00b5)\u2217, (x \u2212 \u00b5)] \ufffd \u03c32 p2 \ufffd p2\ufffd\u2217 \u03c32\n(2)\nwhere \u2217 denotes complex conjugation. A CSCG has p2 = 0\nand therefore the pdf\n\n\u03c32 \ufffd . (3) \u03c0\u03c32 exp \ufffd \u2212|x \u2212 \u00b5|2 NC \ufffd x; \u00b5, \u03c32\ufffd = 1\nThe function\n\n$$m\\left(x\\right)=\\left(x+\\pi\\mod2\\pi\\right)-\\pi\\tag{4}$$\nmaps x to the interval [\u2212\u03c0, \u03c0).\n"
    },
    {
        "level": "##",
        "title": "B. System Model",
        "content": "\n  We use a standard model [1] for optical networks with\nreconfigurable optical add-drop multiplexers (ROADMs). Mul-\ntiple wavelength-division multiplexing (WDM) channels co-\npropagate over a fiber span, and each receiver can access\nonly its channel of interest (COI). Co-propagating channels\ndisturb each other due to nonlinearities, such as cross-phase\nmodulation (XPM) and four-wave mixing (FWM), and the\nworst-case scenario has different co-propagating signals over\nthe entire length of the transmission link. The continuous-time\nbaseband signal for n symbols and 2b interfering channels is\n\nx(0, t) =\n\n$$\\sum_{i=1}^{n}x_{i}g(t-iT)+\\sum_{\\begin{subarray}{c}k=-b\\\\ k\\neq0\\end{subarray}}^{b}\\sum_{i=1}^{n}b_{i}^{(k)}g(t-iT)\\mathrm{e}^{\\mathrm{i}\\omega_{k}t}\\tag{5}$$\nwhere the xi and b(k)\ni are realizations of mutually independent random variables with a common alphabet X and variance\n\u03c32\nx. All channels use root-raised cosine (RRC) pulse-shaping filters g(\u00b7) and symbol rate 1/T . We assume \u2225g\u22252/T = 1, so the per-channel average transmit power is Ptx = \u03c32\nx. The central frequency of the k-th channel is \u03c9k/2\u03c0 where \u03c90 = 0.\n\nSignal propagation over an optical fiber using ideal distributed Raman amplification (IDRA) is described by the nonlinear Schr\u00a8odinger equation (NLSE) [9]\n\n$$\\frac{\\partial x(z,t)}{\\partial z}=-\\mathrm{j}\\frac{\\beta_{2}}{2}\\frac{\\partial^{2}x(z,t)}{\\partial t^{2}}+\\mathrm{j}\\gamma|x(z,t)|^{2}x(z,t)+n(z,t)\\tag{6}$$\nwhere \u03b22 is the dispersion coefficient, \u03b3 the nonlinearity coefficient and n(z, t) additive noise which is usually dominated by amplified spontaneous emission (ASE). The receiver accesses its COI via a bandpass filter with bandwidth 1/T . It then performs sampling, single-channel digital backpropagation (DBP), matched filtering using RRC filters, downsampling to the symbol rate, and mean phase rotation compensation [9]\nto obtain the sequence {yi}.\n"
    },
    {
        "level": "##",
        "title": "C. Cpan-Model",
        "content": "\nSurrogate models based on regular perturbation (RP) [21]\nsimplify computation and analysis. We use the CPAN model from [9] that has a phase noise channel\n\n$Y_{i}=X_{i}\\rm{e}^{i\\Theta_{i}}+N_{i}$\n\nwhere the transmit symbols {Xi} are independent and iden-\ntically distributed (i.i.d.). The additive noise process {Ni} is\nwhite and CSCG with p(ni) = NC\n                                 \ufffd\n                                  ni; 0, \u03c32\n                                        n\n                                         \ufffd\n                                          , and the phase\nnoise process {\u0398i} is a Markov chain with unit memory:\n\n$\\Theta_{i}=\\mu_{\\delta}\\Theta_{i-1}+\\sigma_{\\delta}\\Lambda_{i}$ (8)\n\nwhere {\u2206i} has i.i.d. real-valued, zero-mean, unit-variance,\nGaussian \u2206i. We refer to [9, Equ. (56)] and [9, Equ. (50)] on\nhow to choose \u00b5\u03b4 and \u03c3\u03b4. We set the memory of the CPAN\nmodel to 1 because, without a whitening filter, the AIR hardly\nincreases for larger memory. The additive and phase noise are\nindependent of the transmit string\n\n$\\mathbf{X}=[X_{1},X_{2},\\ldots,X_{n}]$. (9)\nUnlike the Wiener phase noise model [6], the variance of \u0398i does not increases in i, and we have\n\n$\\Theta_{i}\\sim{\\cal N}(0,\\sigma_{\\theta}^{2})$ for all $i$. (10)\n"
    },
    {
        "level": "##",
        "title": "Iii. Sic For Gaussian Inputs",
        "content": "\nConsider CSCG inputs with p(xi) = NC\n\ufffd\nxi; 0, \u03c32\nx\n\ufffd\n. SIC\nbridges the gap between AIRs for memoryless surrogate models [1] and AIRs for surrogate models with memory [9], [6]; see [19, Sec. IV]. For simplicity, we describe SIC with S = 2 stages and consider even n; generalizing to any number of stages is straightforward.\n\nThe transmit vector x of dimension n is divided into two vectors a and b of dimension n/2 in the manner\n\n$$\\mathbf{x}=[a_{1},b_{1},a_{2},b_{2},\\ldots a_{n/2},b_{n/2}].\\tag{11}$$\nFor the receive vector y, a SIC decoder works in two stages:\n1) Decode a symbol-wise using the APPs p(ai|y) for all i.\n2) Decode b symbol-wise using the APPs p(bi|y, a) for all\ni.\nNote that the decoder receives APPs from the detector without\nany inter-symbol dependencies, i.e., p(ai|y) is independent of\nthe ak with k \u0338= i.\nAn AIR for the first stage with independent signaling is\n$$I_{1}(\\mathbf{A};\\mathbf{Y})=\\frac{1}{n/2}\\sum_{i=1}^{n/2}h(A_{i})-h(A_{i}|\\mathbf{Y})\\leq I(\\mathbf{A};\\mathbf{Y})\\tag{12}$$\nwhere we used h(Ai|Y ) \u2265 h(Ai|Y , A1, . . . , Ai\u22121). For the second stage, an AIR is\n\n$$I_{2}(\\mathbf{B};\\mathbf{Y}|\\mathbf{A})=\\frac{1}{n/2}\\sum_{i=1}^{n/2}h(B_{i})-h(B_{i}|\\mathbf{Y},\\mathbf{A})\\leq I(\\mathbf{B};\\mathbf{Y}|\\mathbf{A})\\tag{13}$$\n\nwhere we used $h(B_{i}|\\mathbf{Y},\\mathbf{A})\\geq h(B_{i}|\\mathbf{Y},\\mathbf{A},B_{1},\\ldots,B_{i-1})$. An AIR for SIC is the average of $I_{1}$ and $I_{2}$:\n\n$$I_{\\rm sic}(\\mathbf{X};\\mathbf{Y})=\\frac{1}{2}\\Big{(}I_{1}(\\mathbf{A};\\mathbf{Y})+I_{2}(\\mathbf{B};\\mathbf{Y}|\\mathbf{A})\\Big{)}\\leq I(\\mathbf{X};\\mathbf{Y}).\\tag{14}$$\n"
    },
    {
        "level": "##",
        "title": "A. Surrogate App Based On The Cpan Model",
        "content": "\nThe detector wishes to compute p(ai|y) and p(bi|y, a).\n\nHowever, the true pdfs are unavailable, and we therefore use the surrogate probability\n\n$$q(\\mathbf{x},\\mathbf{y},\\mathbf{\\theta})=p(\\mathbf{x})p(\\mathbf{\\theta})q(\\mathbf{y}|\\mathbf{x},\\mathbf{\\theta})\\tag{15}$$ $$=\\prod_{i=1}^{n}p(x_{i})p(\\theta_{i}|\\theta_{i-1})q(y_{i}|x_{i},\\theta_{i})$$\nwith\n\n$$p(x_{i})=\\mathcal{N}_{\\mathbb{C}}\\left(x_{i};0,\\sigma_{x}^{2}\\right)\\tag{16}$$ $$p(\\theta_{i}|\\theta_{i-1})=\\mathcal{N}\\left(\\theta_{i};\\mu_{\\theta}\\theta_{i-1},\\sigma_{\\delta}^{2}\\right)$$ (17) $$p(\\theta_{1})=\\mathcal{N}\\left(\\theta_{1};0,\\sigma_{\\theta}^{2}\\right)$$ (18) $$q(y_{i}|x_{i},\\theta_{i})=\\mathcal{N}_{\\mathbb{C}}\\left(y_{i};x_{i}\\mathrm{e}^{\\mathrm{i}\\theta_{i}},\\sigma_{n}^{2}\\right)\\tag{19}$$\n\nwhere we slightly abused notation for clarity.\n\n \u2212\u2192\u03b7 \u03b8\u2032\u2032 i \u2212\u2192\u03b7 \u03b8i . . . . . . p(\u03b8i|\u03b8i\u22121) = p(\u03b8i+1|\u03b8i) \u03b8i \u03b8\u2032\u2032 i \u2190\u2212\u03b7 \u03b8\u2032\u2032 i \u2190\u2212\u03b7 \u03b8i \u03b8\u2032 i \u2190\u2212\u03b7 \u03b8\u2032 i \u2212\u2192\u03b7 \u03b8\u2032 i q(yi|xi, \u03b8i) xi \u2212\u2192\u03b7 xi \u2190\u2212\u03b7 xi p(xi) Note that $x$ is a function of $a$ and $b$. The surrogate model allows to approximate $p(a_{i}|\\mathbf{y})$ and $p(b_{i}|\\mathbf{y},\\mathbf{a})$ by\n\n$$q(a_{i}|\\mathbf{y})=\\frac{1}{c_{1}}\\,\\int_{\\mathbb{R}^{n}}\\,\\int_{\\mathcal{A}\\setminus\\{i\\}}\\,q(\\mathbf{x},\\mathbf{y},\\mathbf{\\theta})\\,\\mathrm{d}\\mathbf{x}\\mathrm{d}\\mathbf{\\theta}\\tag{20}$$ $$q(b_{i}|\\mathbf{y},\\mathbf{a})=\\frac{1}{c_{2}}\\,\\int_{\\mathbb{R}^{n}}\\,\\int_{\\mathcal{B}_{\\mathbf{a}}^{\\setminus\\{i\\}}}\\,q(\\mathbf{x},\\mathbf{y},\\mathbf{\\theta})\\,\\mathrm{d}\\mathbf{x}\\mathrm{d}\\mathbf{\\theta}\\tag{21}$$\nwhere c1 and c2 are normalization factors and\n\n$$\\mathcal{A}^{\\backslash\\{i\\}}=\\{\\mathbf{x}\\in\\mathbb{C}^{n}:x_{2i-1}=a_{i}\\}\\tag{22}$$ $$\\mathcal{B}^{\\backslash\\{i\\}}_{\\mathbf{a}}=\\{\\mathbf{x}\\in\\mathbb{C}^{n}:x_{2i}=b_{i},[x_{1},x_{3},\\ldots,x_{n-1}]=\\mathbf{a}\\}\\,.\\tag{23}$$\nWe will marginalize q(x, y, \u03b8) in both SIC-stages, where the marginalized variables depend on the stage.\n"
    },
    {
        "level": "##",
        "title": "B. Efficient Computation Of The Marginal Distributions",
        "content": "\n  The sum-product algorithm (SPA) computes the desired\nmarginals; see [22], [23]. To illustrate the algorithm, we use\nfactor graphs with directed edges carrying messages. The\nmessage of edge e in the arrow direction is denoted \u2212\u2192\u03b7 e(.),\nand that in the opposite direction \u2190\u2212\u03b7 e(.). In general, messages\nare densities, but for simplicity, we approximate most densities\nby real-valued Gaussians. In this case, \u2212\u2192\u00b5 e denotes the mean\nand \u2212\u2192\n    \u03c3 2\n      e the variance of \u2212\u2192\u03b7 e(.), and likewise for \u2190\u2212\u03b7 e(.).\n  1) First Stage Detection: Fig. 1 depicts the branches of the\nfirst SIC-stage.\n\n**Upward Path:** The $X_{i}$ are circularly symmetric, i.e., we have the relation $p(x_{i})=p\\left(x_{i}{\\rm e}^{{\\rm i}\\theta}\\right)$ for all $\\theta$, which implies\n\n$$\\begin{split}\\overleftarrow{\\eta}_{\\,\\theta^{{}^{\\prime}}_{i}}(\\theta_{i})&=\\frac{1}{\\overleftarrow{\\mathbb{C}}_{\\,\\theta^{{}^{\\prime}}_{i}}}\\int_{\\mathbb{C}}p(x_{i})q(y_{i}|x_{i},\\theta_{i})\\,{\\rm d}x_{i}\\\\ &=\\frac{1}{\\leftarrow{\\mathbb{C}}_{\\,\\theta^{{}^{\\prime}}_{i}}}\\int_{\\mathbb{C}}p(x^{\\prime}_{i})q(y_{i}|x^{\\prime}_{i})\\,{\\rm d}x^{\\prime}_{i}\\ ={\\rm const}.\\end{split}\\tag{24}$$\n\nwhere x\u2032\n      i = xiej\u03b8i. By\n                    1\n                   \u2190\n                   \u2212\n                   c \u03b8\u2032\n                     i\n                      , and likewise for other messages,\n\nwe denote a constant that normalizes to a valid pdf.\n\nRightward Path: Using \u2212\u2192\u03b7 \u03b81(\u03b81) = p(\u03b81), we obtain\n\n$$\\begin{split}\\overrightarrow{\\eta}_{\\,\\theta_{2}}(\\theta_{2})&=\\frac{1}{\\overrightarrow{c}_{\\,\\theta_{2}}}\\int_{\\mathbb{R}}\\overrightarrow{\\eta}_{\\,\\theta_{1}}(\\theta_{1})\\overleftarrow{\\eta}_{\\,\\theta_{1}^{\\prime}}(\\theta_{1})p(\\theta_{2}|\\theta_{1})\\,\\mathrm{d}\\theta_{1}\\\\ &=\\int_{\\mathbb{R}}p(\\theta_{1})p(\\theta_{2}|\\theta_{1})\\,\\mathrm{d}\\theta_{1}=p(\\theta_{2})\\end{split}\\tag{25}$$\nand recursively \u2212\u2192\u03b7 \u03b8i(\u03b8i) = p(\u03b8i) for all i.\n\nLeftward Path: We similarly have\n\n$$\\begin{split}\\overleftarrow{\\eta}\\,_{\\theta^{\\prime\\prime}_{n-1}}(\\theta_{n-1})&=\\frac{1}{\\overline{c}\\,_{\\theta^{\\prime\\prime}_{n-1}}}\\,\\int_{\\mathbb{R}}\\,\\overleftarrow{\\eta}\\,_{\\theta^{\\prime}_{n}}(\\theta_{n})p(\\theta_{n}|\\theta_{n-1})\\mathrm{d}\\theta_{n}\\\\ &=\\mathrm{const.}\\end{split}\\tag{26}$$\nand recursively \u2190\u2212\u03b7 \u03b8\u2032\u2032\ni (\u03b8i) is constant in \u03b8i.\n\nDownward Path: We have\n\n$$\\dot{\\eta}_{\\,\\theta^{\\prime}_{i}}(\\theta_{i})=\\frac{1}{\\dot{\\overline{c}}_{\\,\\theta^{\\prime}_{i}}}\\dot{\\eta}_{\\,\\theta_{i}}(\\theta_{i})\\dot{\\overline{\\eta}}_{\\,\\theta^{\\prime\\prime}_{i}}(\\theta_{i})=p(\\theta_{i})\\tag{27}$$\nand\n\n$$\\overleftarrow{\\eta}_{\\,x_{i}}(x_{i})=\\frac{1}{\\overleftarrow{C}_{\\,x_{i}}}\\,\\int_{\\mathbb{R}}\\overrightarrow{\\eta}_{\\,\\theta^{{}^{\\prime}}_{i}}(\\theta_{i})q(y_{i}|x_{i},\\theta_{i})\\mathrm{d}\\theta_{i}$$ $$=\\frac{1}{\\overleftarrow{C}_{\\,x_{i}}}\\,\\int_{\\mathbb{R}}\\mathcal{N}\\left(\\theta_{i};0,\\sigma_{\\theta}^{2}\\right)\\mathcal{N}_{\\mathbb{C}}\\left(y_{i};x_{i}\\mathrm{e}^{\\mathrm{i}\\theta_{i}},\\sigma_{n}^{2}\\right)\\mathrm{d}\\theta_{i}.\\tag{28}$$\n\nThe surrogate APP $q(x_{i}|\\boldsymbol{y})$ may now be calculated using\n\n$$f_{i}(x_{i})=\\frac{1}{cf_{i}}\\overrightarrow{\\eta}_{\\,x_{i}}(x_{i})\\overleftarrow{\\eta}_{\\,x_{i}}(x_{i})\\tag{29}$$\n\nwhere cfi normalizes fi to a valid pdf.\n  We approximate fi by a complex Gaussian density with\nmean \u00b5fi = Efi[X], variance \u03c32\n                            fi = Efi\n                                    \ufffd\n                                     |X \u2212 \u00b5fi|2\ufffd\n                                                and\npseudo-variance p2\n               fi = Efi\n                      \ufffd\n                       (X \u2212 \u00b5fi)2\ufffd\n                                 . As derived in App.\nA, we thus have\n\n$$q(x_{i}|\\mathbf{y})={\\cal N}_{\\mathbb{C}}\\left(x_{i};\\mu_{f_{i}},\\sigma_{f_{i}}^{2},p_{f_{i}}^{2}\\right)\\tag{30}$$\nwith\n\n$$\\mu_{f_{i}}=y_{i}\\frac{\\sigma_{x}^{2}}{\\sigma_{y}^{2}}\\exp\\left(-\\frac{\\sigma_{\\theta}^{2}}{2}\\right)\\tag{31}$$ $$\\sigma_{f_{i}}^{2}=\\frac{\\sigma_{x}^{2}}{\\sigma_{y}^{2}}\\left(\\sigma_{n}^{2}+\\left|y_{i}\\right|^{2}\\frac{\\sigma_{x}^{2}}{\\sigma_{y}^{2}}\\right)-\\left|\\mu_{f_{i}}\\right|^{2}$$ (32) $$p_{f_{i}}^{2}=y_{i}^{2}\\frac{\\sigma_{x}^{4}}{\\sigma_{y}^{4}}\\exp\\left(-2\\sigma_{\\theta}^{2}\\right)-\\mu_{f_{i}}^{2}\\tag{33}$$\n\nwhere $\\sigma_{y}^{2}=\\sigma_{x}^{2}+\\sigma_{n}^{2}$. At this point, we are interested only in $q(x_{i}|\\mathbf{y})$ for odd $i$, as these are the symbols detected in the first SIC-stage.\n\n2) _Second Stage Detection_: In the second stage, the symbols in $\\mathbf{x}$ with an odd index $i$, namely those described by $\\mathbf{a}$, have been detected and decoded. Hence, branches of the form Fig. 1 and branches of the form Fig. 2 alternate. The former corresponds to the elements in $\\mathbf{b}$ or those with even index of $\\mathbf{x}$, respectively, and the latter to those in $\\mathbf{a}$ or odd index of $\\mathbf{x}$.\n\n $\\overrightarrow{\\eta}\\,\\theta_{i}$$\\overrightarrow{\\eta}\\,\\theta_{i}^{\\prime\\prime}$$\\overrightarrow{\\eta}\\,\\theta_{i}^{\\prime\\prime}$$\\overrightarrow{\\eta}\\,\\theta_{i}^{\\prime\\prime}$$\\overrightarrow{\\eta}\\,\\theta_{i}^{\\prime\\prime}$$\\overrightarrow{\\eta}\\,\\theta_{i}^{\\prime\\prime}$$\\overrightarrow{\\eta}\\,\\theta_{i}^{\\prime\\prime}$$\\overrightarrow{\\eta}\\,\\theta_{i}^{\\prime\\prime}$$\\overrightarrow{\\eta}\\,\\theta_{i}^{\\prime\\prime}$$\\overrightarrow{\\eta}\\,\\theta_{i}^{\\prime}$$\\overrightarrow{\\eta}\\,\\theta_{i}^{\\prime}$$\\overrightarrow{\\eta}\\,\\theta_{i}^{\\prime}$$\\overrightarrow{\\eta}\\,\\theta_{i}^{\\prime}$$\\overrightarrow{\\eta}\\,\\theta_{i}^{\\prime}$$\\overrightarrow{\\eta}\\,\\theta_{i}^{\\prime}$$\\overrightarrow{\\eta}\\,\\theta_{i}^{\\prime}$$\\overrightarrow{\\eta}\\,\\theta_{i}^{\\prime}$$\\overrightarrow{\\eta}\\,\\theta_{i}^{\\prime}$$\\overrightarrow{\\eta}\\,\\theta_{i}^{\\prime}$$\\overrightarrow{\\eta}\\,\\theta_{i}^{\\prime}$\\(\\overrightarrow{\\eta\nUpward Path: For odd i, the message passed over \u03b8\u2032\ni is\n\n\u2190\u2212\u03b7 \u03b8\u2032 i(\u03b8i) = 1 \u2190\u2212c \u03b8\u2032 i p(xi)q(yi|xi, \u03b8i) (34) = 1 \u2212 p(xi) \u03c0\u03c32n exp \u03c32n \u2190\u2212c \u03b8\u2032 i \ufffd \ufffd \ufffd\ufffdyi \u2212 xiej\u03b8i\ufffd\ufffd2 \u221d exp \ufffd2|yi||xi| \u03c32n cos \ufffd \u03b8i \u2212 (\u2220yi \u2212 \u2220xi) \ufffd\ufffd .\n\nThis message consists of periodic repetitions of pulses, each\nsimilar to a Gaussian with mean \u2220yi \u2212 \u2220xi + 2\u03c0k for k \u2208 N.\nAs we show later on, messages on the left- and rightward\npaths, i.e., \u2212\u2192\u03b7 \u03b8i and \u2190\u2212\u03b7 \u2032\u2032\n                      \u03b8i, are Gaussians with near-zero mean\nand rapidly decaying tails. As products of these messages\nare passed on, we may focus on the period closest to zero\nby considering m (\u2220yi \u2212 \u2220xi) which maps \u2220yi \u2212 \u2220xi to the\ninterval [\u2212\u03c0, \u03c0). Using the approximation cos(\u03b3) \u2248 1 \u2212 \u03b32/2,\nwhich is valid for small values of \u03b3, we obtain\n\n\u2190\u2212\u03b7 \u03b8\u2032 i(\u03b8i) \u2248 N \ufffd \u03b8i; \u2190\u2212 \u00b5 \u03b8\u2032 i, \u2190\u2212 \u03c3 2 \u03b8\u2032 i \ufffd (35) \u2190\u2212 \u00b5 \u03b8\u2032 i = m (\u2220yi \u2212 \u2220xi) (36) \u2190\u2212 \u03c3 2 \u03b8\u2032 i = \u03c32 n 2|yi||xi|. (37) If i is even, as before, then \u2190\u2212\u03b7 \u03b8\u2032 i(\u03b8i) is constant in \u03b8i. Rightward Path: We show that all messages in the rightward path are approximately Gaussian, that is \u2212\u2192\u03b7 \u03b8i(\u03b8i) \u2248 N \ufffd \u03b8i; \u2212\u2192 \u00b5 \u03b8i, \u2212\u2192 \u03c3 2 \u03b8i \ufffd (38) \u2212\u2192\u03b7 \u03b8\u2032\u2032 i (\u03b8i) \u2248 N \ufffd \u03b8i; \u2212\u2192\u00b5 \u03b8\u2032\u2032 i , \u2212\u2192\u03c3 2 \u03b8\u2032\u2032 i \ufffd . (39)\n\nIf \u2212\u2192\u03b7 \u03b8i is Gaussian, then \u2212\u2192\u03b7 \u03b8\u2032\u2032\n                                  i is either a product of Gaussians\nor a product of a Gaussian and a constant, and hence Gaussian\n[24]. Explicitly, the parameters of \u2212\u2192\u03b7 \u03b8\u2032\u2032\n                                             i depend on i as follows.\n\n- If i is odd, then xi was already decoded in the first stage\nand is a branch of the form shown in Fig. 2. Hence \u2212\u2192\n\u00b5 \u03b8\u2032\u2032\ni\nis a product of Gaussians and [24]\n$$\\overrightarrow{\\mu}_{\\theta_{i}}\\stackrel{{\\leftarrow}}{{\\sigma}}\\stackrel{{2}}{{\\theta}}_{i}+\\overleftarrow{\\mu}_{\\theta_{i}}\\stackrel{{\\rightarrow}}{{\\sigma}}\\stackrel{{2}}{{\\theta}}_{i}\\tag{40}$$ $$\\overrightarrow{\\sigma}\\stackrel{{2}}{{\\theta}}_{i}\\stackrel{{\\leftarrow}}{{\\sigma}}\\stackrel{{2}}{{\\theta}}_{i}$$ (41) $$\\overrightarrow{\\sigma}\\stackrel{{2}}{{\\theta}}_{i}\\stackrel{{\\leftarrow}}{{\\sigma}}\\stackrel{{2}}{{\\theta}}_{i}\\stackrel{{\\leftarrow}}{{\\theta}}_{i}\\stackrel{{\\leftarrow}}{{\\theta}}_{i}\\stackrel{{\\leftarrow}}{{\\theta}}_{i}\\stackrel{{\\leftarrow}}{{\\theta}}_{i}\\stackrel{{\\leftarrow}}{{\\theta}}_{i}\\stackrel{{\\leftarrow}}{{\\theta}}_{i}\\stackrel{{\\leftarrow}}{{\\theta}}_{i}\\stackrel{{\\leftarrow}}{{\\theta}}_{i}\\stackrel{{\\leftarrow}}{{\\theta}}_{i}\\stackrel{{\\leftarrow}}{{\\theta}}_{i}\\stackrel{{\\leftarrow}}{{\\theta}}_{i}\\stackrel{{\\leftarrow}}{{\\theta}}_{i}\\stackrel{{\\leftarrow}}{{\\theta}}_{i}\\stackrel{{\\leftarrow}}{{\\theta}}_{i}\\stackrel{{\\leftarrow}}{{\\theta}}_{i}\\stackrel{{\\leftarrow}}{{\\theta}}_{i}\\stackrel{{\\leftarrow}}{{\\theta}}_{i}\\stackrel{{\\leftarrow}}{{\\theta}}_{i}\\stackrel{{\\leftarrow}}{{\\theta}}_{i}\\stackrel{{\\leftarrow}}{{\\theta}}_{i}\\stackrel{{\\leftarrow}}{{\\theta}}_{i}\\stackrel{{\\leftarrow}}{{\\theta}}_{i}\\stackrel{{\\leftarrow}}{{\\theta}}_{i}\\stackrel{{\\leftarrow}}{{\\theta}}_{i}\\stackrel{{\\leftarrow}}{{\\theta}}_{i}\\stackrel{{\\leftarrow}}{{\\theta}}_{i}\\stackrel{{\\leftarrow}}{{\\theta}}_{i}\\stackrel{{\\leftarrow}}{{\\theta}}_{i}\\stackrel{{\\leftarrow}}\\stackrel{{{\\leftarrow}}}\\tag\n- If i is even, then xi was not decoded in the first stage\nand is a branch of the form shown in Fig. 1. Hence, \u2212\u2192\u03b7 \u03b8\u2032\u2032\ni\n\nis a product of a Gaussian and a constant, and therefore\n\u2212\u2192\n\u00b5 \u03b8\u2032\u2032\n   i = \u2212\u2192\n        \u00b5 \u03b8i and \u2212\u2192\n                 \u03c3 2\n                   \u03b8\u2032\u2032\n                    i = \u2212\u2192\n                         \u03c3 2\n                           \u03b8i.\n\nIf $\\overrightarrow{\\eta}_{\\theta_{i}^{\\prime\\prime}-1}$ is Gaussian, then $\\overrightarrow{\\eta}_{\\theta_{i}}$ is the marginalization over the product of a Gaussian and a conditional Gaussian. We have\n\n$$\\begin{split}\\int_{\\mathbb{R}}\\mathcal{N}\\left(\\theta_{i-1};\\mu,\\sigma^{2}\\right)&\\mathcal{N}\\left(\\theta_{i};\\mu_{\\delta}\\theta_{i-1},\\sigma_{\\delta}^{2}\\right)\\mathrm{d}\\theta_{i-1}\\\\ &=\\mathcal{N}\\left(\\theta_{i};\\mu_{\\delta}\\mu,\\mu_{\\delta}^{2}\\sigma^{2}+\\sigma_{\\delta}^{2}\\right)\\end{split}\\tag{42}$$\nfrom which we obtain\n\n= N \ufffd \u03b8i; \u00b5\u03b4\u00b5, \u00b52 \u03b4\u03c32 + \u03c32 \u03b4 \ufffd (42) R \u2212\u2192\u03b7 \u03b8\u2032\u2032 i\u22121(\u03b8i\u22121)p(\u03b8i|\u03b8i\u22121)d\u03b8i\u22121 \u2212\u2192\u03b7 \u03b8i(\u03b8i) = \ufffd \u2248 N \ufffd \u03b8i; \u00b5\u03b4\u2212\u2192 \u00b5 \u03b8\u2032\u2032 i\u22121, \u00b52 \u03b4\u2212\u2192\u03c3 2 \u03b8\u2032\u2032 i\u22121 + \u03c32 \u03b4 \ufffd . (43)\n\nWith \u2212\u2192\u03b7 \u03b81(\u03b81) = p(\u03b81) = N\n                          \ufffd\n                           \u03b81; 0, \u03c32\n                                 \u03b8\n                                  \ufffd\n                                    for any stage, we\narrive at (38)\u2013(39) by induction.\nLeftward Path: Denote by i\u2032 the largest index of all symbols\ndecoded in earlier stages. In the second of two stages, i\u2032 =\nn\u22121 if n is even and i\u2032 = n else. All branches to the right of\nthe i\u2032-th branch are of the form shown in Fig. 1 and therefore\n\u2190\u2212\u03b7 \u03b8\u2032\u2032\n  i\u2032(\u03b8i\u2032) is constant in \u03b8i\u2032. Therefore, we find that (note the\ndifferent subscripts)\n                \u2190\u2212\u03b7 \u03b8i\u2032 (\u03b8i\u2032) = \u2190\u2212\u03b7 \u03b8\u2032\n                             i\u2032 (\u03b8i\u2032)\n                                                (44)\n\nis approximately Gaussian, see (35). This is also true for i\u2032 =\nn. If \u2190\u2212\u03b7 \u03b8i+1 is Gaussian in \u03b8i+1, then we update\n\nR \u2190\u2212\u03b7 \u03b8i+1(\u03b8i+1)p(\u03b8i+1|\u03b8i)d\u03b8i+1 \u2190\u2212\u03b7 \u03b8\u2032\u2032 i (\u03b8i) = \ufffd . (45) \u03b8i; \u2190\u2212\u00b5 \u03b8i+1 \u2248 N \u00b5\u03b4 , \u2190\u2212\u03c3 2 \u03b8i+1 + \u03c32 \u03b4 \u00b52 \u03b4 \ufffd \ufffd\nwhere the update rule depends on the index i.\n\nSimilar to the rightward path, for i \u2264 i\u2032, we have \u2190\u2212\u03b7 \u03b8i(\u03b8i) \u2248 N \ufffd \u03b8i; \u2190\u2212 \u00b5 \u03b8i, \u2190\u2212 \u03c3 2 \u03b8i \ufffd (46)\n- If i is odd, then\n\n$$\\overleftarrow{\\mu}_{\\,\\theta_{i}}=\\frac{\\overleftarrow{\\mu}_{\\,\\theta_{i}^{\\prime\\prime}}\\overleftarrow{\\sigma}_{\\,\\theta_{i}^{\\prime}}^{2}+\\overleftarrow{\\mu}_{\\,\\theta_{i}^{\\prime}}\\overleftarrow{\\sigma}_{\\,\\theta_{i}^{\\prime\\prime}}^{2}}{\\overleftarrow{\\sigma}_{\\,\\theta_{i}^{\\prime}}^{2}+\\overleftarrow{\\sigma}_{\\,\\theta_{i}^{\\prime\\prime}}^{2}}\\tag{47}$$ $$\\overleftarrow{\\sigma}_{\\,\\theta_{i}}^{2}=\\frac{\\overleftarrow{\\sigma}_{\\,\\theta_{i}^{\\prime}}^{2}\\overleftarrow{\\sigma}_{\\,\\theta_{i}^{\\prime\\prime}}^{2}}{\\overleftarrow{\\sigma}_{\\,\\theta_{i}^{\\prime}}^{2}+\\overleftarrow{\\sigma}_{\\,\\theta_{i}^{\\prime\\prime}}^{2}}.\\tag{48}$$\n* If $i$ is even, then $\\overleftarrow{\\mu}_{\\,\\theta_{i}}=\\overleftarrow{\\mu}_{\\,\\theta_{i}^{\\prime\\prime}}$ and $\\overleftarrow{\\sigma}_{\\,\\theta_{i}}^{2}=\\overleftarrow{\\sigma}_{\\,\\theta_{i}^{\\prime\\prime}}^{2}$.\n\nDownward Path: As both \u2212\u2192\u03b7 \u03b8i(\u03b8i) and \u2190\u2212\u03b7 \u03b8\u2032\u2032\n                                 i (\u03b8i) are Gaussian\nin \u03b8i, their product is also Gaussian. That is, we have\n\n$$\\begin{split}\\overrightarrow{\\eta}_{\\,\\theta^{\\prime}_{i}}(\\theta_{i})&=\\frac{1}{\\overrightarrow{\\epsilon}_{\\,\\theta^{\\prime}_{i}}}\\overrightarrow{\\eta}_{\\,\\theta_{i}}(\\theta_{i})\\overleftarrow{\\eta}_{\\,\\theta^{\\prime\\prime}_{i}}(\\theta_{i})\\\\ &\\approx\\mathcal{N}\\left(\\theta_{i};\\overrightarrow{\\mu}_{\\,\\theta^{\\prime}_{i}},\\overrightarrow{\\sigma}^{2}_{\\,\\theta^{\\prime}_{i}}\\right)\\end{split}\\tag{49}$$\n\nwith\n\n$$\\overrightarrow{\\mu}_{\\,\\theta^{\\prime}_{i}}=\\frac{\\overrightarrow{\\mu}_{\\,\\theta_{i}}\\overrightarrow{\\sigma}^{2}_{\\,\\theta^{\\prime\\prime}}+\\overrightarrow{\\mu}_{\\,\\theta^{\\prime\\prime}_{i}}\\overrightarrow{\\sigma}^{2}_{\\,\\theta_{i}}}{\\overrightarrow{\\sigma}^{2}_{\\,\\theta_{i}}+\\overrightarrow{\\sigma}^{2}_{\\,\\theta^{\\prime\\prime}_{i}}}\\tag{50}$$ $$\\overrightarrow{\\sigma}^{2}_{\\,\\theta^{\\prime}_{i}}=\\frac{\\overrightarrow{\\sigma}^{2}_{\\,\\theta_{i}}\\overrightarrow{\\sigma}^{2}_{\\,\\theta^{\\prime\\prime}_{i}}}{\\overrightarrow{\\sigma}^{2}_{\\,\\theta_{i}}+\\overrightarrow{\\sigma}^{2}_{\\,\\theta^{\\prime\\prime}_{i}}}.\\tag{51}$$\nSimilar to the first stage, for even i we approximate q(xi|y, a)\nby a complex Gaussian. Simulations show that using CSCGs suffices, and the mean and variance are (see App. A)\n\n$$\\mu_{f_{i}}=y_{i}\\frac{\\sigma_{x}^{2}}{\\sigma_{y}^{2}}\\exp\\left(-\\frac{1}{2}\\frac{\\overrightarrow{\\mu}_{\\theta_{i}^{\\prime}}^{2}-(\\overrightarrow{\\mu}_{\\theta_{i}^{\\prime}}-\\mathrm{j}\\overrightarrow{\\sigma}_{\\theta_{i}^{\\prime}}^{2})^{2}}{\\overrightarrow{\\sigma}_{\\theta_{i}^{\\prime}}^{2}}\\right)\\tag{52}$$ $$\\sigma_{f_{i}}^{2}=\\frac{\\sigma_{x}^{2}}{\\sigma_{y}^{2}}\\left(\\sigma_{n}^{2}+|y_{i}|^{2}\\frac{\\sigma_{x}^{2}}{\\sigma_{y}^{2}}\\right)-|\\mu_{f_{i}}|^{2}\\,.\\tag{53}$$\n"
    },
    {
        "level": "##",
        "title": "C. Extension To S Sic-Stages",
        "content": "\nAn extension to S stages is straightforward. The first stage can be detected as described by (30)\u2013(33). For stage s > 1, all xi corresponding to stages s\u2032 < s are assumed to be known.\n\nAlso, the following means and variances should be calculated beforehand for appropriate indices i:\n\n$$\\overleftarrow{\\mu}_{\\theta^{\\prime}_{i}}=m\\left(\\angle_{y_{i}}-\\angle_{x_{i}}\\right),\\quad\\overleftarrow{\\sigma}_{\\theta^{\\prime}_{i}}^{2}=\\frac{\\sigma_{n}^{2}}{2|y_{i}||x_{i}|}.\\tag{54}$$\n\nFor Gaussian messages, we collect the mean and variance in one vector\n\n$$\\overrightarrow{\\boldsymbol{\\eta}}_{\\theta_{i}}=\\left[\\overrightarrow{\\mu}_{\\theta_{i}},\\overrightarrow{\\sigma}_{\\theta_{i}}^{2}\\right]\\tag{55}$$\n\nand likewise for other messages. We also define the function\n\n$$g(\\boldsymbol{\\eta}_{1},\\boldsymbol{\\eta}_{2})=\\left[\\frac{\\mu_{1}\\sigma_{2}^{2}+\\mu_{2}\\sigma_{1}^{2}}{\\sigma_{1}^{2}+\\sigma_{2}^{2}},\\frac{\\sigma_{1}^{2}\\sigma_{2}^{2}}{\\sigma_{1}^{2}+\\sigma_{2}^{2}}\\right]\\tag{56}$$\n\nwhich describes the mean and variance of the product of\nGaussians with parameters \u03b71 and \u03b72.\n  Algorithm 1 shows the computations for stage s. The set\nIs has the symbol indices decoded in earlier stages, e.g., for\nS = 2 that is I1 = \u2205 and I2 = {1, 3, . . ., n \u2212 1}. We have\n\u2212\u2192\n\u03b7 \u03b81 = [0, \u03c32\n          \u03b8]. For i\u2032 = n \u2212 S + (s \u2212 1), which is the index\nof the last symbol in x decoded prior to stage s > 1, we have\n\n$$\\overline{\\eta}_{\\,\\theta_{i^{\\prime}}}=\\left[m\\left(\\angle_{y_{i^{\\prime}}}-\\angle_{x_{i^{\\prime}}}\\right),\\frac{\\sigma_{n}^{2}}{2|y_{i^{\\prime}}||x_{i^{\\prime}}|}\\right]\\tag{57}$$\nand (see (45))\n\n$$\\overleftarrow{\\eta}_{\\,\\theta^{\\prime\\prime}_{i^{\\prime}}-1}=\\left[\\frac{m\\left(\\angle_{y_{i^{\\prime}}}-\\angle_{x_{i^{\\prime}}}\\right)}{\\mu_{\\delta}},\\,\\frac{\\sigma_{n}^{2}}{2|y_{i^{\\prime}}|\\,|x_{i^{\\prime}}|}+\\sigma_{\\delta}^{2}\\right].\\tag{58}$$\n\nLet $\\mathbf{x}^{(s)}$ be the symbols decoded in stage $s$, i.e., for two stages $\\mathbf{a}=\\mathbf{x}^{(1)}$ and $\\mathbf{b}=\\mathbf{x}^{(2)}$. For $i\\in\\{s,s+S,s+2S,\\ldots\\}$, we have\n\n$$q(x_{i}|\\mathbf{y},\\mathbf{x}^{(1)},\\ldots,\\mathbf{x}^{(s-1)})=\\mathcal{N}_{\\mathbb{C}}\\left(x_{i};\\mu_{f_{i}},\\sigma_{f_{i}}^{2}\\right)\\tag{59}$$\n\nwhere $\\mu_{f_{i}}$ and $\\sigma_{f_{i}}^{2}$ can be calculated from the output of algorithm 1 and (52)-(53).\n\nNote that our Gaussian approximate message passing algorithm uses messages that are vectors of dimension two with a mean and variance. Also, the calculations for the $\\overleftarrow{\\eta}_{\\,\\theta^{\\prime}_{i}}$, $\\overrightarrow{\\eta}_{\\,\\theta^{\\prime}_{i}}$, $\\mu_{f_{i}}$, and $\\sigma_{f_{i}}^{2}$ can be parallelized.\n\n Input: y, x(1), . . . , x(s\u22121), Is, s, S, n, i\u2032,\u2212\u2192 \u03b7 \u03b81, \u2190\u2212 \u03b7 \u03b8\u2032\u2032 i\u2032\u22121, \u2190\u2212 \u03b7 \u03b8\u2032 i for i \u2208 Is Output: \u2212\u2192 \u03b7 \u03b8\u2032 i \u22b2 Rightward Path for i \u2190 1 to n \u2212 1 do if i \u2208 Is then \u2212\u2192 \u03b7 \u03b8\u2032\u2032 i \u2190 g(\u2212\u2192 \u03b7 \u03b8i, \u2190\u2212 \u03b7 \u03b8\u2032 i) else\u2212\u2192 \u03b7 \u03b8\u2032\u2032 i \u2190 \u2212\u2192 \u03b7 \u03b8i end if \u2212\u2192 \u03b7 \u03b8i+1 \u2190 \ufffd \u00b5\u03b4\u2212\u2192\u00b5 \u03b8\u2032\u2032 i , \u00b52 \u03b4\u2212\u2192 \u03c3 2 \u03b8\u2032\u2032 i + \u03c32 \u03b4 \ufffd \u00b52 \u03b4 end for \u22b2 Leftward Path for i \u2190 i\u2032 \u2212 1 to 2 do if i \u2208 Is then \u2190\u2212 \u03b7 \u03b8i \u2190 g(\u2190\u2212 \u03b7 \u03b8\u2032\u2032 i , \u2190\u2212 \u03b7 \u03b8\u2032 i) else\u2190\u2212 \u03b7 \u03b8i \u2190 \u2190\u2212 \u03b7 \u03b8\u2032\u2032 i end if \u2190\u2212 \u03b7 \u03b8i\u22121\u2032\u2032 \u2190 \ufffd \u2190 \u2212 \u00b5 \u03b8i \u00b5\u03b4 , \u2190 \u2212 \u03c3 2 \u03b8i +\u03c32 \u03b4 \ufffd end for \u22b2 Downward Path for l \u2190 0 to \u230an/S\u230b \u2212 1 do i \u2190 s + lS \u2212\u2192 \u03b7 \u03b8\u2032 i \u2190 g(\u2212\u2192 \u03b7 \u03b8i, \u2190\u2212 \u03b7 \u03b8\u2032\u2032 i ) end for\n"
    },
    {
        "level": "##",
        "title": "D. Lower Bound On Mutual Information",
        "content": "\nWe lower bound I1(A; Y ) by using\n\n$$h_{q}(A_{i}|\\mathbf{Y})=-\\int p(\\mathbf{y})\\int p(a_{i}|\\mathbf{y})\\log q(a_{i}|\\mathbf{y})\\mathrm{d}a_{i}\\mathrm{d}\\mathbf{y}\\tag{60}$$ $$=h(A_{i}|\\mathbf{Y})+D\\left(p(A_{i}|\\mathbf{Y})||q(A_{i}|\\mathbf{Y})\\right)$$ $$\\geq h(A_{i}|\\mathbf{Y})$$\nwhere D(\u00b7\u2225\u00b7) is informational divergence that is non-negative.\n\nWe thus have\n\n$$I_{1,q}(\\mathbf{A};\\mathbf{Y})=\\frac{1}{n/2}\\sum_{i=1}^{n/2}h(A_{i})-h_{q}(A_{i}|\\mathbf{Y})\\leq I_{1}(\\mathbf{A};\\mathbf{Y}).\\tag{61}$$\nLikewise, we have\n\n$$I_{2,q}(\\mathbf{B};\\mathbf{Y}|\\mathbf{A}):=\\frac{1}{n/2}\\sum_{i=1}^{n/2}h(B_{i})-h_{q}(B_{i}|\\mathbf{Y},\\mathbf{A})\\leq I_{2}(\\mathbf{B};\\mathbf{Y}|\\mathbf{A})\\tag{62}$$\n\nwith\n\n$$\\begin{split}h_{q}(B_{i}|\\mathbf{Y},\\mathbf{A})&=-\\int\\int p(\\mathbf{y},\\mathbf{a})\\int p(b_{i}|\\mathbf{y},\\mathbf{a})\\\\ &\\cdot\\log q(b_{i}|\\mathbf{y},\\mathbf{a})\\,\\mathrm{d}b_{i}\\mathrm{d}\\mathbf{y}\\mathrm{d}\\mathbf{a}.\\end{split}\\tag{63}$$\nAs we use i.i.d. CSCG inputs with variance \u03c32\nx, we have\n\n$h(A_{i})=h(B_{i})=\\log(\\pi\\mbox{e}\\sigma_{x}^{2})$. (64)\nWe approximate (60) and (63) by simulating transmission of Nseq sequences {xk} and {yk} and compute\n\n$$h_{q}(A_{i}|\\mathbf{Y})\\approx-\\frac{1}{N_{\\rm seq}}\\sum_{k=1}^{N_{\\rm seq}}\\log q(a_{k,i}|\\mathbf{y}_{k})\\tag{65}$$ $$h_{q}(B_{i}|\\mathbf{Y},\\mathbf{A})\\approx-\\frac{1}{N_{\\rm seq}}\\sum_{k=1}^{N_{\\rm seq}}\\log q(b_{k,i}|\\mathbf{y}_{k},\\mathbf{a}_{k}).\\tag{66}$$\n"
    },
    {
        "level": "##",
        "title": "E. Simulation Results",
        "content": "\n  Table I lists the simulation parameters; see [9, Sec. VIII].\nHowever, the receiver does not use a whitening filter and uses a\nunit-memory surrogate channel. We use 24 sequences of 8192\nsymbols each for training, e.g., to obtain \u00b5\u03b4 and \u03c32\n                                                   \u03b4, and 120\nsequences of 8192 symbols each for testing, i.e., Nseq = 120.\n  We first investigate the AIRs of the CPAN channel with\nnoise variances that mimic those of the nonlinear fiber-optic\nchannel. Fig. 3 plots the variances \u03c32\n                                     \u03b8, \u03c32\n                                         \u03b4 of the phase noise\nprocess, and the variance \u03c32\n                            n of the AWGN. These variances\nincrease with Ptx due to the nonlinear interference. In contrast,\nthe variance \u03c32\n              ASE of the ASE is constant at approximately\n2.95 \u00b7 10\u22127.\n  Fig. 4a show the AIRs for the following benchmarking\nscenarios:\n\n1\na memoryless AWGN surrogate model,\n2\na memoryless surrogate model with i.i.d. Gaussian phase noise and independent AWGN,\n3\na JDD-receiver based on particle filtering [6], [9], and\n\n 4\n   a genie-aided receiver with perfect knowledge of the\n   phase noise, so the AIR is I(X; Y |\u0398).\nThe solid black curve shows the AWGN channel capacity with\nASE only, which upper bounds the CPAN channel capacity.\n The inequality (14) shows that SIC cannot outperform\nJDD. SIC improves the memoryless receivers, and the mem-\noryless phase noise receiver AIR is the same as the SIC\nAIR with S = 1. As \u0398 and X are independent, we have\nI(X; Y |\u0398) \u2265 I(X; Y ). Thus, the genie-aided receiver with\nperfect knowledge of the phase noise has larger AIRs than the\nJDD receiver.\n Fig. 4a shows that SIC with 2 and 4 stages loses significant\nAIR compared to JDD. To maintain a rate loss of less than 1 %,\none needs at least 8 SIC-stages. The AIR of the SIC receiver\nwith 64 stages is very close to the AIR of the genie-aided\nreceiver with perfect knowledge of the phase noise process.\n\n| Parameter                        |\n|----------------------------------|\n| Fiber Length                     |\n| L                                |\n| 1000 km                          |\n| Attenuation coefficient          |\n| \u03b1                                |\n| 0.2 dB km                        |\n| \u22121                               |\n| Dispersion Coefficient           |\n| \u03b2                                |\n| 2                                |\n| \u221221.7 ps                         |\n| 2                                |\n|                                  |\n| km                               |\n| \u22121                               |\n| Nonlinear coefficient            |\n| \u03b3                                |\n| 1.27 W                           |\n| \u22121                               |\n|                                  |\n| km                               |\n| \u22121                               |\n| Phonon occupancy factor          |\n| \u03b7                                |\n| 1                                |\n| One-sided number of WDM channels |\n| b                                |\n| 2                                |\n\nWe infer that the proposed SIC receiver performs well for\nCPAN models. To further improve the rates for the nonlinear\nfiber-optic channel, one must improve the surrogate model,\ne.g., by considering correlations in the additive noise [6].\n  Studies of the dispersion-free nonlinear-fiber optic channel\nshow that derived models with zero dispersion have AIRs\nthat grow as 1\n\n             2 log(SNR) + O(1) where SNR \u221d Ptx; see [25],\n[26], [27], [28]. In contrast, the CPAN AIRs decrease with Ptx\nbecause the additive noise variance \u03c32\n                                     n increases with Ptx, see\nFig. 3. Both the phase and amplitude of the signal experience\ndistortions that increase with transmit power.\n  Fig. 4b shows the AIRs for the nonlinear fiber-optic chan-\nnel with a receiver that uses the CPAN surrogate model.\nThe solid curve again shows the capacity of the AWGN-\nchannel distorted by ASE only, which upper bounds the\ncapacity [29], [30]. We remark that the inequality in (14)\ndoes not hold for mismatched mutual information (MI), i.e.,\n1/2 (I1,q(A; Y ) + I2,q(B; Y |A)) might exceed Iq(X; Y )\nbased on JDD and particle filtering, as used in [9]. Second,\nthe AIRs of JDD are slightly smaller than those in [9], which\nis mostly due to the lack of a whitening filter.\n  We again see that 8 SIC-stages provide AIRs similar to those\nof JDD. However, for 16 or more SIC-stages, the AIR of SIC\nexceeds that of JDD. We infer that the channel description\nof the SIC channel better approximates the true channel than\nJDD does. The 64-stage SIC-receiver gains approximately\n0.52 bits per channel use (bpcu), or 6.4 %, in rate over the\nmemoryless AWGN receiver.\n"
    },
    {
        "level": "##",
        "title": "Iv. Sic For Ring Constellations",
        "content": "\nThis section studies ring constellations. The transmit symbols have an independent amplitude $R_{i}$ and phase $\\Gamma_{i}$. The amplitude is sampled from a discrete distribution in $\\mathcal{R}=\\{\\tilde{r}_{1},\\ldots,\\tilde{r}_{n_{v}}\\}$, and the phase is sampled from a continuous distribution in $[-\\pi,\\pi)$. We use the distributions\n\n$$P(\\tilde{r}_{i})=w_{i},\\quad p(\\gamma_{i})=\\frac{1}{2\\pi},\\text{for}\\gamma_{i}\\in[-\\pi,\\pi).\\tag{67}$$\nLike CSCG inputs, ring constellations are circularly symmetric, and therefore we have (see (24))\n\n$$\\overleftarrow{\\eta}\\,\\theta^{\\prime}_{i}(\\theta_{i})=\\frac{1}{\\overleftarrow{C}\\,\\theta^{\\prime}_{i}}\\,\\int_{\\mathbb{C}}p(x_{i})q(y_{i}|x_{i},\\theta_{i})\\mathrm{d}x_{i}=\\mathrm{const}.\\tag{68}$$\nMotivated by CSCG inputs, we use equidistant rings with\n\u02dcr\u2113 = \u2113 \u00b7 \u2206r and probabilities w\u2113 that model a Rayleigh Distribution with variance \u03c32\nx, i.e.,\n\n$$w_{\\ell}=\\frac{\\tilde{r}_{\\ell}\\exp\\left(-\\frac{\\tilde{r}_{\\ell}^{2}}{\\sigma_{x}^{2}}\\right)}{\\sum_{m=1}^{n_{r}}\\tilde{r}_{m}\\exp\\left(-\\frac{\\tilde{r}_{m}^{2}}{\\sigma_{x}^{2}}\\right)}.\\tag{69}$$\nWe name this constellation unidistant Rayleigh ring (URR). The transmit power is\n\n$$\\mathrm{E}[|X|^{2}]=\\sigma_{x}^{2}=\\Delta r^{2}\\frac{\\sum_{\\ell=1}^{n_{r}}\\ell^{3}\\exp\\left(-\\frac{\\ell^{2}\\Delta r^{2}}{\\sigma_{x}^{2}}\\right)}{\\sum_{\\ell=1}^{n_{r}}\\ell\\exp\\left(-\\frac{\\ell^{2}\\Delta r^{2}}{\\sigma_{x}^{2}}\\right)}\\tag{70}$$\n\nand we set $\\sigma_{x}^{2}=P_{\\mathrm{tx}}$. The $\\Delta r$, which satisfies the power constraint, is found numerically.\n\nURR constellations approximate a CSCG for large $n_{r}$. Fig. 5 shows the AIRs for memoryless AWGN channels with $\\sigma_{n}^{2}=2.95\\cdot10^{-7}$, which is approximately the ASE noise variance for the parameters in Table I. The horizontal line indicates the largest AIR of 2 SIC-stages in Fig. 4b, which is the peak value we attempt to reach. Observe that 32 rings are needed to prevent significant deviation from Gaussian inputs at the target AIR.\n"
    },
    {
        "level": "##",
        "title": "A. Mutual Information Estimation",
        "content": "\nSuppose X has independent amplitudes R and phases \u0393\nthat are transmitted through a channel pY |X; see Fig. 6. By\n10\n8\nAIR [bpcu]\n6\nthe chain rule of MI, we have\n\n$$I(\\mathbf{X};\\mathbf{Y})+\\underbrace{I(\\mathbf{R},\\mathbf{\\Gamma};\\mathbf{Y}|\\mathbf{X})}_{=0}=I(\\mathbf{R},\\mathbf{\\Gamma};\\mathbf{Y})+\\underbrace{I(\\mathbf{X};\\mathbf{Y}|\\mathbf{R},\\mathbf{\\Gamma})}_{=0}\\tag{71}$$\n\nand hence $I(\\mathbf{X};\\mathbf{Y})=I(\\mathbf{R};\\mathbf{Y})+I(\\mathbf{\\Gamma};\\mathbf{Y}|\\mathbf{R})$. Consider (11) and define\n\n$$a_{i}=r_{2i-1}\\exp(\\mathrm{j}\\alpha_{i}),\\quad b_{i}=r_{2i}\\exp(\\mathrm{j}\\beta_{i}).\\tag{72}$$\n\nWe divide only the phase noise vector into components related\nto a and b because, as we will show later, the absolute value\ncan be detected and decoded in a memoryless fashion, hence\nno SIC receiver is needed.\n  AIRs for the absolute value and phase channels are\n\nIR(R; Y ) = 1 n i=1 H(Ri) \u2212 H(Ri|Y ) n \ufffd (73) n/2 I1(\u03b1; Y |R) = 1 i=1 h(\u03b1i) \u2212 h(\u03b1i|Y , R) n/2 \ufffd n/2 I2(\u03b2; Y |R, \u03b1) = 1 i=1 h(\u03b2i) \u2212 h(\u03b2i|Y , R, \u03b1). n/2 \ufffd\nThe following sum is an AIR for SIC:\n\nIsic(X; Y ) = IR(R; Y ) + 1 2 \ufffd I1(\u03b1; Y |R) + I2(\u03b2; Y |R, \u03b1) \ufffd \u2264 I(X; Y ). (74) Similar to (15), consider q(r, \u03b1, \u03b2, y, \u03b8) = q(r, \u03b3, y, \u03b8) = i=1 P(ri)p(\u03b3i)p(\u03b8i|\u03b8i\u22121)q(yi|ri, \u03b3i, \u03b8i) (75) n \ufffd\n\nwhere \u03b3 describes the angles of the entries of x, and is thus\na function of \u03b1 and \u03b2. The vector x is a function of r and\n\u03b3. As before, we discard dependencies for the sake of clarity.\nThe receiver wishes to calculate\n\nq(ri|y) = 1 c3 r\u2208R\\{i} q(r, \u03b3, y, \u03b8)d\u03b3d\u03b8 (76) Rn \ufffd \ufffd \u03a0 \ufffd q(\u03b1i|y, r) = 1 c4 Rn \ufffd \ufffd \u03a0\\{i} q(r, \u03b3, y, \u03b8)d\u03b3d\u03b8 (77) q(r, \u03b3, y, \u03b8)d\u03b3d\u03b8 (78) q(\u03b2i|y, r, \u03b1) = 1 c5 Rn \ufffd \ufffd \u03a0\\{i} \u03b1 where R\\{i} = {r\u2032 \u2208 Rn : r\u2032 i = ri} (79) \u03a0 = [\u2212\u03c0, \u03c0)n (80) \u03a0\\{i} = {\u03b3 \u2208 [\u2212\u03c0, \u03c0)n : \u03b32i\u22121 = \u03b1i} (81) \u03a0\\{i} \u03b1 = {\u03b3 \u2208 [\u2212\u03c0, \u03c0)n : \u03b1(\u03b3) = \u03b1 \u2227 \u03b32i = \u03b2i}. (82)\nwith \u03b1(\u03b3) = [\u03b31, \u03b33, . . . , \u03b3n\u22121]. As before, we marginalize q(r, \u03b3, y, \u03b8), but the variables subject to marginalization depend on the SIC-stage.\n\n_B. Computing the Marginal Distributions_\n\n_1) Absolute Value Detection:_ The graph used to detect the amplitudes $r$ has branches shown in Fig. 7. Using\n\n$$\\int_{-\\pi}^{\\pi}q(y_{i}|r_{i},\\gamma_{i},\\theta_{i})\\mathrm{d}\\gamma_{i}=\\frac{2}{\\sigma_{n}^{2}}\\exp\\left(-\\frac{|y_{i}|^{2}+r_{i}^{2}}{\\sigma_{n}^{2}}\\right)I_{0}\\left(\\frac{2|y_{i}|r_{i}}{\\sigma_{n}^{2}}\\right)\\tag{83}$$\n\nand $p(\\gamma_{i})=\\frac{1}{2\\pi}$, one can again show that\n\n$$\\begin{split}\\overleftarrow{\\eta}_{\\,\\theta_{i}^{\\prime}}(\\theta_{i})&=\\frac{1}{\\overleftarrow{\\mathrm{C}}\\,\\theta_{i}^{\\prime}}\\sum_{r_{i}^{\\prime}\\in\\mathcal{R}}\\int_{-\\pi}^{\\pi}p(\\gamma_{i})P(r_{i})q(y_{i}|r_{i},\\gamma_{i},\\theta_{i})\\mathrm{d}\\gamma_{i}\\\\ &=\\mathrm{const.}\\end{split}\\tag{84}$$\n\nAs in (27), we obtain\n\n$$\\overrightarrow{\\eta}_{\\,\\theta_{i}^{\\prime}}(\\theta_{i})=p(\\theta_{i})\\tag{85}$$ $$\\begin{split}\\overleftarrow{\\eta}\\,r_{i}\\,(r_{i})&=\\frac{1}{\\overleftarrow{c}\\,r_{i}}\\,\\int_{\\mathbb{R}}\\overrightarrow{\\eta}\\,\\theta_{i}^{\\prime}(\\theta_{i})\\int_{-\\pi}^{\\pi}p(\\gamma_{i})q(y_{i}|r_{i},\\gamma_{i},\\theta_{i})\\mathrm{d}\\gamma_{i}\\mathrm{d}\\theta_{i}\\\\ &\\propto\\exp\\left(-\\frac{r_{i}^{2}}{\\sigma_{n}^{2}}\\right)I_{0}\\left(\\frac{2|y_{i}|r_{i}}{\\sigma_{n}^{2}}\\right).\\end{split}\\tag{86}$$\n\nWith this, upon receiving $y_{i}$ one can compute\n\n$$q(r_{i}|\\mathbf{y})=\\frac{P(r_{i})\\overleftarrow{\\eta}\\,r_{i}(r_{i})}{\\sum_{\\vec{r}\\in\\mathcal{R}}P(\\vec{r})\\overleftarrow{\\eta}\\,r_{i}(\\vec{r})}.\\tag{87}$$\n\nNote that the computations for different $i$ may run in parallel.\n\nWe now investigate SIC with two stages for absolute value detection. In the second stage, branches of the type shown in Fig. 7 and Fig. 8 alternate. For odd $i$, we have branches of the form shown in Fig. 8 and\n\n$$\\begin{split}\\overleftarrow{\\eta}\\,\\theta_{i}^{\\prime}(\\theta_{i})&=\\frac{1}{\\overleftarrow{c}\\,\\theta_{i}^{\\prime}}\\,\\int_{-\\pi}^{\\pi}p(\\gamma_{i})q(y_{i}|r_{i},\\gamma_{i},\\theta_{i})\\mathrm{d}\\gamma_{i}\\\\ &=\\text{const}.\\end{split}\\tag{88}$$\n\nwhere we used (83). Following the same steps as before, we\nrecover (87). Therefore, the receiver does not use the entries\nof r decoded in the first stage. We can hence use (87) to detect\nall elements in r and achieve no gain using SIC.\n  2) Phase Detection, First Stage: The graph is a concatena-\ntion of branches of the form shown in Fig. 8. Using (83), we\nagain have \u2190\u2212\u03b7 \u03b8\u2032\n              i(\u03b8i) = const., and \u2212\u2192\u03b7 \u03b8\u2032\n                                    i(\u03b8i) = p(\u03b8i). Similar\nto (34), we obtain\n\n$$q(y_{i}|r_{i},\\gamma_{i},\\theta_{i})\\approx$$ $$\\frac{1}{\\sqrt{|y_{i}|r_{i}}}\\mathcal{N}\\left(|y_{i}|;r_{i},\\frac{\\sigma_{n}^{2}}{2}\\right)\\mathcal{N}\\left(\\theta_{i};m\\left(\\angle_{y_{i}}-\\gamma_{i}\\right),\\frac{\\sigma_{n}^{2}}{2|y_{i}|r_{i}}\\right).\\tag{89}$$\n\nUsing $\\overrightarrow{\\eta}_{\\theta_{i}^{\\prime}}(\\theta_{i})=\\mathcal{N}\\left(\\theta_{i};0,\\sigma_{\\theta}^{2}\\right)$, we thus have\n\n$$q(\\gamma_{i}|\\boldsymbol{y},\\boldsymbol{r})=\\frac{1}{c_{6}}\\overrightarrow{\\eta}_{\\gamma_{i}}(\\gamma_{i})\\int_{\\mathbb{R}}\\overrightarrow{\\eta}_{\\theta_{i}^{\\prime}}(\\theta_{i})q(y_{i}|r_{i},\\gamma_{i},\\theta_{i})\\mathrm{d}\\theta_{i}\\tag{90}$$ $$\\approx\\mathcal{N}\\left(m\\left(\\angle_{y_{i}}-\\gamma_{i}\\right);0,\\sigma_{\\theta}^{2}+\\frac{\\sigma_{n}^{2}}{2|y_{i}|r_{i}}\\right).$$\n\nThe scaling constant ensures (90) has unit integral over the\nsupport of \u03b3i. However, as the tails decay rapidly, this constant\nis larger than, but very close to 1 and may be omitted.\n  3) Phase Detection, Second Stage: Branches of the form\nshown in Fig. 2 for odd i and Fig. 8 for even i alternate. In\nthe former case, we use the approximation (35), while in the\nlatter case \u2190\u2212\u03b7 \u03b8\u2032\n              i(\u03b8i) is constant in \u03b8i. With the same steps as\nbefore, we obtain\n\n$$\\overrightarrow{\\eta}_{\\theta^{\\prime}_{i}}(\\theta_{i})\\approx\\mathcal{N}\\left(\\theta_{i};\\overrightarrow{\\mu}_{\\theta^{\\prime}_{i}};\\overrightarrow{\\sigma}_{\\theta^{\\prime}_{i}}^{2}\\right)\\tag{91}$$\n\nwhere (50) and (51) give the expressions for $\\overrightarrow{\\mu}_{\\theta^{\\prime}_{i}}$ and $\\overrightarrow{\\sigma}_{\\theta^{\\prime}_{i}}^{2}$. Similar to (90), we now have\n\n$$q(\\gamma_{i}|\\mathbf{y},\\mathbf{r},\\mathbf{\\alpha})=\\mathcal{N}\\left(m\\left(\\angle_{y_{i}}-\\gamma_{i}-\\overrightarrow{\\mu}_{\\theta^{\\prime}_{i}}\\right);0,\\overrightarrow{\\sigma}_{\\theta^{\\prime}_{i}}^{2}+\\frac{\\sigma_{n}^{2}}{2|y_{i}|r_{i}}\\right)\\tag{92}$$\n\nwhere we omitted the normalization, as discussed above.\n"
    },
    {
        "level": "##",
        "title": "C. Extension To S Sic-Stages",
        "content": "\n  Extending the algorithm to S stages is straightforward. We\nfirst decode r using (87) and the first stage of \u03b3 using (90).\nFor the s-th stage, we reuse algorithm 1 to obtain \u2212\u2192\n                                                    \u03b7 \u03b8\u2032\n                                                       i =\n\ufffd\u2212\u2192\u00b5 \u03b8\u2032\n    i, \u2212\u2192\u03c3 2\n        \u03b8\u2032\n         i\n\n         \ufffd\n           and calculate q(\u03b3i|y, r, \u03b3(1), . . . , \u03b3(s\u22121)) for i \u2208\n{s, s + S, s + 2S, . . .}, as indicated by (92).\n"
    },
    {
        "level": "##",
        "title": "D. Lower Bound On Mutual Information",
        "content": "\nUsing the same approach as in Sec. III-D, define\n\n$$I_{R,q}(\\mathbf{R};\\mathbf{Y})=\\frac{1}{n}\\sum_{i=1}^{n}H(R_{i})-H_{q}(R_{i}|Y_{i})\\tag{93}$$ $$\\leq I_{R}(\\mathbf{R};\\mathbf{Y})$$ $$I_{1,q}(\\mathbf{\\alpha};\\mathbf{Y}|\\mathbf{R})=\\frac{1}{n/2}\\sum_{i=1}^{n/2}h(\\alpha_{i})-h_{q}(\\alpha_{i}|\\mathbf{Y},\\mathbf{R})$$ (94) $$\\leq I_{1}(\\mathbf{\\alpha};\\mathbf{Y}|\\mathbf{R})$$ $$I_{2,q}(\\mathbf{\\beta};\\mathbf{Y}|\\mathbf{R},\\mathbf{\\alpha})=\\frac{1}{n/2}\\sum_{i=1}^{n/2}h(\\beta_{i})-h_{q}(\\beta_{i}|\\mathbf{Y},\\mathbf{R},\\mathbf{\\alpha})$$ (95) $$\\leq I_{2}(\\mathbf{\\beta};\\mathbf{Y}|\\mathbf{R},\\mathbf{\\alpha}).$$\nNote that\n\n$$H(R_{i})=-\\sum_{\\ell=1}^{n_{r}}w_{\\ell}\\log w_{\\ell},\\quad h(\\alpha_{i})=h(\\beta_{i})=\\log2\\pi\\tag{96}$$\n\nfor all $i$. The surrogate channel (differential) conditional entropies can be approximated by simulation as in Sec. III-D.\n"
    },
    {
        "level": "##",
        "title": "E. Simulation Results",
        "content": "\n  Fig. 9 shows the AIRs for 2 SIC-stages. The AIRs of\nCSCG modulation are plotted in dashed black for reference.\nAs expected from Fig. 5, the AIR increases with the number\nof rings and saturates at 32 rings. Fig. 10 shows the rates as\na function of the number of SIC-stages for 32 rings. This is\nsimilar to the results for CSCG modulation in Fig. 4.\n  The phase noise variance depends on the amplitude statis-\ntics. For example, M-PSK or ring constellations with one ring\n\ncause little phase noise, whereas Gaussian modulation causes\nsignificant phase noise [5]. Therefore, we have a tradeoff: in-\ncreasing the number of rings increases the amplitude channel's\nrate and the phase noise variance. The left plot in Fig. 11\nshows that for two SIC-stages, the AIR of the phase channel\ndecreases with an increasing number of rings. The right side\nshows that the AIR of the amplitude channel increases by a\nlarger amount, and hence the overall AIR increases for an\nincreasing number of rings.\n"
    },
    {
        "level": "##",
        "title": "V. Conclusion & Outlook",
        "content": "\n  We studied SIC-receivers to compensate for nonlinearity in\noptical fiber. The receiver used the CPAN model as a surrogate\nchannel, and we simplified the SPA by using Gaussian mes-\nsages. We proposed receiver algorithms for CSCG modulation\nand ring constellations that provide AIRs comparable to those\nof JDD receivers [9] for 16 or more SIC-stages. The ring\nconstellations perform as well as CSCG modulation for 32\nor more rings. For future work, we plan to study discrete\nconstellations and multi-level coding with off-the-shelf codes,\nas well as dual-polarization and space-division multiplexing.\nAnother interesting direction is to discard single-channel back-\n\npropagation and use the proposed receiver to compensate for self-phase modulation.\n\n           ACKNOWLEDGMENT\nThe authors wish to thank Daniel Plabst for inspiring discus-\nsions.\n The authors acknowledge the financial support by the Fed-\neral Ministry of Education and Research of Germany in the\nprogramme of \"Souver\u00a8an. Digital. Vernetzt.\". Joint project 6G-\nlife, project identification number: 16KISK002.\n\nFor CSCG inputs, let\n\nf(x) = 1 R \u2212\u2192\u03b7 \u03b8\u2032(\u03b8)q(y|x, \u03b8)d\u03b8 (97) cf p(x) \ufffd C p(x)q(y|x, \u03b8)dxd\u03b8 and cf = \ufffd R \u2212\u2192\u03b7 \u03b8\u2032(\u03b8) \ufffd = q(y) (98)\nwith q(y) = NC\n\ufffd\ny; 0, \u03c32\ny\n\ufffd\n. Using\n\ng(x) = x, for Ef[X] |x|2, for Ef[|X|2] x2, for Ef[X2] (99) \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 C g(x)f(x)dx\nthe second-order moments can be calculated with\n\ufffd\n\nd\u02dcx (100) C g(\u02dcx) p(\u02dcx)q(y|\u02dcx, 0) = \ufffd \ufffd R \u2212\u2192\u03b7 \u03b8\u2032(\u03b8)e\u2212kj\u03b8d\u03b8 \ufffd \ufffd\ufffd \ufffd =:a q(y) \ufffd \ufffd\ufffd \ufffd =:b(\u02dcx)\n\nwhere \u02dcx = xej\u03b8 and k = 1 for Ef[X], k = 0 for Ef[|X|2],\nand k = 2 for Ef[X2].\n  Using \u2212\u2192\u03b7 \u03b8\u2032(\u03b8) = N\n                     \ufffd\n                      \u03b8; \u2212\u2192\u00b5 \u03b8\u2032, \u2212\u2192\u03c3 2\n                              \u03b8\u2032\n                                \ufffd\n                                  and completing the\nsquares gives\n\n\u2212\u2192 \u00b5 2 \u03b8\u2032 \u2212 (\u2212\u2192\u00b5 \u03b8\u2032 \u2212 kj\u2212\u2192\u03c3 2 \u03b8\u2032)2 2 \u2212\u2192 \u03c3 2 \u03b8\u2032 \ufffd . (101) a = exp \ufffd \u22121\nAlso, b(\u02dcx) is a CSCG\n\n$$b(\\tilde{x})={\\cal N}_{\\rm C}\\left(\\tilde{x};y\\frac{\\sigma_{x}^{2}}{\\sigma_{y}^{2}},\\frac{\\sigma_{x}^{2}\\sigma_{n}^{2}}{\\sigma_{y}^{2}}\\right)\\tag{102}$$\nand therefore\n\ny \u03c32 x \u03c32y , for g(\u02dcx) = \u02dcx \u03c32 x . \u03c32y \u03c32y C g(\u02dcx)b(\u02dcx)d\u02dcx = \ufffd \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \ufffd \u03c32 n + |y|2 \u03c32 x \ufffd , for g(\u02dcx) = |\u02dcx|2 y2 \u03c34 x \u03c34y , for g(\u02dcx) = \u02dcx2. \uf8f4 \uf8f4 \uf8f4 \uf8f3\n(103)\nThe moments of f follow directly:\n\n\u2212\u2192 \u00b5 2 \u03b8\u2032 \u2212 (\u2212\u2192\u00b5 \u03b8\u2032 \u2212 j\u2212\u2192\u03c3 2 \u03b8\u2032)2 \u00b5f = y \u03c32 x 2 \u2212\u2192 \u03c3 2 \u03b8\u2032 \ufffd (104) \u03c32y exp \ufffd \u22121 \u03c32 f = \u03c32 x \u03c32y \u03c32y \ufffd \u03c32 n + |y|2 \u03c32 x \ufffd \u2212 |\u00b5f|2 (105) \u2212\u2192 \u00b5 2 \u03b8\u2032 \u2212 (\u2212\u2192\u00b5 \u03b8\u2032 \u2212 2j\u2212\u2192 \u03c3 2 \u03b8\u2032)2 p2 f = y2 \u03c34 x 2 \u2212\u2192 \u03c3 2 \u03b8\u2032 \ufffd \u2212 \u00b52 f. \u03c34y exp \ufffd \u22121 (106)\n"
    },
    {
        "level": "##",
        "title": "References",
        "content": "\n[1] R.-J. Essiambre, G. Kramer, P. J. Winzer, G. J. Foschini, and B. Goebel,\n\"Capacity Limits of Optical Fiber Networks,\" *J. Lightw. Technol.*,\nvol. 28, no. 4, pp. 662\u2013701, 2010.\n[2] A. Mecozzi and R. Essiambre, \"Nonlinear Shannon limit in pseudolinear\ncoherent systems,\" *J. Lightw. Technol.*, vol. 30, no. 12, pp. 2011\u20132024,\nJune 2012.\n[3] R. Dar, M. Feder, A. Mecozzi, and M. Shtaif, \"Properties of nonlinear\nnoise in long, dispersion-uncompensated fiber links,\" *Opt. Express*,\nvol. 21, no. 22, pp. 25 685\u201325 699, Nov 2013.\n[4] R. Dar, M. Shtaif, and M. Feder, \"New bounds on the capacity of the\nnonlinear fiber-optic channel,\" *Opt. Lett.*, vol. 39, no. 2, pp. 398\u2013401,\nJan 2014.\n[5] R. Dar, M. Feder, A. Mecozzi, and M. Shtaif, \"Pulse Collision Picture of\nInter-Channel Nonlinear Interference in Fiber-Optic Communications,\"\nJ. Lightw. Technol., vol. 34, no. 2, pp. 593\u2013607, 2016.\n[6] M. Secondini, E. Agrell, E. Forestieri, D. Marsella, and M. R. Camara,\n\"Nonlinearity Mitigation in WDM Systems: Models, Strategies, and\nAchievable Rates,\" *J. Lightw. Technol.*, vol. 37, no. 10, pp. 2270\u20132283,\n2019.\n[7] M. Secondini, S. Civelli, E. Forestieri, and L. Z. Khan, \"New lower\nbounds on the capacity of optical fiber channels via optimized shaping\nand detection,\" *J. Lightw. Technol.*, vol. 40, no. 10, pp. 3197\u20133209, 2022.\n[8] F. J. Garc\u00b4\u0131a-G\u00b4omez and G. Kramer, \"Mismatched models to lower\nbound the capacity of dual-polarization optical fiber channels,\" J. Lightw. Technol., vol. 39, no. 11, pp. 3390\u20133399, 2021.\n[9] \u2014\u2014, \"Mismatched Models to Lower Bound the Capacity of Optical\nFiber Channels,\" *J. Lightw. Technol.*, vol. 38, no. 24, pp. 6779\u20136787,\n2020.\n[10] J. Dauwels and H.-A. Loeliger, \"Computation of information rates by\nparticle methods,\" *IEEE Trans. Inf. Theory*, vol. 54, no. 1, pp. 406\u2013409,\n2008.\n[11] \u2014\u2014, \"Phase estimation by message passing,\" in IEEE Int. Conf.\nCommun., vol. 1, 2004, pp. 523\u2013527 Vol.1.\n[12] G. Colavolpe, A. Barbieri, and G. Caire, \"Algorithms for Iterative\nDecoding in the Presence of Strong Phase Noise,\" IEEE J. Selected Areas Commun., vol. 23, no. 9, pp. 1748\u20131757, 2005.\n[13] S. Shayovitz and D. Raphaeli, \"Message passing algorithms for phase\nnoise tracking using Tikhonov mixtures,\" *IEEE Trans. Commun.*, vol. 64,\nno. 1, pp. 387\u2013401, 2016.\n[14] M. P. Yankov, T. Fehenberger, L. Barletta, and N. Hanik, \"Lowcomplexity tracking of laser and nonlinear phase noise in WDM optical\nfiber systems,\" *J. Lightw. Technol.*, vol. 33, no. 23, pp. 4975\u20134984, 2015.\n[15] A. F. Alfredsson, E. Agrell, and H. Wymeersch, \"Iterative detection and\nphase-noise compensation for coded multichannel optical transmission,\"\nIEEE Trans. Commun., vol. 67, no. 8, pp. 5532\u20135543, 2019.\n[16] U. Wachsmann, R. F. Fischer, and J. B. Huber, \"Multilevel codes:\nTheoretical concepts and practical design rules,\" IEEE Trans. Inf. Theory, vol. 45, no. 5, pp. 1361\u20131391, 1999.\n[17] H. Pfister, J. Soriaga, and P. Siegel, \"On the achievable information rates\nof finite state ISI channels,\" in *IEEE Global Telecommun. Conf.*, vol. 5,\n2001, pp. 2992\u20132996 vol.5.\n[18] J. B. Soriaga, H. D. Pfister, and P. H. Siegel, \"Determining and\napproaching achievable rates of binary intersymbol interference channels\nusing multistage decoding,\" *IEEE Trans. Inf. Theory*, vol. 53, no. 4, pp.\n1416\u20131429, 2007.\n[19] T. Prinz, D. Plabst, T. Wiegart, S. Calabr`o, N. Hanik, and G. Kramer,\n\"Successive Interference Cancellation for Bandlimited Channels with\nDirect Detection,\" *IEEE Trans. Commun.*, vol. 72, no. 3, pp. 1330\u20131340,\n2024.\n[20] D.\nPlabst,\nT.\nPrinz,\nF.\nDiedolo,\nT.\nWiegart,\nG.\nB\u00a8ocherer,\nN.\nHanik,\nand\nG.\nKramer,\n\"Neural\nnetwork\nequalizers\nand\nsuccessive interference cancellation for bandlimited channels with\na nonlinearity,\" *IEEE Trans. Commun., submitted*, 2024. [Online].\nAvailable: https://arxiv.org/abs/2401.09217\n[21] A. Mecozzi and R.-J. Essiambre, \"Nonlinear shannon limit in pseudolinear coherent systems,\" *J. Lightw. Technol.*, vol. 30, no. 12, pp. 2011\u2013\n2024, 2012.\n[22] F. Kschischang, B. Frey, and H.-A. Loeliger, \"Factor graphs and the\nsum-product algorithm,\" *IEEE Trans. Inf. Theory*, vol. 47, no. 2, pp.\n498\u2013519, 2001.\n[23] H.-A. Loeliger, \"An introduction to factor graphs,\" IEEE Signal Proc.\nMag., vol. 21, no. 1, pp. 28\u201341, 2004.\n[24] P. Bromiley, \"Products and convolutions of Gaussian probability density\nfunctions,\" University of Manchester, Tech. Rep., 2014.\n[25] K. S. Turitsyn, S. A. Derevyanko, I. V. Yurkevich, and S. K. Turitsyn, \"Information capacity of optical fiber channels with zero average\ndispersion,\" *Phys. Rev. Lett.*, vol. 91, p. 203901, 2003.\n[26] M. I. Yousefi and F. R. Kschischang, \"On the per-sample capacity of\nnondispersive optical fibers,\" *IEEE Trans. Inf. Theory*, vol. 57, no. 11,\npp. 7522\u20137541, 2011.\n[27] G. Kramer, \"Autocorrelation function for dispersion-free fiber channels\nwith distributed amplification,\" *IEEE Trans. Inf. Theory*, vol. 64, no. 7,\npp. 5131\u20135155, 2018.\n[28] C. H\u00a8ager and E. Agrell, \"Data-driven estimation of capacity upper\nbounds,\" *IEEE Commun. Lett.*, vol. 26, no. 12, pp. 2939\u20132943, 2022.\n[29] G. Kramer, M. I. Yousefi, and F. R. Kschischang, \"Upper bound on the\ncapacity of a cascade of nonlinear and noisy channels,\" in IEEE Inf.\nTheory Workshop, 2015, pp. 1\u20134.\n[30] M. I. Yousefi, G. Kramer, and F. R. Kschischang, \"Upper bound on the\ncapacity of the nonlinear schr\u00a8odinger channel,\" in IEEE Can. Workshop Inf. Theory, 2015, pp. 22\u201326."
    }
]