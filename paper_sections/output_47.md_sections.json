[
    {
        "level": "##",
        "title": "Vidla: Video-Language Alignment At Scale",
        "content": "\nMamshad Nayeem Rizve Fan Fei Jayakrishnan Unnikrishnan Son Tran Benjamin Z. Yao Belinda Zeng Mubarak Shah Trishul Chilimbi\n{mnrizve, feiff, jayunn, sontran, benjamy, zengb, shahmub, trishulc}@amazon.com\n"
    },
    {
        "level": "##",
        "title": "Abstract",
        "content": "\nIn this paper, we propose VidLA, an approach for videolanguage alignment at scale. There are two major limitations of previous video-language alignment approaches. First, they do not capture both short-range and long-range temporal dependencies and typically employ complex hierarchical deep network architectures that are hard to integrate with existing pretrained image-text foundation models. To effectively address this limitation, we instead keep the network architecture simple and use a set of data tokens that operate at different temporal resolutions in a hierarchical manner, accounting for the temporally hierarchical nature of videos. By employing a simple two-tower architecture, we are able to initialize our video-language model with pretrained image-text foundation models, thereby boosting the final performance.\n\nSecond, existing video-language alignment works struggle due to the lack of semantically aligned large-scale training data.\n\nTo overcome it, we leverage recent LLMs to curate the largest video-language dataset to date with better visual grounding. Furthermore, unlike existing video-text datasets which only contain short clips, our dataset is enriched with video clips of varying durations to aid our temporally hierarchical data tokens in extracting better representations at varying temporal scales. Overall, empirical results show that our proposed approach surpasses state-of-the-art methods on multiple retrieval benchmarks, especially on longer videos, and performs competitively on classification benchmarks.\n"
    },
    {
        "level": "##",
        "title": "1. Introduction",
        "content": "\nVision-language alignment is crucial for solving many vision-language tasks like text-guided retrieval [41, 42, 47, 65, 74], visual question answering [11, 40, 88, 99], visual captioning [40, 42, 61, 86], etc.\n\nIn the past few years, training on web-scale image-text pairs has significantly improved performance in many of these vision-language tasks [43, 44, 65, 99]. However, humans perceive the world as a continuous stream of images, which is also reflected in the increasing number of videos uploaded to the web daily. Despite the ubiquitous use of videos, when compared to image-language alignment, video-language alignment is significantly more challenging for two key reasons.\n\nFirst, unlike image-language data, it is much harder to collect aligned video-language data at scale. To address this issue, most prior works utilize automatic speech recognition (ASR) systems [2, 66, 102] to extract textual transcripts and generate paired video-language data for largescale training [57, 93, 100]. However, it has been shown that transcripts often corresponds poorly with their associated visual contents [32, 56, 57, 72]. As a result, some recent works [12, 38, 46, 79] skipped large-scale videolanguage training and worked around by utilizing languagealigned image encoders, followed by lightly adapting them with temporal modules on small-scale video datasets with paired textual descriptions [6, 59]. However, training on such small-scale datasets often leads to overfitting [94] and does not encourage learning temporally diverse representations [38]. *Second*, since the vision transformer architecture lacks strong visual inductive bias such as that in CNN-type architectures, it requires large-scale pretraining data for effective generalization [21, 67]. In case of videos, this problem is amplified further due to the added temporal dimension.\n\nFacing this challenge, to be more efficient, existing works utilize factorized [4, 7, 60] or hierarchical [45, 48, 52, 68] space-time attention.\n\nHowever, neither of these solutions are optimal for large-scale video-language alignment, as factorized space-time attention overly focuses on aggregating redundant local spatiotemporal information [45], while hierarchical space-time attention makes the use of pretrained language-aligned nonhierarchical image encoders [44, 65] challenging. Our work addresses both challenges in large-scale video-language alignment using large language models and a novel hierarchical temporal attention mechanism.\n\nIn addition, having large-scale video-text data set is crucial for video-language alignment training. Towards that end, we construct a very large dataset, with \u223c800M videotext pairs, to train video-language alignment model at scale. In this context, we propose several simple data curation strategies using LLMs [8, 14\u201316, 78] to improve the semantic correlation between textual description and associated visual content of large-scale video-language datasets. First, we utilize recent multi-modal large language models (MLLM) [17, 42, 49, 98, 105] to generate auxiliary captions grounded in visual content. Second, to scale our solution, we generate captions at a low frame rate, capitalizing on temporal redundancy of videos. Third, we augment the existing video-language datasets by incorporating videos of varying lengths to facilitate robust alignment across diverse temporal scales. We utilize LLMs for summarizing longer video descriptions, preventing training asymmetry when we sample the same number of frames for videos of all durations but use longer texts for longer videos. The LLM summarization also helps when long textual descriptions cover disparate concepts.\n\nTo efficiently utilize the non-hierarchical image-text pretrained models while accounting for the temporally hierarchical nature of videos, we factorize the space-time attention operation into two parts: local and global temporal attention. First, we focus on modeling the *local* temporal relationships by treating videos as collections of temporal tubes of single patch tokens. This attention operation focuses on capturing fine-grained motion across frames. Next, to model *global* spatio-temporal relationships in a temporally *hierarchical* manner, inspired from prior art [24, 104], we incorporate several Multi-Scale Temporal [MST] tokens that interact with all patch tokens at varying temporal resolutions to summarize the video semantic concepts at various temporal scales. To make this space-time attention operation more efficient, we update the patch tokens by attending over all the [MST] tokens and other patch tokens from the same frame. Finally, we utilize a [CLS] token to attentively aggregate information from all the [MST] and patch tokens. We utilize this aggregated spatio-temporal representation for video-language alignment training. Our hierarchical temporal attention design not only models local temporal relationships but also models global temporal relationships at different temporal hierarchies while utilizing strong pretrained image-text encoders.\n\nTo summarize, in this work, we make two major technical contributions: (i) we propose several techniques to utilize LLMs to generate a large scale video-text dataset where the generated text has high semantic correlation with the visual content. (ii) we design a hierarchical temporal modeling approach that effectively incorporates pretrained imagetext encoders and handles videos of varying lengths in the training set to improve downstream performance as shown in Figure 1. We extensively evaluate the performance of our method on several video-text retrieval benchmarks to demonstrate its efficacy and the effectiveness of our data curation techniques and modeling approach. A summary of our approach is provided in Figure 2.\n"
    },
    {
        "level": "##",
        "title": "2. Related Works",
        "content": "\nVision-Language Representation Learning Recently, image-language models [34, 44, 65, 74, 97, 99] have drawn huge attention because of their effectiveness in learning generic visual representations that are transferable to several downstream tasks like classification, retrieval, etc. This success can partly be attributed to the recently available large-scale image-text datasets [18, 69\u201371, 76]. However, in case of videos, there's no large-scale aligned videolanguage dataset.\n\nTherefore, to perform video-language pretraining, most recent works [23, 29, 55, 62, 94] bootstrap from a pretrained image-language model and then perform some form of lightweight adaptation on the video datasets.\n\nAdapting CLIP to Video Many video foundation model works aim to adapt CLIP for video representation. Most use a straightforward approach and encode frame samples with CLIP image encoder and pool across frames with various mechanisms [23, 29, 55] to represent the video. Other works insert additional temporal specific modelings such as divided space-time attention [7] or adapters [33] into CLIP vision transformer (ViT) layers [12, 62, 63, 79, 96]. Among others there are also works using novel parallel architectures [50] or using addition special tokens to capture temporal in-\n\n| Clip Duration   | # clips   | Length (s)         |\n|-----------------|-----------|--------------------|\n| Subtitle        | Caption   |                    |\n| # sent          | # words   | Summarized # words |\n| Short           | 496M      | 13.2               |\n| Medium          | 212M      | 30.4               |\n| 4.7             | 72.1      | 19.5               |\n| Long            | 100M      | 60.3               |\n\nteraction between frames [94].\n\nVideo-Language Datasets For image-language pretraining, web images paired with alt-text have demonstrated to be extremely effective and can scale up to billions of samples [11, 69, 75, 81]. Video dataset using similar alt-text such as WebVid [6] are often at a much smaller scale. Alternatively, VideoCC [59] dataset is generated by finding visually similar video clips from existing image text data.\n\nVideo subtitle datasets on the other hand are much more widely available and easy to scale, leading to wide adoption [57, 93, 100, 101], however these type of videos are often very short clips segmented by sentence boundaries, and the subtitles are usually noisy and have poor visual grounding. In this work, instead of generating a new dataset, we propose a way to effectively use existing large scale video dataset to improve video text alignment.\n"
    },
    {
        "level": "##",
        "title": "3. Video-Language Pretraining Dataset",
        "content": "\nA key component of our Video-Language Alignment method summarized in Figure 2 is a high quality large scale video-language dataset. In order to be able to effectively train the video-text models with hierarchical temporal attention, and to allow the model to learn to align videos with different temporal scales, we need a dataset with videos of different lengths, and corresponding text annotations with varying levels of temporal abstraction.\n\nWe describe our novel data curation scheme in detail below.\n\nSource Videos We utilize 20 million videos from the YT-\nTemporal-1B [100] dataset for creating video-text dataset since its the largest collection of publicly available videos. These videos cover a diverse set of video concepts, and have been filtered to ensure better visual groundedness as well as to protect user privacy. Unlike prior works which utilize the video frame-subtitle pairs from this dataset, we create a new dataset composed of *video clips* paired with subtitles and generated captions which we call YT-VidLA-800M. Next, we describe our multi-scale video clip extraction process.\n\nVideo Clip Extraction To extract video clips, first, we punctuate the ASR subtitles using a bert-based [20] punctuation model to split the full subtitle into sentences. Next, we split the videos at sentence boundaries to generate clips, where each clip covers one or more sentences. To facilitate video-language alignment across different temporal scales, we split each video into clips of varying temporal lengths. To be particular, our shortest clips are on average 13 seconds long in duration, which are similar in length (6-13 seconds) to videos in existing large-scale datasets [57, 59, 93]. The medium length clips on the other hand are on average 30 seconds long, which is similar in length to videos in common retrieval benchmarks [3, 92]. The longer clips are on average 60 seconds in duration. Overall, we extract around 500 million short video clips, 200 million medium length video clips, and 100 million long video clips as summarized in Table 1. Next, we discuss about our visually grounded auxiliary caption generation process.\n\nCaption Generation To improve visual grounding in language supervision, we generate auxiliary captions for each clip using multi-modal LLMs.\n\nParticularly, we utilize BLIP-2 [42] to generate captions for the frames of the extracted video clips. To be efficient, capitalizing on the temporal redundancy of videos, we generate these captions at a very low frame-rate (\u223c 0.1 FPS). To aggregate the framelevel captions to generate the video clip-level caption, we perform text summarization, which we discuss next.\n\nText Summarization We use an LLM [78] to summarize the individual frame captions to obtain the caption for each video clip. Additionally, we summarize ASR subtitles from longer videos to obtain right abstraction for video-language alignment training. Furthermore, caption and subtitle summarization address another practical problem: it reduces the size of the input text, making it feasible to fit within the context length window of CLIP's pretrained text encoder. After this operation, each video clip is paired with two summarized texts corresponding to ASR subtitle and generated caption. We present the statistics of YT-VidLA-800M before and after summarization in Table 1.\n"
    },
    {
        "level": "##",
        "title": "4. Method",
        "content": "\nIn VidLA, we utilize an extension of the two-tower architecture for image-language alignment from CLIP [65]. Particularly, we retain the CLIP text encoder architecture and extend CLIP's vision encoder to improve its temporal modeling capability by introducing a novel attention mechanism illustrated in Figure 3. We provide details of our video encoder in the following.\n\nPreliminaries The vision encoder accepts as input a video clip v consisting of T RGB frames vt \u2208 RH\u00d7W \u00d73, t \u2208\n{0, 1*, ..., T* \u2212 1} each of size H \u00d7 W pixels sampled from the video. Following vision transformer [21] and pretrained image-language models [44, 65], we split each frame into non-overlapping patches of size P \u00d7 P yielding N\n=\nHW/P 2 patches for each frame. Each of the NT patches is linearly mapped with a learnable matrix and then combined with learnable spatial and temporal position embeddings to obtain a sequence of TN patch tokens, represented by \ufffdZ0 \u2208 RT N\u00d7d, where d denotes the dimension of each token. We incorporate a set of UV [MST] tokens to capture summarized information at different temporal scales from the video where U represents the number temporal hierarchies and V represents the number of [MST] tokens at each temporal hierarchy. We also include a [CLS] token to capture the global representation of the video (see e.g., [19]).\n\nWe create the final input sequence, Z0 \u2208 R(1+UV +T N)\u00d7d, by prepending the learnable [CLS] token and UV additional\n[MST] tokens to the sequence of TN patch tokens. The sequence of input tokens are passed through L transformer layers. We use Zl to denote the sequence after the l-th layer.\n\nIn each layer the sequence is treated with two steps of attention followed by an MLP layer as summarized in Figure 3 and detailed next.\n\nSpatially Local Temporal Attention Inspired from a long line of works [24, 25, 73] that seek to model finegrained temporal motion for video understanding, we employ spatially local temporal attention.\n\nAs the first operation in any l-th layer of the transformer, we remove the [CLS] and\n[MST] tokens from the sequence of input tokens to that layer, Zl\u22121, to apply this attention only on the patch tokens,\n\ufffdZl\u22121 \u2208 RT N\u00d7d. To capture finegrained temporal motion during this attention operation, each patch token only attends to patch tokens from other frames in the same spatial position, effectively allowing attention only along the temporal dimension. This operation can be represented using an attention mask, \ufffd\nM \u2208 RT N\u00d7T N, formally defined as\n\n\ufffd\n 0\n    if mod(|j \u2212 i|, N) = 0\n \u2212\u221e\n    otherwise.\n\n\ufffd\nMi,j =\n\nSpatially local temporal attention is then performed as\n\n\ufffdZl\n SlT = MMSA(LN(\ufffdZl\u22121), \ufffd\n                    M) + \ufffdZl\u22121\n                                 (1)\n\nwhere LN(.) is layer normalization [5] operation and\nMMSA(., .) is masked multi-head self-attention which can\nbe expressed as MMSA(Z, M) := softmax(QKT /\n                                           \u221a\n\n                                          d +\nM)V \u2208 RT N\u00d7d; here Q, K, V are query, key, value\nembeddings of the sequence of input tokens Z obtained\nthrough linear projection and M is the input attention mask.\n  After the attention computation, we again prepend\nthe [CLS] and [MST] tokens to the updated patch to-\nkens,\n     \ufffdZl\n      SlT , to obtain the token sequence Zl\n                                       SlT\n                                            =\n[(Zl\u22121)[CLS], (Zl\u22121)[MST], \ufffdZl\n                     SlT ].\nGlobal Spatio-Temporal Attention To efficiently model\nthe global spatio-temporal semantics in a hierarchical man-\nner, we utilize the hierarchical [MST] tokens for guiding\nthe global spatio-temporal attention. We employ an asym-\nmetric attention mechanism to update the [CLS], [MST],\nand patch tokens as illustrated in the second grid in Fig-\nure 3. To keep the attention operation computationally effi-\ncient, each patch token attends to all patch tokens from the\nsame frame, and to all the UV [MST] tokens \u2208 RUV \u00d7d.\nThe patch token updates can be expressed using an at-\ntention mask, M[PATCH] \u2208 RT N\u00d7(1+UV +T N), defined as\n\nM[PATCH] = [0, \ufffd MG] where 0 is a TN \u00d7 (1 + UV ) matrix of zeros and \ufffd MG is a TN \u00d7 TN matrix with N \ufffd \ufffd 0 if \ufffd i N \ufffd = \ufffd j \u2212\u221e otherwise \ufffd MG i,j =\nwhere \u230a.\u230b indicates the FLOOR function.\n\nThe update procedure for [MST] tokens is designed to capture the temporally hierarchical nature of video concepts. The attention mask for each [MST] token is determined by the hierarchy level of that token, ranging from\n0 to U \u2212 1, and the temporal scale r.\n\nSpecifically, the\n[MST] tokens from a particular hierarchy level u attend to [MST] tokens from lower temporal hierarchies and to the [PATCH] tokens from every ru-th frame. As there are V [MST] tokens in each hierarchy level, the updates for the [MST] tokens can be expressed using another attention mask, M[MST] \u2208 RUV \u00d7(1+UV +T N) where the first V rows correspond to [MST] tokens of hierarchy level 0, followed by V rows of hierarchy level 1, and so on. The attention mask can be formally expressed as M[MST] =\n[\u2212\u221e1, \ufffd\nM[MST],self, \ufffd\nM[MST],patch] where 1 is a UV \u00d7 1\nvector of all 1's, M[MST],self is a UV \u00d7 UV matrix and M[MST],patch is a UV \u00d7 TN matrix with\n\nV \ufffd \u2265 \ufffd j V \ufffd \ufffd 0 if \ufffd i \u2212\u221e otherwise \ufffd M[MST],self i,j = V \u230b\ufffd = 0 \ufffd 0 if mod \ufffd\ufffd j N \ufffd , r\u230a i \u2212\u221e otherwise \ufffd M [MST],patch i,j =\nNote that both patch and [MST] tokens do not attend to the [CLS] token to limit propagation of *global* information into the these *local* tokens. We update the [CLS] token by attending to all the patch and [MST] tokens. This asymmetric update ensures that the [CLS] token merely acts as an aggregator where it attentively pulls information from all tokens. We denote the attention mask for updating the [CLS]\ntoken as M[CLS] \u2208 R1\u00d7(1+UV +T N). We set all the entries of M[CLS] to 0 to allow attention computation with all tokens.\n\nFinally, we vertically stack these attention masks,\n[M[CLS], M[MST], M[PATCH]], to generate the attention mask, M, for global spatio-temporal attention. The global spatiotemporal attention mechanism also includes an MLP and skip connection as summarized in the following,\n\nZl GST = MMSA(LN(Zl SlT ), M)) + Zl SlT Zl = MLP(Zl GST ) + Zl GST (2)\nWe propagate these updated token embeddings, Zl, to the next transformer layer. Finally, we use a linear projection of the [CLS] token from the last transformer layer as the video embedding for video-language alignment training.\n\nPretraining Objective For video-language alignment training, we use language supervision from both ASR subtitle, ts, and caption, tc. Let's assume s \u2208 RD, c \u2208 RD\nand v \u2208 RD are the encoded features vectors for subtitle, caption and video. We use the commonly used info-NCE loss [10] as the objective function for video-language alignment training. The overall objective function is\n\n$$\\mathcal{L}=\\frac{1}{B}\\sum_{i=1}^{B}(\\mathcal{L}_{vs}^{i})+\\frac{1}{B}\\sum_{i=1}^{B}(\\mathcal{L}_{vc}^{i})\\tag{3}$$\nwhere, Lvs and Lvc are info-NCE loss between video representation and the language representation from subtitle s and caption c respectively; for each loss,\n\n${\\mathcal{L}_{v t}^i=-\\log\\frac{\\exp(v_i^\\top t_i/\\tau)}{\\sum_{j=1}^{B}\\exp(v_i^\\top t_j/\\tau)}-\\log\\frac{\\exp(t_i^\\top v_i/\\tau)}{\\sum_{j=1}^{B}\\exp(t_i^\\top v_j/\\tau)}}$\n\nwhere ${t\\in\\{c,s\\}}$, ${B}$ is the batch size and ${\\tau}$ is the learnable temperature scale.\n"
    },
    {
        "level": "##",
        "title": "5. Experiments And Results",
        "content": "\nImplementation Details We initialize our text and video encoders form pretrained OpenAI CLIP [65] checkpoints. We randomly initialize the [MST] tokens. To ensure that the initializion of our video encoder is close to CLIP's vision encoder, we initialize the projection matrices of spatially local temporal attention with zero. During training, we uniformly sample 12 frames from each video clip. We use multi-scale random crop [83] with a ratio of 1.0 and 0.8\nto crop the video to 224 \u00d7 224 while preserving aspect ratio. We also apply random horizontal flip for augmentation. We train our models for 3 epochs. We use a initial learning rate of 2e \u2212 5 with cosine decay to 4e \u2212 8. For training, we utilize 128 A100 GPUs and set the batch size to 4096. We set the number of hierarchies, U, to 3, the number of\n[MST] tokens in each hierarchy, V , to 4, and the temporal scale r to 2. We provide additional training and finetuning implementation details in the Supplementary.\n\nVideo-Text Retrieval Datasets We evaluate our retrieval performance on MSR-VTT [92], DiDeMo [3], ActivityNet Captions [37], MSVD [9] and VATEX [89] datasets. On all these datasets, we finetune on the standard training split and test it on the standard test/val splits. Following prior works [6, 39, 91, 94], we concatenate the multiple descriptions to form a paragraph and perform paragraph-to-video retrieval on DiDeMo and ActivityNet Captions datasets.\n\nMain Results We compare the retrieval performance of our proposed method VidLA with other recent works on MSR-VTT, DideMo, ActivityNet Captions, MSVD, and VATEX datasets and report the results in Table 2 and 3. We use VidLA-X/Y to denote the variant of our model that\n\nMethod\nMSR-VTT Text-to-Video\nMSR-VTT Video-to-Text\nR@1\nR@5\nR@10\nAvg\nMdR\u2193\nMnR\u2193\nR@1\nR@5\nR@10\nAvg\nMdR\u2193\nMnR\u2193\nClipBERT [39]\n22.0\n46.8\n59.9\n42.9\n6.0\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\nSupport Set [64]\n30.1\n58.5\n69.3\n52.6\n3.0\n\u2212\n28.5\n58.6\n71.6\n52.9\n3.0\n\u2212\nHD-VILA [93]\n35.6\n65.3\n78.0\n59.6\n3.0\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\nAll-in-One [80]\n37.9\n68.1\n77.1\n61.0\n\u2212\n\u2212\n37.5\n66.1\n77.4\n60.3\n\u2212\n\u2212\nFrozen [6]\n32.5\n61.5\n71.2\n55.1\n3.0\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\nCLIP-ViT-B/32\nCLIP4Clip [55]\n44.5\n71.4\n81.6\n65.8\n2.0\n15.3\n42.7\n70.9\n80.6\n64.7\n2.0\n11.6\nCenterCLIP [103]\n44.2\n71.6\n82.1\n66.0\n2.0\n15.1\n42.8\n71.7\n82.2\n65.6\n2.0\n10.9\nCLIP2TV [27]\n46.1\n72.5\n82.9\n67.2\n2.0\n15.2\n43.9\n73.0\n82.8\n66.6\n2.0\n11.1\nCAMoE* [13]\n47.3\n74.2\n84.5\n68.7\n2.0\n11.9\n49.1\n74.3\n84.3\n69.2\n2.0\n9.9\nDRL [87]\n47.4\n74.6\n83.8\n68.6\n2.0\n\u2212\n45.3\n73.9\n83.3\n67.5\n2.0\n\u2212\nSTAN* [50]\n49.0\n74.8\n83.5\n69.1\n2.0\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\nPIDRo [31]\n48.2\n74.9\n83.3\n68.8\n2.0\n12.6\n47.4\n74.8\n84.1\n68.8\n2.0\n8.7\nCap4Video [91]\n49.3\n74.3\n83.8\n69.1\n2.0\n12.0\n47.1\n73.7\n84.3\n68.4\n2.0\n8.7\nUATVR* [22]\n49.8\n76.1\n85.5\n70.5\n2.0\n12.9\n51.1\n74.8\n85.1\n70.3\n1.0\n8.3\nCLIPViP [94]\n50.1\n74.8\n84.6\n69.8\n1.0\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\nCLIPViP* [94]\n55.9\n77.0\n86.8\n73.2\n1.0\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\nVidLA-B/32\n55.6\n79.7\n86.9\n74.1\n1.0\n11.4\n55.1\n79.9\n88.0\n74.3\n1.0\n6.9\nVidLA-B/32*\n60.9\u21915.0\n81.6\n89.4\n77.3\n1.0\n8.7\n60.8\u21919.7\n82.4\n89.1\n77.4\n1.0\n6.3\nCLIP-ViT-B/16\nBridgeFormer [28]\n37.6\n64.8\n75.1\n59.2\n3.0\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\nCLIP2TV [27]\n49.3\n74.7\n83.6\n69.2\n2.0\n13.5\n46.9\n75.0\n85.1\n69.0\n2.0\n10.0\nTS2-Net [51]\n49.4\n75.6\n85.3\n70.1\n2.0\n13.5\n46.6\n75.9\n84.9\n69.1\n2.0\n8.9\nCap4Video [91]\n51.4\n75.7\n83.9\n70.3\n1.0\n12.4\n49.0\n75.2\n85.0\n69.7\n2.0\n8.0\nDRL* [87]\n53.3\n80.3\n87.6\n73.7\n1.0\n\u2212\n56.2\n79.9\n87.4\n74.5\n1.0\n\u2212\nSTAN* [50]\n54.6\n78.4\n85.1\n72.7\n1.0\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\nPIDRo* [31]\n55.9\n79.8\n87.6\n74.4\n1.0\n10.7\n54.5\n78.3\n87.3\n73.4\n1.0\n7.5\nUATVR* [22]\n53.5\n79.5\n88.1\n73.7\n1.0\n10.2\n54.5\n79.1\n87.9\n73.8\n1.0\n7.6\nCLIPViP [94]\n54.2\n77.2\n84.8\n72.1\n1.0\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\nCLIPViP* [94]\n57.7\n80.5\n88.2\n75.5\n1.0\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\nVidLA-B/16\n58.0\n81.1\n87.8\n75.6\n1.0\n10.4\n56.1\n80.5\n88.7\n75.1\n1.0\n6.8\nVidLA-B/16*\n61.1\u21913.4\n83.8\n90.4\n78.4\n1.0\n8.1\n63.1\u21916.9\n84.7\n90.8\n79.5\n1.0\n6.1\nTwo Stage Models with Cross-Modal Fusion Re-Ranking\nVINDLU\u2020[12]\n46.5\n71.5\n80.4\n66.1\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\nUMT\u2020 [46]\n51.0\n76.5\n84.2\n70.6\n\u2212\n\u2212\n49.0\n77.0\n84.7\n70.2\n\u2212\n\u2212\nInternVideo(ViT-L)\u2020* [90]\n55.2\n79.6\n87.5\n74.1\n\u2212\n\u2212\n57.9\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\nuses ViT-X/Y as the vision encoder, e.g., VidLA-B/32 uses ViT-B/32 as the vision encoder. We present results with and without using dual-softmax [13] for score normalization prior to ranking at the inference stage. Our proposed method outperforms all prior works using a similar ViT backbone by a significant margin. Particularly, from results reported in Table 2, we observe that VidLA-B/32 outperforms the second best method, CLIP-ViP, by 5.5% on MSR- VTT for text-to-video retrieval in terms of R@1 without dual-softmax. We notice similar improvement (3.8%) with ViT-B/16 backbone. We also notice a large improvement on the video-to-text retrieval task. Table 3, demonstrates a similar pattern on other four datasets. Particularly, we observe a larger improvement on datasets with longer videos such as ActivityNet Captions and DiDeMo, where our proposed method outperforms the second best method, CLIP- ViP, by 8.4% and 10.1% respectively. These results demonstrate that our proposed method not only outperforms the prior best method but also attains larger improvement if the downstream dataset is temporally longer.\n"
    },
    {
        "level": "##",
        "title": "6. Analysis And Discussion",
        "content": "\nWe empirically validate our design choices on the model architecture, dataset temporal scales, language supervision as well as their combined effect by conducting a series of experiments to evaluate the model's retrieval performance. In all experiments, unless otherwise specified, we use the VidLA-B/32 model pretrained on an 80M subset of the YT- VidLA-800M dataset for 1 epoch, finetuned on MSR-VTT dataset. For these analysis experiments, we evaluate the retrieval performance without DSL. This 80M subset is constructed by sampling about 2M random source videos and then splitting them into short, medium and long clips as discussed in Section 3. For a fair comparison with other methods, we also utilize the same ViT-B/32 model as the vision encoder, initialized from the same CLIP checkpoint, and trained with the same compute and data budget.\n\nAttention Design To analyze the effectiveness of [MST]\nguided hierarchical temporal attention mechanism, we conduct a series of experiments with different attention configurations and report the results in Table 4. The first two\n\nMethod\nDiDeMo\nActivityNet Captions\nMSVD\nVatex\nR@1\nR@5\nR@10\nR@1\nR@5\nR@10\nR@1\nR@5\nR@10\nR@1\nR@5\nR@10\nClipBERT [39]\n20.4\n48.0\n60.8\n21.3\n49.0\n63.5\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\nSupport Set [64]\n\u2212\n\u2212\n\u2212\n29.2\n61.6\n\u2212\n28.4\n60.0\n72.9\n45.9\n82.4\n90.4\nHD-VILA [93]\n28.8\n57.4\n69.1\n28.5\n57.4\n94.0\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\nAll-in-One [80]\n32.7\n61.4\n73.5\n22.4\n53.7\n67.7\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\nFrozen [6]\n34.6\n65.0\n74.7\n\u2212\n\u2212\n\u2212\n33.7\n64.7\n76.3\n\u2212\n\u2212\n\u2212\nCLIP-ViT-B/32\nCLIP4Clip [55]\n43.4\n70.2\n80.6\n40.5\n72.4\n\u2212\n46.2\n76.1\n84.6\n\u2212\n\u2212\n\u2212\nCenterCLIP [103]\n\u2212\n\u2212\n\u2212\n43.9\n74.6\n85.8\n47.6\n77.5\n86.0\n\u2212\n\u2212\n\u2212\nCLIP2TV [27]\n45.5\n69.7\n80.6\n44.1\n75.2\n\u2212\n47.0\n76.5\n85.1\n\u2212\n\u2212\n\u2212\nCAMoE* [13]\n43.8\n71.4\n\u2212\n51.0\n77.7\n\u2212\n49.8\n79.2\n87.0\n\u2212\n\u2212\n\u2212\nDRL [87]\n47.9\n73.8\n82.7\n44.2\n74.5\n86.1\n48.3\n79.1\n87.3\n63.5\n91.7\n96.5\nSTAN* [50]\n51.3\n75.1\n83.4\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\nPIDRo* [31]\n48.6\n75.9\n84.4\n44.9\n74.5\n86.1\n47.5\n77.5\n86.0\n\u2212\n\u2212\n\u2212\nUATVR [22]\n43.1\n71.8\n82.3\n\u2212\n\u2212\n\u2212\n46.0\n76.3\n85.1\n61.3\n91.0\n95.6\nCLIPViP [94]\n48.6\n77.1\n84.4\n51.1\n78.4\n88.3\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\nCLIPViP* [94]\n53.8\n79.6\n86.5\n59.1\n83.9\n91.3\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\nVidLA-B/32\n56.9\n82.2\n89.2\n61.3\n84.8\n91.3\n48.6\n77.9\n85.7\n66.5\n86.2\n88.4\nVidLA-B/32*\n62.2\u21918.4\n84.6\n90.0\n69.2\u219110.1\n88.2\n93.3\n52.7\u21912.9\n80.4\n87.0\n73.7\u21917.2\n87.6\n89.1\nCLIP-ViT-B/16\nBridgeFormer [28]\n37.0\n62.2\n73.9\n\u2212\n\u2212\n\u2212\n52.0\n82.8\n90.0\n\u2212\n\u2212\n\u2212\nDRL [87]\n49.0\n76.5\n84.5\n46.2\n77.3\n88.2\n50.0\n81.5\n89.5\n65.7\n92.6\n96.7\nUATVR [22]\n45.8\n73.7\n83.3\n\u2212\n\u2212\n\u2212\n49.7\n79.0\n87.3\n64.5\n92.6\n96.8\nCap4Video [91]\n52.0\n79.4\n87.5\n\u2212\n\u2212\n\u2212\n51.8\n80.8\n88.3\n66.6\n93.1\n97.0\nCLIPViP [94]\n50.5\n78.4\n87.1\n53.4\n81.4\n90.0\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\nCLIPViP* [94]\n55.3\n82.0\n89.3\n61.4\n85.7\n92.6\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\nVidLA-B/16\n61.1\n83.7\n89.1\n65.2\n87.4\n92.8\n51.5\n79.9\n86.9\n69.2\n87.1\n88.9\nVidLA-B/16*\n64.8\u21916.9\n86.0\n91.8\n73.0\u219110.8\n89.9\n93.6\n55.9\u21913.9\n82.3\n88.3\n75.8\u21919.2\n88.3\n89.3\nTwo Stage Models with Cross-Modal Fusion Re-Ranking\nVINDLU\u2020[12]\n61.2\n85.8\n91.0\n55.0\n81.4\n89.7\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\nUMT\u2020 [46]\n61.6\n86.8\n91.5\n58.3\n83.9\n91.5\n71.9\n94.5\n97.8\n\u2212\n\u2212\n\u2212\nInternVideo(ViT-L)* [90]\n57.9\n82.4\n88.9\n62.2\n85.9\n93.2\n58.4\n84.5\n90.4\n71.1\n\u2212\n\u2212\n[MST]\nHierarchy\nLocal\nMSR-VTT Retrieval\nR@1\nR@5\nR@10\nAvg\n\u2717\n\u2717\n\u2717\n49.1\n75.3\n83.5\n69.3\n\u2713\n\u2717\n\u2717\n49.2\n77.6\n85.2\n70.7\n\u2713\n\u2713\n\u2717\n50.0\n77.6\n85.4\n71.0\n\u2713\n\u2717\n\u2713\n51.3\n76.5\n85.0\n70.9\n\u2713\n\u2713\n\u2713\n53.5\n77.5\n85.6\n72.2\nMulti-Scale\nMSR-VTT Retrieval\nR@1\nR@5\nR@10\nAvg\n\u2717\n51.9\n78.2\n85.6\n71.9\n\u2713\n53.5\n77.5\n85.6\n72.2\n\nrows demonstrate the effectiveness of [MST] tokens, even without any temporal hierarchy. Third row demonstrates the effectiveness of introducing multiple temporal hierarchies in [MST] tokens. On the other hand, the fourth row shows the effectiveness of spatially-local temporal attention, where it provides a significant improvement in terms of R@1 retrieval performance over the seon. Finally, the last row confirms the efficacy of our proposed temporal attention mechanism, providing a substantial 4.4% improvement over the baseline. Overall, these results not only validate the effectiveness of our proposed attention mechanism but also highlight the efficacy of its individual components.\n\nTemporal Scales in Pretraining Data To analyze the impact of incorporating multiple temporal scales in the proposed pretraining dataset, we compare a model pretrained on the 80M subset containing short, *medium* and *long* clips against a model trained on only *short* short clips from the same set of 2M videos. For a fair comparison, we train these models for same number of steps. We present the finetuned results in Table 5 and observe that including multiple scales in the pretraining dataset helps boost retrieval performance.\n\nRetrieval Performance on Videos of Different Lengths To conduct a more finegrained analysis of the performance of our method, in the left plot of Figure 4, we compare the performances of VidLA with respect to other attention methods on videos of different lengths.\n\nFor this analysis, we report MSR-VTT R@1 results for three splits of videos in the validation set. Particulalry, we sort the videos by length and pick the shortest third for the short split, longest third for the long split and the remaining for the medium split. We observe that VidLA consistently outperforms other methods on all splits of different video lengths.\n\n| Sub               | Cap   | Sum   |\n|-------------------|-------|-------|\n| MSR-VTT Retrieval |       |       |\n| R@1               | R@5   | R@10  |\n| \u2713                 | \u2713     | \u2717     |\n| 36.3              | 65.0  | 76.3  |\n| \u2717                 | \u2713     | \u2713     |\n| 48.9              | 74.1  | 84.0  |\n| \u2713                 | \u2717     | \u2713     |\n| 50.1              | 76.7  | 84.5  |\n| \u2713                 | \u2713     | \u2713     |\n| 53.5              |       |       |\n| 77.5              | 85.6  |       |\n| 72.2              |       |       |\n\nTraining Data Size It is well-known that performance of retrieval models scales with the pretraining data size in the contrastive learning setting. We study our model's performance as a function of the pretraining dataset size by pretraining different models on datasets of sizes 80M, 10M and 1M. We report the results in the right plot on Figure 4 and compare the performance of VidLA with other attention methods. We notice that VidLA outperforms all the methods across all data scales.\n\nEffect of Different Language Supervision To validate the efficacy of utilizing both subtitles and captions for language supervision, as well as the effectiveness of text summarization, we pretrain our model with different combinations of text sources and summarization. From the results presented in Table 6, we observe that the model's performance is better with supervision from both subtitles and captions compared to using only one of the two. Additionally, removing summarization significantly degrades performance. Without summarization, video-text alignment suffers due to increased verbosity in longer videos and the inability to leverage CLIP's pretrained embedding layer due to increased context length.\n\n| Method             | Frames     |\n|--------------------|------------|\n| K400               | Sth-sth-v2 |\n| Views              | Top-1      |\n| TimeSformer-B/16 [ | 7          |\n| 1                  |            |\n| \u00d7                  |            |\n| 3                  |            |\n| 80.7               |            |\n| 1                  |            |\n| \u00d7                  |            |\n| 3                  |            |\n| 62.4               |            |\n| VideoMAE-B/16 [    | 77         |\n| 5                  |            |\n| \u00d7                  |            |\n| 3                  |            |\n| 81.5               |            |\n| 2                  |            |\n| \u00d7                  |            |\n| 3                  |            |\n| 70.8               |            |\n| VideoMAE-v2-B/16 [ | 82         |\n| 5                  |            |\n| \u00d7                  |            |\n| 3                  |            |\n| 81.5               |            |\n| 2                  |            |\n| \u00d7                  |            |\n| 3                  |            |\n| 71.2               |            |\n| ViViT-L/16 [       | 4          |\n| 1                  |            |\n| \u00d7                  |            |\n| 3                  |            |\n| 81.7               |            |\n| 1                  |            |\n| \u00d7                  |            |\n| 1                  |            |\n| 65.9               |            |\n| VideoSwin-B [      | 53         |\n| 3                  |            |\n| \u00d7                  |            |\n| 4                  |            |\n| 82.7               |            |\n| 1                  |            |\n| \u00d7                  |            |\n| 3                  |            |\n| 69.6               |            |\n| UMT-B/16           |            |\n| 800                | e          |\n| [                  | 46         |\n| 3                  |            |\n| \u00d7                  |            |\n| 4                  |            |\n| 85.7               |            |\n| 2                  |            |\n| \u00d7                  |            |\n| 3                  |            |\n| 70.8               |            |\n| VidLA-B/32         | 16         |\n| 5                  |            |\n| \u00d7                  |            |\n| 3                  |            |\n| 82.4               |            |\n| 2                  |            |\n| \u00d7                  |            |\n| 3                  |            |\n| 67.9               |            |\n| VidLA-B/16         | 16         |\n| 5                  |            |\n| \u00d7                  |            |\n| 3                  |            |\n| 84.9               |            |\n| 2                  |            |\n| \u00d7                  |            |\n| 3                  |            |\n| 69.9               |            |\n\nClassification Results Even though our proposed method primarily focuses on video-language alignment, we evaluate the performance of our method on a related downstream task, *i.e*., action recognition. We add a classification head on top of the video encoder from VidLA and finetune it on the popular benchmark datasets Kinetics-400 [36] and Something-Something-V2 [30].\n\nWe report the results of the finetuned models in Table 7. Although VidLA was pretrained only for video-language alignment, we observe that VidLA performs competitively even against models such as VideoMAE that use dense pretraining objectives to promote the learning of finegrained features.\n"
    },
    {
        "level": "##",
        "title": "7. Conclusion",
        "content": "\nIn this work, we propose a novel hierarchical temporal modeling architecture that captures temporal relationships at multiple temporal scales while remaining flexible to leverage image-text pretrained models. We also introduce an approach for utilizing LLMs to create the largest videolanguage dataset with better semantic alignment between video and language. We empirically validate the efficacy of our proposed hierarchical temporal attention mechanism as well as its design choices on data with varying temporal lengths and at different dataset sizes, demonstrating its advantage over other performant temporal modeling approaches. Our extensive experimentation also validates our data curation choices. Overall, our results highlight the importance of both high-quality large-scale training data as well as simple and scalable temporal architecture, and establishes VidLA as the new state-of-the-art on multiple video-text retrieval benchmarks while demonstrating its competitiveness on classification benchmarks.\n"
    },
    {
        "level": "##",
        "title": "References",
        "content": "\n[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. *Advances in Neural Information Processing Systems*,\n35:23716\u201323736, 2022. 2\n[2] Dario Amodei,\nSundaram Ananthanarayanan,\nRishita\nAnubhai, Jingliang Bai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Qiang Cheng, Guoliang Chen, et al. Deep speech 2: End-to-end speech recognition in english and mandarin. In International conference on machine learning, pages 173\u2013182. PMLR, 2016. 1\n[3] Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef\nSivic, Trevor Darrell, and Bryan Russell. Localizing moments in video with natural language. In Proceedings of the IEEE international conference on computer vision, pages\n5803\u20135812, 2017. 3, 5\n[4] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen\nSun, Mario Lu\u02c7ci\u00b4c, and Cordelia Schmid. Vivit: A video\nvision transformer. In Proceedings of the IEEE/CVF international conference on computer vision, pages 6836\u20136846,\n2021. 1, 8\n[5] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.\nLayer normalization.\narXiv preprint arXiv:1607.06450,\n2016. 4\n[6] Max Bain, Arsha Nagrani, G\u00a8ul Varol, and Andrew Zisserman.\nFrozen in time: A joint video and image encoder\nfor end-to-end retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1728\u2013 1738, 2021. 1, 3, 5, 6, 7\n[7] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is\nspace-time attention all you need for video understanding? In *ICML*, volume 2, page 4, 2021. 1, 2, 8\n[8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020. 2\n[9] David Chen and William B Dolan. Collecting highly parallel data for paraphrase evaluation. In Proceedings of the\n49th annual meeting of the association for computational\nlinguistics: human language technologies, pages 190\u2013200, 2011. 5\n[10] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on\nmachine learning, pages 1597\u20131607. PMLR, 2020. 5\n[11] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al. Pali: A jointly-scaled multilingual language-image model. arXiv\npreprint arXiv:2209.06794, 2022. 1, 3\n[12] Feng Cheng, Xizi Wang, Jie Lei, David Crandall, Mohit\nBansal, and Gedas Bertasius.\nVindlu: A recipe for effective video-and-language pretraining. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10739\u201310750, 2023. 1, 2, 6, 7\n\n[13] Xing Cheng, Hezheng Lin, Xiangyu Wu, Fan Yang, and\nDong Shen. Improving video-text retrieval by multi-stream corpus alignment and dual softmax loss.\narXiv preprint\narXiv:2109.04290, 2021. 6, 7, 2, 3\n[14] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2023. 2\n[15] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. *arXiv preprint arXiv:2204.02311*, 2022.\n[16] Hyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, et al.\nScaling\ninstruction-finetuned language models.\narXiv preprint\narXiv:2210.11416, 2022. 2\n[17] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat\nTiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi.\nInstructblip: Towards generalpurpose vision-language models with instruction tuning, 2023. 2\n[18] Karan Desai, Gaurav Kaul, Zubin Aysola, and Justin Johnson. Redcaps: Web-curated image-text data created by the people, for the people. *arXiv preprint arXiv:2111.11431*, 2021. 2\n[19] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. BERT: pre-training of deep bidirectional transformers for language understanding.\nIn Jill Burstein,\nChristy Doran, and Thamar Solorio, editors, Proceedings of\nthe 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN,\nUSA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 4171\u20134186. Association for Computational Linguistics, 2019. 4\n[20] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova.\nBert:\nPre-training of deep bidirectional\ntransformers for language understanding.\narXiv preprint\narXiv:1810.04805, 2018. 3\n[21] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.\nAn image is worth 16x16 words:\nTransformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 1, 3\n[22] Bo Fang, Wenhao Wu, Chang Liu, Yu Zhou, Yuxin Song,\nWeiping Wang, Xiangbo Shu, Xiangyang Ji, and Jingdong Wang.\nUatvr: Uncertainty-adaptive text-video retrieval.\nIn Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 13723\u201313733, October 2023. 6, 7\n[23] Han Fang, Pengfei Xiong, Luhui Xu, and Yu Chen.\nClip2video: Mastering video-text retrieval via image clip. arXiv preprint arXiv:2106.11097, 2021. 2\n[24] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and\nKaiming He. Slowfast networks for video recognition. In\nProceedings of the IEEE/CVF international conference on\ncomputer vision, pages 6202\u20136211, 2019. 2, 4, 1\n[25] Christoph Feichtenhofer, Axel Pinz, and Andrew Zisserman. Convolutional two-stream network fusion for video action recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1933\u2013 1941, 2016. 4\n[26] Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, William Yang\nWang, Lijuan Wang, and Zicheng Liu.\nViolet: End-toend video-language transformers with masked visual-token modeling. *arXiv preprint arXiv:2111.12681*, 2021. 1\n[27] Zijian Gao, Jingyu Liu, Weiqi Sun, Sheng Chen, Dedan\nChang, and Lili Zhao. Clip2tv: Align, match and distill for video-text retrieval. *arXiv preprint arXiv:2111.05610*,\n2021. 6, 7\n[28] Yuying Ge, Yixiao Ge, Xihui Liu, Dian Li, Ying Shan, Xiaohu Qie, and Ping Luo. Bridging video-text retrieval with multiple choice questions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16167\u201316176, 2022. 6, 7\n[29] Satya Krishna Gorti, No\u00a8el Vouitsis, Junwei Ma, Keyvan\nGolestan, Maksims Volkovs, Animesh Garg, and Guangwei Yu. X-pool: Cross-modal language-video attention for text-video retrieval. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 5006\u20135015, 2022. 2\n[30] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The \"something something\" video database for learning and evaluating visual common sense. In *ICCV*, volume 1, page 5, 2017. 8\n[31] Peiyan Guan, Renjing Pei, Bin Shao, Jianzhuang Liu,\nWeimian Li, Jiaxi Gu, Hang Xu, Songcen Xu, Youliang Yan, and Edmund Y. Lam.\nPidro: Parallel isomeric attention with dynamic routing for text-video retrieval.\nIn\nProceedings of the IEEE/CVF International Conference\non Computer Vision (ICCV), pages 11164\u201311173, October\n2023. 6, 7\n[32] Tengda Han, Weidi Xie, and Andrew Zisserman. Temporal\nalignment networks for long-term video. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 2906\u20132916, 2022. 1\n[33] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna\nMorrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly.\nParameter-efficient\ntransfer learning for nlp. In International Conference on\nMachine Learning, pages 2790\u20132799. PMLR, 2019. 2\n[34] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana\nParekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig.\nScaling up visual and vision-language\nrepresentation learning with noisy text supervision. In International conference on machine learning, pages 4904\u2013 4916. PMLR, 2021. 2\n[35] Peng Jin, Hao Li, Zesen Cheng, Kehan Li, Xiangyang Ji,\nChang Liu, Li Yuan, and Jie Chen. Diffusionret: Generative text-video retrieval with diffusion model. arXiv preprint\narXiv:2303.09867, 2023. 2, 3\n[36] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,\nChloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, Mustafa Suleyman, and Andrew Zisserman. The kinetics human action video dataset, 2017. 8\n[37] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and\nJuan Carlos Niebles. Dense-captioning events in videos. In\nProceedings of the IEEE international conference on computer vision, pages 706\u2013715, 2017. 5\n[38] Jie Lei, Tamara L Berg, and Mohit Bansal. Revealing single\nframe bias for video-and-language learning. arXiv preprint\narXiv:2206.03428, 2022. 1\n[39] Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L Berg,\nMohit Bansal, and Jingjing Liu. Less is more: Clipbert for video-and-language learning via sparse sampling. In Proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 7331\u20137341, 2021. 5, 6, 7\n[40] Chenliang Li, Haiyang Xu, Junfeng Tian, Wei Wang, Ming\nYan, Bin Bi, Jiabo Ye, Hehong Chen, Guohai Xu, Zheng Cao, et al. mplug: Effective and efficient vision-language learning by cross-modal skip-connections. arXiv preprint arXiv:2205.12005, 2022. 1\n[41] Gen Li, Nan Duan, Yuejian Fang, Ming Gong, and Daxin\nJiang.\nUnicoder-vl: A universal encoder for vision and\nlanguage by cross-modal pre-training. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 11336\u201311344, 2020. 1\n[42] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023. 1, 2, 3\n[43] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\nBlip: Bootstrapping language-image pre-training for unified vision-language understanding and generation.\nIn\nInternational Conference on Machine Learning, pages 12888\u201312900. PMLR, 2022. 1\n[44] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare,\nShafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. Align before fuse:\nVision and language representation\nlearning with momentum distillation. Advances in neural\ninformation processing systems, 34:9694\u20139705, 2021. 1, 2, 3\n[45] Kunchang Li, Yali Wang, Peng Gao, Guanglu Song, Yu Liu,\nHongsheng Li, and Yu Qiao.\nUniformer: Unified transformer for efficient spatial-temporal representation learning. In The Tenth International Conference on Learning\nRepresentations, ICLR 2022, Virtual Event, April 25-29,\n2022. OpenReview.net, 2022. 1, 2\n[46] Kunchang Li, Yali Wang, Yizhuo Li, Yi Wang, Yinan He,\nLimin Wang, and Yu Qiao. Unmasked teacher: Towards training-efficient video foundation models. arXiv preprint arXiv:2303.16058, 2023. 1, 6, 7, 8\n[47] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, et al.\nOscar: Object-semantics aligned pretraining for vision-language tasks.\nIn Computer Vision\u2013\nECCV 2020: 16th European Conference, Glasgow, UK,\nAugust 23\u201328, 2020, Proceedings, Part XXX 16, pages 121\u2013\n137. Springer, 2020. 1\n\n[48] Yanghao Li, Chao-Yuan Wu, Haoqi Fan, Karttikeya Mangalam, Bo Xiong, Jitendra Malik, and Christoph Feichtenhofer. Mvitv2: Improved multiscale vision transformers for classification and detection. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 4804\u20134814, 2022. 1\n[49] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee.\nVisual\ninstruction\ntuning.\narXiv\npreprint\narXiv:2304.08485, 2023. 2\n[50] Ruyang Liu, Jingjia Huang, Ge Li, Jiashi Feng, Xinglong\nWu, and Thomas H Li. Revisiting temporal modeling for clip-based image-to-video knowledge transferring. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 6555\u20136564, 2023. 2, 6, 7\n[51] Yuqi Liu, Pengfei Xiong, Luhui Xu, Shengming Cao, and\nQin Jin. Ts2-net: Token shift and selection transformer for text-video retrieval. In European Conference on Computer\nVision, pages 319\u2013335. Springer, 2022. 6\n[52] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang,\nStephen Lin, and Han Hu. Video swin transformer. In Proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 3202\u20133211, 2022. 1\n[53] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang,\nStephen Lin, and Han Hu. Video swin transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 3202\u20133211, June\n2022. 8\n[54] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. *arXiv preprint arXiv:1711.05101*, 2017. 1\n[55] Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei,\nNan Duan, and Tianrui Li. Clip4clip: An empirical study of clip for end to end video clip retrieval and captioning. Neurocomputing, 508:293\u2013304, 2022. 2, 6, 7, 1, 3\n[56] Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan\nLaptev, Josef Sivic, and Andrew Zisserman. End-to-end learning of visual representations from uncurated instructional videos. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 9879\u2013 9889, 2020. 1\n[57] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac,\nMakarand\nTapaswi,\nIvan\nLaptev,\nand\nJosef\nSivic.\nHowto100m: Learning a text-video embedding by watching hundred million narrated video clips. In Proceedings\nof the IEEE/CVF international conference on computer vision, pages 2630\u20132640, 2019. 1, 3\n[58] Liliane Momeni, Mathilde Caron, Arsha Nagrani, Andrew\nZisserman, and Cordelia Schmid.\nVerbs in action: Improving verb understanding in video-language models. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 15579\u201315591, 2023. 2\n[59] Arsha Nagrani, Paul Hongsuck Seo, Bryan Seybold, Anja\nHauth, Santiago Manen, Chen Sun, and Cordelia Schmid. Learning audio-video modalities from image captions. In European Conference on Computer Vision, pages 407\u2013426.\nSpringer, 2022. 1, 3\n[60] Daniel Neimark, Omri Bar, Maya Zohar, and Dotan Asselmann. Video transformer network. In Proceedings of\nthe IEEE/CVF international conference on computer vision, pages 3163\u20133172, 2021. 1\n[61] Van-Quang Nguyen, Masanori Suganuma, and Takayuki\nOkatani. Grit: Faster and better image captioning transformer using dual visual features. In European Conference on Computer Vision, pages 167\u2013184. Springer, 2022. 1\n[62] Bolin Ni, Houwen Peng, Minghao Chen, Songyang Zhang,\nGaofeng Meng, Jianlong Fu, Shiming Xiang, and Haibin Ling.\nExpanding language-image pretrained models for\ngeneral video recognition.\nIn European Conference on\nComputer Vision, pages 1\u201318. Springer, 2022. 2\n[63] Junting Pan, Ziyi Lin, Xiatian Zhu, Jing Shao, and Hongsheng Li. St-adapter: Parameter-efficient image-to-video transfer learning. Advances in Neural Information Processing Systems, 35:26462\u201326477, 2022. 2\n[64] Mandela Patrick, Po-Yao Huang, Yuki Asano, Florian\nMetze, Alexander Hauptmann, Joao Henriques, and Andrea Vedaldi. Support-set bottlenecks for video-text representation learning. *arXiv preprint arXiv:2010.02824*, 2020. 6,\n7\n[65] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In *International conference on machine learning*, pages 8748\u20138763. PMLR, 2021. 1, 2, 3, 5\n[66] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman,\nChristine McLeavey, and Ilya Sutskever.\nRobust speech\nrecognition via large-scale weak supervision.\nIn International Conference on Machine Learning, pages 28492\u2013\n28518. PMLR, 2023. 1\n[67] Maithra Raghu, Thomas Unterthiner, Simon Kornblith,\nChiyuan Zhang, and Alexey Dosovitskiy. Do vision transformers see like convolutional neural networks? Advances in Neural Information Processing Systems, 34:12116\u2013 12128, 2021. 1\n[68] Chaitanya Ryali, Yuan-Ting Hu, Daniel Bolya, Chen Wei,\nHaoqi Fan, Po-Yao Huang, Vaibhav Aggarwal, Arkabandhu Chowdhury, Omid Poursaeed, Judy Hoffman, et al.\nHiera: A hierarchical vision transformer without the bellsand-whistles. *arXiv preprint arXiv:2306.00989*, 2023. 1\n[69] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:25278\u201325294, 2022. 2, 3\n[70] Christoph Schuhmann, Richard Vencu, Romain Beaumont,\nRobert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki.\nLaion-\n400m: Open dataset of clip-filtered 400 million image-text pairs. *arXiv preprint arXiv:2111.02114*, 2021.\n[71] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu\nSoricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pages\n2556\u20132565, 2018. 2\n[72] Nina Shvetsova, Anna Kukleva, Xudong Hong, Christian\nRupprecht, Bernt Schiele, and Hilde Kuehne. Howtocaption: Prompting llms to transform video annotations at scale. *arXiv preprint arXiv:2310.04900*, 2023. 1\n\n[73] Karen Simonyan and Andrew Zisserman. Two-stream convolutional networks for action recognition in videos. Advances in neural information processing systems, 27, 2014. 4\n[74] Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela. Flava: A foundational language and vision alignment model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n15638\u201315650, 2022. 1, 2\n[75] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue\nCao.\nEva-clip: Improved training techniques for clip at\nscale. *arXiv preprint arXiv:2303.15389*, 2023. 3\n[76] Bart Thomee, David A. Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth,\nand Li-Jia Li. Yfcc100m: The new data in multimedia research. *Commun. ACM*, 59(2):64\u201373, jan 2016. 2\n[77] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang.\nVideomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training. Advances in neural\ninformation processing systems, 35:10078\u201310093, 2022. 8\n[78] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,\nAmjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 2, 3, 1\n[79] Junke Wang, Dongdong Chen, Zuxuan Wu, Chong Luo,\nLuowei Zhou, Yucheng Zhao, Yujia Xie, Ce Liu, Yu-Gang Jiang, and Lu Yuan.\nOmnivl:\nOne foundation model\nfor image-language and video-language tasks.\nAdvances\nin neural information processing systems, 35:5696\u20135710, 2022. 1, 2\n[80] Jinpeng\nWang,\nYixiao\nGe,\nRui\nYan,\nYuying\nGe,\nKevin Qinghong Lin, Satoshi Tsutsui, Xudong Lin, Guanyu Cai, Jianping Wu, Ying Shan, et al. All in one: Exploring unified video-language pre-training. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 6598\u20136608, 2023. 6, 7, 1\n[81] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li,\nKevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang. Git: A generative image-to-text transformer for vision and language. *arXiv preprint arXiv:2205.14100*, 2022.\n3\n[82] Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yinan He, Yi Wang, Yali Wang, and Yu Qiao. Videomae v2: Scaling video masked autoencoders with dual masking. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 14549\u201314560, June 2023. 8\n[83] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua\nLin, Xiaoou Tang, and Luc Van Gool.\nTemporal segment networks for action recognition in videos.\nIEEE\ntransactions on pattern analysis and machine intelligence,\n41(11):2740\u20132755, 2018. 5\n[84] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua\nLin, Xiaoou Tang, and Luc Van Gool.\nTemporal seg-\nment networks for action recognition in videos.\n\nIEEE\nTransactions on Pattern Analysis and Machine Intelligence, 41(11):2740\u20132755, 2019. 1\n\n[85] Mengmeng Wang, Jiazheng Xing, and Yong Liu. Actionclip: A new paradigm for video action recognition. arXiv preprint arXiv:2109.08472, 2021. 2\n[86] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai,\nZhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang.\nOfa: Unifying architectures, tasks, and\nmodalities through a simple sequence-to-sequence learning framework. In International Conference on Machine Learning, pages 23318\u201323340. PMLR, 2022. 1\n[87] Qiang Wang, Yanhao Zhang, Yun Zheng, Pan Pan, and\nXian-Sheng Hua.\nDisentangled representation learning\nfor text-video retrieval. *arXiv preprint arXiv:2203.07111*,\n2022. 6, 7, 2, 3\n[88] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, et al. Image as a foreign language: Beit pretraining for all vision and visionlanguage tasks. *arXiv preprint arXiv:2208.10442*, 2022. 1\n[89] Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang\nWang, and William Yang Wang.\nVatex: A large-scale,\nhigh-quality multilingual dataset for video-and-language research. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4581\u20134591, 2019. 5\n[90] Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun\nHuang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun Wang, et al. Internvideo: General video foundation models via generative and discriminative learning. arXiv preprint arXiv:2212.03191, 2022. 6, 7, 1\n[91] Wenhao Wu, Haipeng Luo, Bo Fang, Jingdong Wang, and\nWanli Ouyang. Cap4video: What can auxiliary captions do for text-video retrieval? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10704\u201310713, 2023. 5, 6, 7\n[92] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large\nvideo description dataset for bridging video and language. In Proceedings of the IEEE conference on computer vision\nand pattern recognition, pages 5288\u20135296, 2016. 3, 5\n[93] Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong\nSun, Bei Liu, Huan Yang, Jianlong Fu, and Baining Guo. Advancing high-resolution video-language representation with large-scale video transcriptions.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5036\u20135045, 2022. 1, 3, 6, 7\n[94] Hongwei Xue, Yuchong Sun, Bei Liu, Jianlong Fu, Ruihua\nSong, Houqiang Li, and Jiebo Luo. Clip-vip: Adapting pretrained image-text model to video-language representation alignment. *arXiv preprint arXiv:2209.06430*, 2022. 1, 2, 3, 5, 6, 7\n[95] Shen Yan, Tao Zhu, Zirui Wang, Yuan Cao, Mi Zhang,\nSoham Ghosh, Yonghui Wu, and Jiahui Yu.\nVideococa:\nVideo-text modeling with zero-shot transfer from contrastive captioners. *arXiv preprint arXiv:2212.04979*, 2022.\n1\n[96] Taojiannan Yang, Yi Zhu, Yusheng Xie, Aston Zhang,\nChen Chen, and Mu Li.\nAim: Adapting image mod-\nels for efficient video action recognition.\n\narXiv preprint arXiv:2302.03024, 2023. 2\n\n[97] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe\nNiu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and Chunjing Xu.\nFilip: Fine-grained interactive languageimage pre-training.\narXiv preprint arXiv:2111.07783,\n2021. 2\n[98] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan,\nYiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al.\nmplug-owl: Modularization empowers\nlarge language models with multimodality. arXiv preprint arXiv:2304.14178, 2023. 2\n[99] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung,\nMojtaba Seyedhosseini, and Yonghui Wu.\nCoca: Contrastive captioners are image-text foundation models. arXiv\npreprint arXiv:2205.01917, 2022. 1, 2\n[100] Rowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu, Yanpeng Zhao, Mohammadreza Salehi, Aditya Kusupati, Jack\nHessel, Ali Farhadi, and Yejin Choi. Merlot reserve: Neural script knowledge through vision and language and sound. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16375\u201316387, 2022. 1, 3\n[101] Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu,\nJae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi. Merlot: Multimodal neural script knowledge models. Advances\nin Neural Information Processing Systems, 34:23634\u2013 23651, 2021. 3\n[102] Yu Zhang, James Qin, Daniel S Park, Wei Han, Chung-\nCheng Chiu, Ruoming Pang, Quoc V Le, and Yonghui Wu.\nPushing the limits of semi-supervised learning for automatic speech recognition.\narXiv preprint\narXiv:2010.10504, 2020. 1\n[103] Shuai Zhao, Linchao Zhu, Xiaohan Wang, and Yi Yang.\nCenterclip: Token clustering for efficient text-video retrieval. In Proceedings of the 45th International ACM SI-\nGIR Conference on Research and Development in Information Retrieval, pages 970\u2013981, 2022. 6, 7, 2, 3\n[104] Bolei Zhou, Alex Andonian, Aude Oliva, and Antonio Torralba.\nTemporal relational reasoning in videos.\nIn Proceedings of the European conference on computer vision\n(ECCV), pages 803\u2013818, 2018. 2\n[105] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 2\n"
    },
    {
        "level": "##",
        "title": "A. Implementation Details",
        "content": "\nIn this section, we provide additional implementation details of pretraining as well finetuning for both retrieval and classification tasks. We also include additional details regarding YT-VidLA-800M creation.\n"
    },
    {
        "level": "##",
        "title": "A.1. Additional Pretraining Details",
        "content": "\nFor pretraining, we set the weight decay to 0.02. We use AdamW [54] optimizer and set \u03b21 and \u03b22 to 0.9 and 0.999\nrespectively. We set gradient clipping to 10.\n"
    },
    {
        "level": "##",
        "title": "A.2. Finetuning Details For Retrieval",
        "content": "\nFor retrieval finetuning on all downstream datasets, we utilize 32 frames. For updating model parameters, we utilize AdamW optimizer with an initial learning rate of 1e \u2212 5\nand decay it to 1e \u2212 6 using cosine annealing.\n\nWe set the weight decay to 0.2. For VidLA-B/32 finetuning on all datasets, we set the batch size to 128, whereas, for VidLA-\nB/16 finetuning, we set the batchsize to 196. Unlike pretraining, we do not perform any multi-scale spatial crop for finetuning, rather, we perform center crop while preserving aspect ratio. We train VidLA-B/32 model for 440 steps on MSVD dataset. We respectively train VidLA-B/32 for\n2.5\u00d7, 2.5\u00d7, 2\u00d7, and 6\u00d7 steps on MSR-VTT, DiDeMo, ActivityNet Captions, and VATEX datasets. For VidLA-\nB/model, we train for 4\u00d7, 1.5\u00d7, 2\u00d7, 1\u00d7, and 6\u00d7 steps on MSR-VTT, DiDeMo, ActivityNet Captions, MSVD and VATEX datasets, respectively.\n"
    },
    {
        "level": "##",
        "title": "A.3. Finetuning Details For Classification",
        "content": "\nFor finetuning the video encoders from the pre-trained VidLA-B/32 for classification, we train for 100 epochs with a batch size of 512 for Kinetics-400 and 40 epochs with batch size 1024 for Something-Something-v2. For VidLA- B/16, we train for 75 epochs and 40 epochs respectively for Kinetics-400 and Something-Something-v2, each with a batch size of 512. We use the dense sampling approach of [24] for Kinetics-400, and TSN [84] uniform sampling for Something-Something-v2.\n"
    },
    {
        "level": "##",
        "title": "A.4. Additional Details For Yt-Vidla-800M Creation",
        "content": "\nFor caption generation, we use Blip-2 ViT-L OPT2.7B [42].\n\nWe utilize Llama-2 13B Chat [78] model for caption and ASR subtitle summarization. We provide the summarization prompt in the following.\n\n Summarize the following sentences\ninto a single sentence, not exceeding\n25 words.\n           Do not output any additional\ntext and use any external information.\n<input text>\n"
    },
    {
        "level": "##",
        "title": "B. Zero-Shot Retrieval",
        "content": "\nWe evaluate the zero-shot retrieval performance of VidLA- B/16 on the MSR-VTT dataset. We present the results in Table 8. We compare the performance with both two-tower and two-stage methods, without specific constraints on vision backbone size (e.g., ViT-B/16). We observe that VidLA outperforms all the prior works by a significant margin that employ similar-sized architectures.\n\nTo be more specific, VidLA outperforms the recent two-stage method UMT [46] by 6%. Surprisingly, we also find that VidLA outperforms InternVideo [90] by 0.8%, even though InternVideo utilizes a larger vision backbone. These results provide further evidence for the effectiveness of VidLA in the video-language alignment task.\n\n|                       |      | Method   |   R@1 |\n|-----------------------|------|----------|-------|\n| Frozen [              |  6   | ]        |  18.7 |\n| VIOLET [              | 26   | ]        |  25.9 |\n| CLIP4Clip [           | 55   | ]        |  30.6 |\n| VINDLU                |      |          |       |\n| \u2020                     |      |          |       |\n| [                     | 12   | ]        |  32   |\n| Singularity [         | 38   | ]        |  34   |\n| OmniVL [              | 79   | ]        |  34.6 |\n| VideoCoCa [           | 95   | ]        |  34.3 |\n| UMT-B                 |      |          |       |\n| \u2020                     |      |          |       |\n| [                     | 46   | ]        |  35.5 |\n| InternVideo(ViT-L)* [ | 90   | ]        |  40.7 |\n| VidLA-B/16            | 36.6 |          |       |\n| VidLA-B/16*           |      |          |       |\n| 41.5                  |      |          |       |\n"
    },
    {
        "level": "##",
        "title": "C. Zero-Shot Multiple Choice",
        "content": "\nThe objective of the zero-shot multiple choice task is to find the correct answer from a list of options. We evaluate the performance of VidLA-B/16 on the MSR-VTT dataset for this task and report the results in Table 9. We observe that VidLA outperforms the previous state-of-the-art, InternVideo, by 0.9%. We also notice that VidLA outperforms All-in-one by 11.9%. Overall, these results further demonstrate the generalizability of our large-scale video-language alignment training.\n\n|                      |    | Method   |   Accuracy |\n|----------------------|----|----------|------------|\n| All-in-one [         | 80 | ]        |       80.3 |\n| InternVideo(ViT-B) [ | 90 | ]        |       91.3 |\n| VidLA-B/16           |    |          |            |\n| 92.2                 |    |          |            |\n"
    },
    {
        "level": "##",
        "title": "D. Zero-Shot Classification",
        "content": "\nWe evaluate the performance of VidLA-B/16 on the zero-shot action classification task using the Kinetics-400\ndataset. We only compare with methods that do not utilize Kinetics-400 data (labels and videos) for pretraining. To compare with prior art, we report results on both validation and test sets of Kinetics-400. We perform a 'single-view'\nevaluation and utilize uniformly sampled 32 frames. For evaluation, following CLIP [65], we obtain the embedding of all the class labels using the text encoder while utilizing the default prompt templates for Kinetics-400 classification. We consider the prediction as correct if the groundtruth label has the maximum similarity with the video embedding. We report the results in Table 10.\n\nWe observe that our proposed method outperforms all prior works by a noticeable margin. To be precise, VidLA outperforms CLIP by around 11% on both validation and test sets. VidLA also outperforms, ActionCLIP [85] by 3.8%.\n\nWe also notice that VidLA outperforms current state-of-the-art VFC [58] on both validation and test sets. Note that VFC [58] employs a verb focused contrastive loss during pretraining which greatly improves performance on action recognition task since the extracted verbs are, in principle, very similar to the downstream action recognition labels. Therefore, we also report a baseline score from VFC [58] which does not employ such a verb focused contrastive objective. VidLA outperforms the VFC-baseline by 3.5% on the Kinetics-400 test set. These results further validate the effectiveness of VidLA in action recognition, complementing the finetuned results reported in Table 7 of the main text.\n\n| Method         |   Top-1 Accuracy |\n|----------------|------------------|\n| Validation-Set |                  |\n| CLIP [         |               65 |\n| ActionCLIP [   |               85 |\n| VFC [          |               58 |\n| VidLA-B/16     |                  |\n| 60.2           |                  |\n| Test-Set       |                  |\n| Flamingo-3B [  |                1 |\n| Flamingo-80B [ |                1 |\n| Flamingo-9B [  |                1 |\n| CLIP [         |               65 |\n| VFC-Baseline [ |               58 |\n| VFC [          |               58 |\n| VidLA-B/16     |                  |\n| 59.1           |                  |\n"
    },
    {
        "level": "##",
        "title": "E. Video-To-Text Retrieval Results",
        "content": "\nWe present video-to-text retrieval results on DiDeMo, ActivityNet Captions, MSVD, and VATEX datasets in Table 11, 12, 13, and 14 respectively. Similar to the text-tovideo retrieval results reported in the main text, we observe that VidLA outperforms prior art on video-to-text retrieval by a significant margin in most settings.\n\nHowever, InternVideo outperforms VidLA-B/16 on the VATEX dataset, likely due to its larger backbone architecture.\n\nMethod\nDiDeMo Retrieval\nR@1\nR@5\nR@10\nAvg\nCLIP-ViT-B/32\nCLIP4Clip [55]\n42.5\n70.6\n80.2\n64.4\nCAMoE* [13]\n45.5\n71.2\n\u2212\n\u2212\nDRL [87]\n45.4\n72.6\n82.1\n66.7\nDiffusionRet* [35]\n50.3\n75.1\n82.9\n69.4\nVidLA-B/32\n55.0\n82.8\n88.9\n75.6\nVidLA-B/32*\n62.2\n85.1\n91.1\n79.5\nCLIP-ViT-B/16\nDRL [87]\n49.9\n75.4\n83.3\n69.5\nInternVideo(ViT-L)*\n59.1\n\u2212\n\u2212\n\u2212\nUMT\u2020\n59.5\n84.9\n90.5\n78.3\nVidLA-B/16\n59.6\n84.4\n90.2\n78.0\nVidLA-B/16*\n67.5\n87.5\n91.7\n82.2\nMethod\nActivityNet Captions Retrieval\nR@1\nR@5\nR@10\nAvg\nCLIP-ViT-B/32\nCLIP4Clip [55]\n42.5\n74.1\n85.8\n67.5\nCenterCLIP [103]\n44.5\n75.7\n86.2\n68.8\nDRL [87]\n42.2\n74.0\n86.2\n67.5\nCAMoE* [13]\n49.9\n77.4\n\u2212\n\u2212\nDiffusionRet* [35]\n47.4\n76.3\n86.7\n70.1\nVidLA-B/32\n60.8\n85.0\n91.8\n79.2\nVidLA-B/32*\n69.1\n88.8\n93.7\n83.9\nCLIP-ViT-B/16\nDRL [87]\n45.7\n76.5\n87.8\n70.0\nCenterCLIP [103]\n46.7\n77.1\n88.0\n70.6\nUMT\u2020\n56.0\n83.5\n91.7\n77.1\nInternVideo(ViT-L)*\n62.8\n\u2212\n\u2212\n\u2212\nVidLA-B/16\n63.4\n87.5\n93.2\n81.4\nVidLA-B/16*\n72.9\n90.6\n94.9\n86.1\n"
    },
    {
        "level": "##",
        "title": "F. Effect Of Image-Language Pretraining",
        "content": "\nA key objective of our work is to introduce minimal architectural modifications to effectively utilize pretrained image-language models.\n\nTo analyze the effectiveness of image-language pretraining, we conduct experiments where we initialize the network weights randomly instead of using the CLIP weights. We evaluate the performance on three data scales and present the retrieval results on the MSR- VTT dataset in Figure 5. Figure 5 demonstrates that initializing the model weights with CLIP weights significantly outperforms the case where we initialize the model weights randomly at all data scales. This finding further validates the efficacy of VidLA in effectively utilizing the pretrained image-language weights and highlights the challenge of training from scratch for video-language alignment, which\n\nMethod\nMSVD Retrieval\nR@1\nR@5\nR@10\nAvg\nCLIP-ViT-B/32\nCLIP4Clip [55]\n62.0\n87.3\n92.6\n80.6\nDRL [87]\n62.3\n86.3\n92.2\n80.3\nCAMoE* [13]\n49.8\n79.2\n87.0\n72.0\nDiffusionRet [35]\n61.9\n88.3\n92.9\n81.0\nCenterCLIP [103]\n63.5\n86.4\n92.6\n80.8\nVidLA-B/32\n66.5\n92.4\n95.6\n84.8\nVidLA-B/32*\n77.4\n96.0\n98.7\n90.7\nCLIP-ViT-B/16\nCenterCLIP [103]\n68.4\n90.1\n95.0\n84.5\nDRL [87]\n68.7\n92.5\n95.6\n85.6\nInternVideo(ViT-L)*\n76.3\n\u2212\n\u2212\n\u2212\nUMT\u2020\n74.0\n94.6\n97.3\n88.6\nVidLA-B/16\n68.8\n93.4\n96.2\n86.1\nVidLA-B/16*\n80.3\n97.2\n98.4\n92.0\nMethod\nVATEX Retrieval\nR@1\nR@5\nR@10\nAvg\nCLIP-ViT-B/32\nDRL [87]\n77.0\n98.0\n99.4\n91.5\nVidLA-B/32\n79.8\n99.5\n99.8\n93.0\nVidLA-B/32*\n85.0\n99.8\n99.9\n94.9\nCLIP-ViT-B/16\nDRL [87]\n80.1\n98.5\n99.5\n92.7\nInternVideo(ViT-L)*\n87.2\n\u2212\n\u2212\n\u2212\nVidLA-B/16\n81.1\n99.5\n99.9\n93.5\nVidLA-B/16*\n85.6\n99.8\n100.0\n95.1\n\nrequires a large amount of training data.\n"
    },
    {
        "level": "##",
        "title": "G. Analysis Of [Mst] Token Configuration",
        "content": "\nTo investigate the impact of various design aspects of [MST]\ntokens, we systematically vary the number of hierarchies, U, and the temporal scale, r and evaluate the performance on the MSR-VTT dataset. We conduct these experiments on the 80 million data scale, consistent with most of the analysis done in the main text. We report the results in Table 15.\n\nThe first and the second rows of Table 15 demonstrate that incorporating [MST] tokens for a single hierarchy, U =\n1, leads to noticeable performance gains. The third and fourth rows further illustrate that increasing the number of hierarchies to 2 provides additional improvements, and setting the temporal scale to a higher value leads to better results. Finally, the last two rows indicate that adding one more hierarchy provides further improvement. However, we do not notice a significant difference between different temporal scales. Note that the [MST] token design in the last row is our default setting. The results in Table 15 provides further support for our hypothesis that incorporating hierarchy into the design of [MST] tokens plays a crucial role in improving video-language alignment performance.\n\nU-V -r\nMSR-VTT Retrieval\nR@1\nR@5\nR@10\nAvg\n\u2717\n49.1\n75.3\n83.5\n69.3\n1-4-1\n51.3\n76.5\n85.0\n70.9\n2-4-2\n52.1\n76.6\n84.2\n71.0\n2-4-3\n52.3\n76.9\n85.7\n71.6\n3-4-3\n53.2\n77.0\n85.6\n71.9\n3-4-2\n53.5\n77.5\n85.6\n72.2\n"
    },
    {
        "level": "##",
        "title": "H. Results On Hierarchical Dataset",
        "content": "\nWe demonstrated VidLA's efficacy on both short and long videos in Figure 4 (left). To further illustrate VidLA's effectiveness in a strictly hierarchical setting, we constructed a temporally hierarchical test set with 1K short, mid and long clip triplets centered around the same timestamp.\n\nZeroshot retrieval performance in Table 16 shows VidLA outperforms current state-of-the-art method, CLIPViP, even when trained on the same pretraining dataset (YT-VidLA-80M).\n\n| Method       |   Short |   Medium |   Long |\n|--------------|---------|----------|--------|\n| CLIPViP-B/32 |    85.9 |     86.8 |   88.1 |\n| VidLA-B/32   |    86.8 |     90.5 |   92.5 |\n"
    },
    {
        "level": "##",
        "title": "I. Analysis Of Pretraining Dataset",
        "content": "\nTo analyze the effect of different pretraining datasets, we pretrain VidLA-B/32 on diffrent pretraining datasets and later finetune on downstream datasets and report the results in Table 17. The first row reports the results where no pretaining on video data was performed and the model was only finetuned on the downstream dataset. From second and third row we notice that training on YT-VidLA-10M outperforms training on WebVid-10M. The last two rows demonstrate the advantage of our large-scale training where the performance improves significantly with scale.\n\n| Dataset       |   MSR-VTT |   DiDeMo |   ActivityNet |\n|---------------|-----------|----------|---------------|\n| WIT-400M      |      42.9 |     40.7 |          40.7 |\n| WebVid-10M    |      43.6 |     46.6 |          46.9 |\n| YT-VidLA-10M  |      49.3 |     46.3 |          49.7 |\n| YT-VidLA-80M  |      53.5 |     52.3 |          53.9 |\n| YT-VidLA-800M |      55.6 |     56.9 |          61.3 |\n"
    }
]