[
    {
        "level": "#",
        "title": "Information Compression In Dynamic Information Disclosure Games",
        "content": "\nDengwang Tang and Vijay G. Subramanian\n\n  Abstract\u2014 We consider a two-player dynamic information\ndesign problem between a principal and a receiver\u2014a game is\nplayed between the two agents on top of a Markovian system\ncontrolled by the receiver's actions, where the principal obtains\nand strategically shares some information about the underlying\nsystem with the receiver in order to influence their actions.\nIn our setting, both players have long-term objectives, and\nthe principal sequentially commits to their strategies instead\nof committing at the beginning. Further, the principal cannot\ndirectly observe the system state, but at every turn they can\nchoose randomized experiments to observe the system partially.\nThe principal can share details about the experiments to the\nreceiver. For our analysis we impose the truthful disclosure\nrule: the principal is required to truthfully announce the details\nand the result of each experiment to the receiver immediately\nafter the experiment result is revealed. Based on the received\ninformation, the receiver takes an action when its their turn,\nwith the action influencing the state of the underlying system.\nWe show that there exist Perfect Bayesian equilibria in this\ngame where both agents play Canonical Belief Based (CBB)\nstrategies using a compressed version of their information,\nrather than full information, to choose experiments (for the\nprincipal) or actions (for the receiver). We also provide a\nbackward inductive procedure to solve for an equilibrium in\nCBB strategies.\n"
    },
    {
        "level": "##",
        "title": "I. Introduction",
        "content": "\nIn many modern engineering and socioeconomic problems and systems, such as cyber-security, transportation networks, and e-commerce, information asymmetry is an inevitable aspect that crucially impacts decision making. In these systems, agents need to decide on their actions under limited information about the system and each other. In many situations, agents can overcome (some of) the information asymmetry by communicating with each other. However, agents can be unwilling to share information when agents' goals are not aligned with each other, since having some information that another agent does not know can be an advantage. In general, communication between agents with diverging incentives cannot be naturally established without rules/protocols that everyone agrees upon, and all agents suffer due to the breakdown of the information exchange. For example, drug companies are required by regulations to disclose their trial results truthfully. The public can then trust the results and This work is supported by NSF Grant No. ECCS 1750041, ECCS\n2038416, ECCS 1608361, CCF 2008130, ARO Award No. W911NF-17- 1-0232, MIDAS Sponsorship Funds by General Dynamics, and Okawa Foundation Research Grant.\n\nD. Tang is with Ming Hsieh Department of Electrical and Computer Engineering, University of Southern California, Los Angeles, CA 90089, USA dengwang@usc.edu V. G. Subramanian is with the Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI 48109, USA\nvgsubram@umich.edu benefit from the drug. In turn, the drug companies can make a profit. Without government regulations, drug companies and the public will both suffer due to mistrust. In many realworld dynamic systems, information exchange and decision making can happen repeatedly as the system/environment changes over time\u2014for example, public companies disclose information periodically which impacts stockholders' decisions; (COVID-19) vaccine producers conduct their trials and release results sequentially which impacts the government's purchasing decisions; during an epidemic, health authorities update their recommendations on the use of face masks over time according to changing levels of infection, etc. Therefore, in the face of information asymmetry, it is important to establish rules/protocols to facilitate repeated information exchange among agents in multi-agent dynamic systems.\n\nIn the economics literature, there are mainly two approaches to the above problem, namely mechanism design [1] and information design [2]. In mechanism design, less informed agents can extract information from more informed agents by committing to how they will use the collected information beforehand. Whereas in information design, more informed agents can partially disclose information to less informed agents. The more informed agents commit on the manner in which they partially disclose their information. In both approaches, all agents can benefit from the information exchange. For both approaches, one can classify the pertinent literature into two groups: (i) static settings, where both information disclosure and decision making take place only once; and (ii) dynamic settings, where agents repeatedly disclose information and take actions over time on top of an ever changing environment/physical system. Mechanism design and information design for the dynamic settings are more challenging than the static settings since agents need to anticipate future information disclosure when taking an action. Mechanism design in dynamic settings has been studied extensively in the literature [3], [4], [5], [6]. In most of the works on information design in dynamic settings, the receivers are assumed to be myopic [7], [8], [9], [10], [11], [12], [13]. This assumption greatly simplifies receivers' decision making. There have been a few papers studying information design problems where all agents in the system have long-term goals [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24]. These papers typically assume that the principal commits to their strategy for the whole game before the game starts as in a Stackelberg equilbrium [25]. The bulk of this literature also assumes that the principal observes the underlying state perfectly. However, these assumptions can be inappropriate for many applications. If the protocol gives more informed agents the power to commit to a strategy for the whole time horizon at the beginning of time, then the more informed agent can implement punishment strategies by threatening to withhold information if less informed agents do not obey their \"instructions\"\u2014see Example 1. Thus, the informed agents could abuse their commitment power to implement otherwise non-credible threats instead of using it for efficient information disclosure. This is not a desirable outcome\u2014for example, online map services should not threaten to withhold service if a driver refuses to take the recommended route; and similarly, public health authorities may want to use persuasion instead of threats to encourage mask wearing during an epidemic. Again, focusing on the public health setting, during an epidemic the authorities may not know the full extent of the disease spread, but only an estimate of it (using testing and other methods). In this context, transparency to the public on the part of the authorities\u2014in disclosing measurement methods and data\u2014 is important for persuasion based schemes to be effective.\n\nIn this work, we focus on the dynamic information design problem. Specifically, we consider a dynamic game between a principal and a receiver on top of a Markovian system. Both the principal and the receiver have long-term objectives. The principal cannot directly (and perfectly) observe 1 the system state, but can choose randomized experiments to partially observe the system. The principal is also allowed to choose any experiment, but they must announce the experimental setup and results truthfully to the receiver before the receiver takes their action. Both these aspects of our model are motivated by the public health setting described earlier. The receiver takes action on each turn based on the information received to date, which then influences the underlying system.\n\nContributions: In the class of dynamic information disclosure games among a principal and a receiver discussed above, under the assumption of truthful disclosure, we identify compression based strategies, called Canonical Belief Based (CBB) strategies, for both players to play at equilibrium. Here both agents use strategies based on a compressed version of their information, rather than the full information, to choose their actions\u2014experiments (for the principal), and actions (for the receiver). We develop a backward inductive sequential decomposition procedure to find such equilibrium strategies, and we show that the procedure always has at least one solution. Finally, we investigate examples of such games to provide insight to CBB-strategy-based equilibria.\n\nOrganization: The rest of this work is organized as follows: In Section I-A, we provide an example where a principal can abuse its commitment power. We formulate the problem in Section II. Then, we provide some preliminary results in discrete geometry in Section III. In Section IV we state our main results. In Section V we study some examples. We discuss difficulties with potential extensions of our result in Section VI. Finally, we conclude in Section VII. Details of all the technical proofs are in the Appendix.\n\nNotation: We use capital letters to represent random variables, bold capital letters to denote random vectors, and lower case letters to represent realizations. We use superscripts to indicate teams and agents, and subscripts to indicate time. We use t1 : t2 to indicate the collection of timestamps (t1, t1 + 1, \u00b7 \u00b7 \u00b7 *, t*2). For example X1\n1:3 stands for the random vector (X1\n1, X1\n2, X1\n3). For random variables or random vectors, we use the corresponding script capital letters (italic capital letters for greek letters) to denote the space of values these random vectors can take. For example, Hi t denotes the space of values the random vector Hi t can take. We use P(\u00b7) and E[\u00b7] to denote probabilities and expectations, respectively. We use \u2206(\u03a9) to denote the set of probability distributions on a finite set \u03a9.\n"
    },
    {
        "level": "##",
        "title": "A. A Motivating Example",
        "content": "\nThe following is an example where given the power to commit to a strategy for the whole game before the game starts (i.e. the Stackelberg game setting), the principal can use otherwise non-credible threats.\n\nExample 1. Consider a two-stage game of two players: the principal P, and the agent/receiver R. The state of the system at time t is Xt. The states are uncontrolled, and X1, X2\nare i.i.d. uniform random variables taking values in {0, 1}.\n\nThe principal can observe Xt at time t while the receiver cannot. At stage t, The principal transmits message Mt to the receiver and the receiver takes an action Ut \u2208 {*a, b, c, d*}.\n\nThe instantaneous payoff for both players are given by\n\nrA\n1 (0, a) = 1, rA\n1 (0, b) = 1.01, rA\n1 (0, c) = rA\n1 (0, d) = \u22121000\nrA\n1 (1, c) = 1, rA\n1 (1, d) = 1.01, rA\n1 (1, a) = rA\n1 (1, b) = \u22121000\nrB\n1 (0, a) = 500, rB\n1 (0, b) = 1, rB\n1 (0, c) = rB\n1 (0, d) = \u22121000\n\nrB\n 1 (1, c) = 500, rB\n               1 (1, d) = 1, rB\n                            1 (1, a) = rB\n                                      1 (1, b) = \u22121000\n\nand rA\n    2 (\u00b7, \u00b7) = rB\n             2 (\u00b7, \u00b7) = rA\n                      1 (\u00b7, \u00b7).\n  Suppose that the principal has the power to commit to a\nstrategy (g1, g2) at the beginning of the game. Then, (given\nthe Stackelberg setting) an optimal strategy for the principal\nis the following: fully reveal the state at t = 1 (i.e. M1 =\nX1); if the receiver plays a or c at t = 1, then transmit no\ninformation at t = 2; and if the receiver plays b or d at t = 1,\nthen fully reveal the state at t = 2. Then, the receiver's best\nresponse to the principal's strategy is the following: at t = 1:\nplay b if M1 = 0, and play d if M1 = 1; and at time t = 2:\nplay a if M2 = 0, and play c if M2 = 1.\n  In the resulting equilibrium, the principal effectively\nthreatens the receiver to comply to their interest at time t = 1\nby not giving information at time t = 2, even though the\ninterests of both parties are aligned at t = 2. In fact, without\nposing a threat to the receiver at time 2, the principal cannot\nconvince them to play b or d at time 1.\n"
    },
    {
        "level": "##",
        "title": "Ii. Problem Formulation",
        "content": "\nWe consider a finite-horizon two players dynamic game between the principal A and the agent/receiver B. The game consists of T stages, where, in each stage, the principal moves before the receiver. The game features an underlying dynamic system with state Xt. At each time t \u2208 [T], the receiver chooses an action Ut. Then, the system transits to the next state Xt+1 \u223c Pt(Xt, Ut), where Pt : Xt \u00d7 Ut \ufffd\u2192\n\u2206(Xt+1) is the transition kernel. The initial state X1 has prior distribution \u02c6\u03c0 \u2208 \u2206(X1). The initial distribution \u02c6\u03c0 and transition kernels P = (Pt)T\nt=1 are common knowledge to both players. We assume that neither player can observe the state Xt directly. However, at each time t, the principal can conduct an *experiment* to learn about Xt. In this work, an experiment2 refers to an observation kernel that can be chosen by the principal. We impose the rule that the experiments are required to be public\u2014both the principal and the receiver know the settings (the probabilities in the observation kernel), and the outcome (the observation itself) of the experiment. Specifically, at each time t, the principal chooses an observation kernel \u03c3t : Xt \ufffd\u2192 \u2206(Mt), and announces \u03c3t to the receiver. The experiment outcome Mt is then realized, and observed by both the principal and the receiver. Note that if the horizon T\n= 1, then the above setting is the same as the classical information design problem considered in [2].\n\nAssumption 1. Xt, Ut, Mt are finite sets with |Mt| sufficiently large.\n\n  The order of events happening at time t is given as the\nfollowing: (1) The principal commits to an experiment \u03c3t,\nand announces it to the receiver; (2) The measurement result\nMt is revealed to both the principal and receiver; (3) The\nreceiver takes action Ut; and (4) Xt transits to the next state.\n  Let St be the space of experiments. The principal uses a\n(pure) strategy to choose their experiment gA\n                                           t\n                                             : S1:t\u22121 \u00d7\nM1:t\u22121 \u00d7 U1:t\u22121 \ufffd\u2192 St. For convenience, define HA\n                                                   t\n                                                      =\nS1:t\u22121\u00d7M1:t\u22121\u00d7U1:t\u22121. The receiver uses a (pure) strategy\ngB\n t\n   : S1:t \u00d7 M1:t \u00d7 U1:t\u22121 \ufffd\u2192 Ut. For convenience, define\nHB\n t\n    = S1:t \u00d7 M1:t \u00d7 U1:t\u22121. The principal's goal is to\nmaximize JA(g) = Eg \ufffd\ufffdT\n                          t=1 rA\n                              t (Xt, Ut)\n                                        \ufffd\n                                         . The receiver's\n\ngoal is to maximize JB(g) = Eg \ufffd\ufffdT\n                                   t=1 rB\n                                        t (Xt, Ut)\n                                                 \ufffd\n                                                  . The\ninstantaneous reward functions (rA\n                                 t , rB\n                                    t )T\n                                       t=1 are common\nknowledge to both agents.\n  The belief of the principal at time t is a function \u00b5A\n                                                    t :\nM1:t\u22121 \u00d7 S1:t\u22121 \u00d7 U1:t\u22121 \ufffd\u2192 \u2206(X1:t). The belief of the\nreceiver at time t (after knowing \u03c3t and observing Mt) is a\nfunction \u00b5B\n         t : M1:t \u00d7 S1:t \u00d7 U1:t\u22121 \ufffd\u2192 \u2206(X1:t).\n  Inspired by the \"mechanism picking game\" defined in\n[29], we call the above game a signal picking game, and\nwe will study Perfect Bayesian Equilibria for our game.\n\nDefinition 1 (PBE). A Perfect Bayesian Equilibrium is a pair (*g, \u00b5*), where\n\n- g is sequentially rational given \u00b5 (=\n\ufffd\n\u00b5A\n1:T , \u00b5B\n1:T\n\ufffd\n).\n- \u00b5 can be updated using Bayes law whenever the denominator is non-zero.\n"
    },
    {
        "level": "##",
        "title": "Iii. Background: Discrete Geometry",
        "content": "\nIn this section, we introduce some notations and results of discrete geometry that are necessary for our main results.\n\nDefinition 2. Let f be a real-valued function on a polytope3\n\u03a9. Then, f is called a (continuous) piecewise linear function if there exist polytopes C1, \u00b7 \u00b7 \u00b7 *, C*k such that\n- f is linear on each Cj for j = 1, \u00b7 \u00b7 \u00b7 *, k*; and\n- C1 *\u222a \u00b7 \u00b7 \u00b7 \u222a* Ck = \u03a9.\n\nLemma 1. Let \u03a91, \u03a92 be polytopes. Let \u2113 : \u03a91 \ufffd\u2192 \u03a92 be\nan affine function and f : \u03a92 \ufffd\u2192 R be a piecewise linear\nfunction. Then the composite function f \u25e6 \u2113 : \u03a91 \ufffd\u2192 R is\npiecewise linear.\n\nProof. See Appendix A.\n\nNext, we introduce the notion of a triangulation.\n\nDefinition 3. [30] Let \u03a9 be a finite dimensional polytope. A\ntriangulation \u03b3 of \u03a9 is a finite collection of simplices (i.e.\nconvex hulls of a finite, affinely independent set of points)\nsuch that\n\n(1) If a simplex C \u2208 \u03b3, then all faces of C are in \u03b3;\n(2) For any two simplices C1, C2 \u2208 \u03b3, C1 \u2229 C2 is a\n(possibly empty) face of C1; and\n(3) The union of all simplices in \u03b3 equals \u03a9.\nFor a function f : \u03a9 \ufffd\u2192 R and a triangulation \u03b3, let I(*f, \u03b3*)\ndenote the linear interpolation of f based on the triangulation\n\u03b3, i.e.\n\n$\\mathbb{I}(f,\\gamma)(\\omega):=\\alpha_{1}f(\\omega_{1})+\\cdots+\\alpha_{k}f(\\omega_{k})$.\n\nif \u03c9\n        \u2208 C, where C\n                                 \u2208 \u03b3 is a simplex with vertices\n\u03c91, \u00b7 \u00b7 \u00b7 , \u03c9k, and \u03c9\n                              =\n                                   \u03b11\u03c91 + \u00b7 \u00b7 \u00b7 + \u03b1k\u03c9k for some\n\u03b11, \u00b7 \u00b7 \u00b7 , \u03b1k \u2265 0 such that \u03b11 + \u00b7 \u00b7 \u00b7 + \u03b1k = 1.\n\nLemma 2. For any real-valued function f on a polytope\n\u03a9, I(f, \u03b3) is a well-defined, continuous piecewise linear function.\n\n3A polytope is a convex hull of a finite set in Rd where *d <* +\u221e.\n\nProof. See Appendix A.\n\nFor each \u03c9 \u2208 \u03a9 and triangulation \u03b3, we have shown that there exists a unique way to represent \u03c9 as a convex combination of the vertices of one simplex from \u03b3. One can treat this convex combination as a finite measure. Denote this finite measure by C(*\u03c9, \u03b3*). Then we have I(*f, \u03b3*)(\u03c9) =\n\ufffd\nf(\u00b7)dC(*\u03c9, \u03b3*).\n\nDefinition 4. Let f be a real-valued function on \u03a9. The concave closure cav(f) of f is defined as a function \u03c1 such that\n\n$\\rho(\\omega):=\\sup\\{z:(\\omega,z)\\in\\mbox{cvxg}(f)\\}\\quad\\forall\\omega\\in\\Omega$\nwhere cvxg(f) \u2282 \u03a9 \u00d7 R is the convex hull of the graph of f.\n\nFor certain functions f, their concave closures can be represented as a triangulation based interpolation of the original function. Define the set of all such triangulations as arg cav(f), i.e.\n\narg cav(f) := {\u03b3 is a triangulation of \u03a9 : I(*f, \u03b3*) = cav(f)}.\n\nThe following lemma identifies a class of functions with the above property.\n\nLemma 3. Let f1, \u00b7 \u00b7 \u00b7 , fk, \u03c11, \u00b7 \u00b7 \u00b7 , \u03c1k be continuous piecewise linear functions on a polytope \u03a9. For \u03c9 \u2208 \u03a9, define\n\n$\\Upsilon(\\omega)=\\arg\\max_{j=1,\\cdots,k}f_{j}(\\omega),$ and $\\Psi(\\omega)=\\max_{j\\in\\Upsilon(\\omega)}\\rho_{j}(\\omega).$\nThen arg cav(\u03a8) is non-empty, i.e. there exists a triangulation \u03b3 of \u03a9 such that the concave closure of \u03a8 is equal to I(\u03a8, \u03b3).\n\nProof. See Appendix A.\n"
    },
    {
        "level": "##",
        "title": "Iv. Main Results",
        "content": "\nIn this section, we introduce our main result\u2014Theorem\n1\u2014, which provides a dynamic programming characterization of a subset of PBE of the signal picking game.\n\nNote that due to the assumption of public experiments, the signal picking game is a game with symmetric information 4\nafter each experiment is conducted. The principal's advantages lies in the fact that they have the power to determine the choice of experiments. Thus, standard results on strategyindependence of beliefs (e.g. [31]) imply that the beliefs of both players in this game are strategy-independent, i.e. there is a canonical belief system. Similar strategy-independent belief systems are also constructed and used in [32]. We describe this belief system as follows.\n\n**Definition 5**.: Define the Bayesian update function $\\xi_{t}:\\Delta(\\mathcal{X}_{t})\\times\\mathcal{S}_{t}\\times\\mathcal{M}_{t}\\mapsto\\Delta(\\mathcal{X}_{t})$ by setting for each $x_{t}\\in\\mathcal{X}_{t}$\n\n$$\\xi_{t}(x_{t}|\\pi_{t},\\sigma_{t},m_{t}):=\\frac{\\pi_{t}(x_{t})\\sigma_{t}(m_{t}|x_{t})}{\\sum_{\\tilde{x}_{t}}\\pi_{t}(\\tilde{x}_{t})\\sigma_{t}(m_{t}|\\tilde{x}_{t})}$$\n\nfor all $(\\pi_{t},\\sigma_{t},m_{t})$ such that the denominator is non-zero. When the denominator is zero, $\\xi_{t}(\\pi_{t},\\sigma_{t},m_{t})$ is defined to be the uniform distribution.\n\nDefinition 6. The canonical belief system is a collection\nof functions (\u03baA\n               t , \u03baB\n                   t )t\u2208T , \u03bai\n                           t : Hi\n                                 t \ufffd\u2192 \u2206(Xt), i \u2208 {A, B}\ndefined recursively through the following step. Denote \u03c0i\n                                                      t =\n\u03bai\n t(hi\n    t), i \u2208 {A, B}, t \u2208 T , and then we have\n\n- \u03c0A\n   1 := \u02c6\u03c0, the prior distribution of X1;\n\n- \u03c0B\n   t := \u03bet(\u03c0A\n             t , \u03c3t, mt);\n\n- \u03c0A\n   t+1 := \u2113t(\u03c0B\n               t , ut), where \u2113t : \u2206(Xt)\u00d7Ut \ufffd\u2192 \u2206(Xt+1)\n  is defined by\n\n\u02dcxt \u03c0t(\u02dcxt)Pt(xt+1|\u02dcxt, ut). \u2113t(\u03c0t, ut)(xt+1) := \ufffd\nWe consider a subclass of strategies for both the principal and the receiver, called canonical belief based (CBB) strategies, wherein player i \u2208 {*A, B*} chooses their experiment or action, respectively, at time t based solely on \u03a0i t = \u03bai t(Hi t)\ninstead of Hi t. Let \u03bbA\nt : \u2206(Xt) *\ufffd\u2192 S*t be the CBB strategy of the principal, and \u03bbB\nt : \u2206(Xt) *\ufffd\u2192 U*t be the CBB strategy of the receiver. Then, saying that player i is using CBB strategy\n\u03bbi t is equivalent to saying that they are using the strategy\n\ngi t(hi t) = \u03bbi t(\u03bai t(hi t)), \u2200hi t \u2208 Hi t.\nGiven an experiment and a distribution on the state, the posterior belief of the receiver is a random variable (a function of the random outcome). In an information disclosure game, it is helpful to consider the following sub-problem: how to design an experiment such that the receiver's belief, as a random variable, follows a certain distribution. The next definition formalizes this concept. This concept was used in classical one-shot information design setting [2] as well.\n\nDefinition 7. [2] An experiment \u03c3t \u2208 St is said to induce\na distribution \u03b7 \u2208 \u2206f(\u2206(Xt))\u2014that is, \u03b7 is a distribution\nwith finite support on the set of distributions \u2206(Xt)\u2014from\n\u03c0t \u2208 \u2206(Xt) [2] if for all \u02dc\u03c0t \u2208 \u2206(Xt),\n\n\u02dcxt\n   \u03c3t( \u02dcmt|\u02dcxt)\u03c0t(\u02dcxt).\n\n$$\\eta(\\tilde{\\pi}_{t})=\\sum_{\\tilde{m}_{t}}{\\bf1}_{\\{\\tilde{\\pi}_{t}=\\xi_{t}(\\pi_{t},\\sigma_{t},\\tilde{m}_{t})\\}}\\sum_{\\tilde{x}_{t}}$$\n4As mentioned earlier\u2014in Footnote 1\u2014, there are significant challenges A distribution \u03b7 is said to be *inducible from* \u03c0t if there exists some experiment \u03c3t that induces \u03b7 from \u03c0t.\n\n**Remark 1.** In [2], the authors showed that a distribution is $\\eta\\in\\Delta_{f}(\\Delta(\\mathcal{X}_{t}))$ is inducible from $\\pi_{t}$ if and only if $\\pi_{t}$ is the center of mass of $\\eta$, i.e. $\\pi_{t}=\\sum_{\\tilde{\\pi}_{t}\\in\\operatorname{supp}(\\eta)}\\eta(\\tilde{\\pi}_{t})\\cdot\\tilde{\\pi}_{t}$.\n\nWe now introduce our main result, which describes a backward induction procedure to find a PBE where both players use CBB strategies.\n"
    },
    {
        "level": "##",
        "title": "Theorem 1. Let",
        "content": "\n$$V_{T+1}^{A}(\\cdot)=V_{T+1}^{B}(\\cdot):=0$$\n\n_For each $t=T,T-1,\\cdots,1$ and $\\pi_{t}\\in\\Delta(\\mathcal{X}_{t})$, define_\n\n$$\\hat{q}_{t}^{i}(\\pi_{t},u_{t}):=\\sum_{\\tilde{x}_{t}}r_{t}^{i}(\\tilde{x}_{t},u_{t})\\pi_{t}(\\tilde{x}_{t})+V_{t+1}^{i}(\\ell_{t}(\\pi_{t},u_{t}))$$ $$\\forall i\\in\\{A,B\\};\\tag{1a}$$ $$\\Upsilon_{t}(\\pi_{t}):=\\max_{u_{t}}\\max_{\\hat{q}_{t}^{B}(\\pi_{t},u_{t})};$$ (1b) $$\\hat{v}_{t}^{A}(\\pi_{t}):=\\max_{u_{t}\\in\\Upsilon(\\pi_{t})}\\hat{q}_{t}^{A}(\\pi_{t},u_{t});$$ (1c) $$\\hat{v}_{t}^{B}(\\pi_{t}):=\\max_{u_{t}}\\hat{q}_{t}^{B}(\\pi_{t},u_{t});$$ (1d) $$\\gamma_{t}\\in\\arg\\operatorname{cov}(\\hat{v}_{t}^{A});$$ (1e) $$V_{t}^{i}(\\pi_{t}):=\\mathbb{I}(\\hat{v}_{t}^{i},\\gamma_{t})\\quad\\forall i\\in\\{A,B\\}.\\tag{1f}$$\n\nLet \u03bb\u2217B\n     t (\u03c0t) be any ut \u2208 Ut that attains the maximum\nin (1c). Let \u03bb\u2217A\n              t (\u03c0t) be any experiment that induces the\nfinite measure C(\u03c0t, \u03b3t) from \u03c0t. Then, the CBB strategies\n(\u03bb\u2217A, \u03bb\u2217B) form (the strategy part of) a PBE, and V A\n                                                    1 (\u02c6\u03c0)\nand V B\n     1 (\u02c6\u03c0) are the equilibrium payoffs for the principal and\nthe receiver respectively in this PBE.\n\nProof Outline. In Lemma 4 in Appendix B we construct a\nbelief system \u00b5\u2217 that is consistent with any strategy profile.\nHence, we only need to show sequential rationality of \u03bb\u2217.\n  To show the receiver's sequential rationality, we prove\nthe following: Fixing the principal's strategy to be \u03bb\u2217A, the\nreceiver is facing an MDP with state \u03a0B\n                                      t and action Ut. The\nproof then follows via standard stochastic control arguments.\n  To show the principal's sequential rationality, we prove\nthe following: Fixing the receiver's strategy to be \u03bb\u2217B, the\nprincipal is facing an MDP with state \u03a0A\n                                         t\n                                            and action \u03a3t.\nThis proof follows cia standard stochastic control arguments\ncoupled with information design results [2].\n  The details of the proof are presented in Appendix B.\n\n  The following proposition states that the sequential de-\ncomposition procedure described in Theorem 1 is well de-\nfined and always has a solution.\n\nProposition 1. There always exists a CBB strategy profile\n(\u03bb\u2217A, \u03bb\u2217B) that satisfies Eqs. (1) in Theorem 1.\n\nProof. Induction on time t is used for the proof.\n  Induction Invariant: V A\n                          t , V B\n                              t\n                                  are well-defined continu-\nous piecewise linear functions.\n  Induction Base: The induction variant is clearly true for\nt = T + 1 since V A\n                  T +1, V B\n                        T +1 are constant functions.\n\nInduction Step: Suppose that the induction invariant holds for t + 1.\n\n- Step 1: For each ut *\u2208 U*t, using the fact that \u2113t(\u03c0t, ut)\nis affine in \u03c0t, by Lemma 1, qA\nt , qB\nt\nare continuous\npiecewise linear functions in \u03c0t.\n- Step 2: By Lemma 3, \u03b3t is well-defined. - Step 3: By Lemma 2, V A\nt , V B\nt\nare continuous piecewise\nlinear functions.\nThis completes the proof.\n"
    },
    {
        "level": "##",
        "title": "A. Extension",
        "content": "\nIn many real-world settings, the receivers have the option to quit the game at any time. Our model and results can be extended to finite horizon games where the receiver can decide to terminate the game at any time before time T.\n\nProposition 2. Let Ut \u2282 Ut be the set of actions that\nterminates the game at time t. If we define V i\n                                           t , qi\n                                              t, \u03bb\u2217i\n                                                 t\n                                                    for\neach i \u2208 {A, B}, t \u2208 T as in (1) except that (1a) is changed\nto\n\n$\\hat{q}_{t}^{i}(\\pi_{t},u_{t})$\n\n$:=\\sum_{\\tilde{x}_{t}}r_{t}^{i}(\\tilde{x}_{t},u_{t})\\pi_{t}(\\tilde{x}_{t})+\\begin{cases}V_{t+1}^{i}(\\ell_{t}(\\pi_{t},u_{t}))&\\text{if}u_{t}\\not\\in\\overline{\\mathcal{U}}_{t}\\\\ 0&\\text{if}u_{t}\\in\\overline{\\mathcal{U}}_{t}\\end{cases}$\n\nfor i \u2208 {A, B}. Then the CBB strategies (\u03bb\u2217A, \u03bb\u2217B) form\n(the strategy part of) a PBE, and V A\n                                       1 (\u02c6\u03c0) and V B\n                                                    1 (\u02c6\u03c0) are\nthe equilibrium payoff for the principal and the receiver\nrespectively in this PBE.\n\nProof. Similar to Theorem 1.\n"
    },
    {
        "level": "##",
        "title": "V. Examples",
        "content": "\nWe implement the sequential decomposition algorithm of Proposition 2 in MATLAB for binary state spaces (i.e. |Xt| =\n2). We run the algorithm on the following examples of the signal picking game.\n\nExample 2. Consider the quickest detection game defined in [24]. In this game, the underlying state Xt is binary and uncontrolled, with Xt = {1, 2}. State 2 is an absorbing state, i.e. P(Xt+1 = 2 | Xt = 2) = 1, whereas the system can jump from state 1 to state 2 at any time with probability p, i.e. P(Xt+1 = 2 | Xt = 1) = p where p \u2208 (0, 1).\n\nThe receiver would like to detect (the epoch of) the jump from state 1 to state 2 as accurately as possible. At each time the receiver has two options: Ut = j stands for declaring state j for j = 1, 2. The instantaneous reward of the receiver is given by\n\n$$r_{t}^{B}(X_{t},U_{t})=\\begin{cases}-1&\\text{if}X_{t}=1,U_{t}=2\\\\ -c&\\text{if}X_{t}=2,U_{t}=1\\\\ 0&\\text{otherwise}\\end{cases}$$\n\nwhere $c\\in(0,1)$. Once the receiver declares state 2, the game ends immediately.\n\nThe principal would like the receiver to stay in the system as long as possible. The instantaneous reward for the principal is\n\n$$r_{t}^{A}(X_{t},U_{t})=\\begin{cases}1&\\text{if}U_{t}=1\\\\ 0&\\text{otherwise}\\end{cases}$$\nSetting p = 0.2, c = 0.1, we obtained the qB\nt and V A\nt functions specified in Proposition 2 in Figure 4. The horizontal axis represents \u03c0t(1). In the figures for V A\nt functions, the vertices of the triangulation \u03b3t are labeled. The vertices represent the set of beliefs that the principal could induce, and they completely describe the principal's CBB strategy. If the vertex is labeled with red circles, the receiver will take action Ut = 1 at this posterior belief. If, instead, the vertex is labeled with blue triangles, the receiver will take action Ut = 2 at this posterior belief.\n\nFrom the figures, one can see that at any stage, there is only one possible belief that the principal would induce which leads to the receiver quitting the game (i.e. select Ut = 2). This is consistent with the principal's objective of keeping the receiver in the system. Just like in static information design problems [2], [33], when it is better off for the receiver to declare change, i.e., quit, under the current belief, the principal would promise to tell the receiver that the state is 2 with some probability \u02dcp when the state is indeed 2, and tell the receiver nothing otherwise. In doing so, the receiver would believe that the state is 1 with a higher probability when the principal does not tell the receiver anything. The principal chooses \u02dcp to be precisely the value for which the receiver is willing to stay in the system [2].\n\nWhen t is close to T, the end of the game, the principal would only prefer to declare state 2 if they believe that \u03c0t(1)\nis very small. This is due to the fact that \"false alarms\" are costlier than delayed detection in this game. When t is further away from T, the threshold of \u03c0t(1) for the principal to declare state 2 becomes larger. This holds because when the game is close to end, the receiver has the \"safe\" option to declare state 1 (at a small cost) until the end to avoid false alarms (which are costly). However, this option is less preferable when the gap between t and T is large.\n\nWhen t is further away from T, the principal's value function seems to converge. This is due to the fact that the receiver has the option to quit the game and staying in the game is costly in general.\n\nExample 3. Consider a game between a principal and a detector. In this game, the underlying state Xt is binary and uncontrolled with Xt = {\u22121, 1}. At any time, the system can jump to the other state with probability p \u2208 (0, 1), i.e.\n\n$$\\mathbb{P}(X_{t+1}=-j\\mid X_{t}=j)=p,\\qquad\\forall j\\in\\{-1,1\\}.$$\nThe receiver has three actions: Ut = j stands for declaring state j for j = \u22121, 1. Both Ut = \u22121 and Ut = +1 terminate the game. In addition, the receiver can choose to wait at a cost with action Ut = 0. The instantaneous reward of the\n\nreceiver for $c\\in(0,1)$ is given by\n\n$$r_{t}^{B}(X_{t},U_{t})=\\begin{cases}1&\\text{if}X_{t}=U_{t}\\\\ -c&\\text{if}U_{t}=0\\\\ 0&\\text{otherwise}\\end{cases}.$$\n\nThe principal would like the receiver to stay in the system as long as possible. The instantaneous reward for the principal is\n\n$$r_{t}^{A}(X_{t},U_{t})=\\begin{cases}1&\\text{if}U_{t}=0\\\\ 0&\\text{otherwise}\\end{cases}$$\nSetting p = 0.2 and c = 0.15, we obtained the qB\nt and V A\nt functions specified in Proposition 2\u2014see Figure 5. The horizontal axis represents \u03c0t(\u22121). The figures follows the same interpretation as the figures in Example 2. (The markers for actions are different from previous figures, but they are self-explanatory.)\nDifferent from Example 2, the value functions and CBB\nstrategies at equilibrium oscillate with a period of 4 (given p = 0.2, c = 0.15) instead of converging as t gets further away from the horizon T.\n"
    },
    {
        "level": "##",
        "title": "Vi. Discussion",
        "content": "\nNaturally, one may consider extending the above result to two settings: (a) when a public noisy observation of the state is available in addition to the principal's experiment, (b) when there are multiple receivers. However, our result is immediately extendable to neither setting. This is since the techniques we use in this paper depend heavily on the piecewise linear structure of \u02c6q and V -functions in (1), as well as the preservation of this piecewise linear structure under backward induction. Specifically, when the functions\n\u02c6qA\nt , \u02c6qB\nt are piecewise linear, the concave closure of \u02c6vA\nt can be expressed as a triangulation based interpolation (through Lemma 3), which in turn allows us to apply the same triangulation to \u02c6vB\nt , and thus ensuring the continuity and piecewise linearity of V B\nt . However, this structure does not appear in general in the extensions.\n\nWe describe an attempt to extend Theorem 1 to settings\n(a) and (b) in the most straightforward way. In the case of setting (a), one needs to change the belief update in (1a) from\n\u2113t(\u03c0t, ut) to some other update function that incorporates the public observation. However, unlike \u2113t(\u03c0t, ut), the new update function may not be linear in \u03c0t. Therefore this procedure cannot preserve piecewise linear properties.\n\nIn the case of setting (b), ut will represent a vector of actions of all receivers, and one needs to change the definition of \u03a5t(\u03c0t) in (1b) to be the set of mixed strategy Nash equilibrium (or alternatively correlated equilibrium) action profiles of the following stage game: Receiver i chooses an action in Ui t, and receives payoff \u02c6qi t(\u03c0t, ut). In this setting, \u03a5t(\u03c0t) is a set of probability measures on the product set Ut. The new \u02c6vA\nt function can then be given by\n\n\u02dcut qA t (\u03c0t, \u02dcut)\u03b7t(\u02dcut) \u02c6vA t (\u03c0t) = arg max \u03b7t\u2208\u03a5t(\u03c0t) \ufffd However, in this case, continuity and piecewise linearity of $\\hat{q}_{t}$ are not enough to ensure that the value function $V_{t}^{A}$ possesses the same property. To see this, consider the following example with two receivers $B$ and $C$. Let $\\mathcal{U}_{t}^{B}=\\mathcal{U}_{t}^{C}=\\{1,2\\}=\\mathcal{X}_{t}=\\{1,2\\}$. Let $p=\\pi_{t}(1)$. Then, all functions of $\\pi_{t}$ can be expressed as a function of $p$. Suppose that\n\n$$q_{t}^{B}(\\pi_{t},u_{t})=\\begin{cases}1&\\text{if}u_{t}^{B}=u_{t}^{C}\\\\ 0&\\text{otherwise}\\end{cases},$$ $$q_{t}^{C}(\\pi_{t},u_{t})=\\begin{cases}p+1&\\text{if}u_{t}^{B}=1,u_{t}^{C}=2\\\\ 1&\\text{if}u_{t}^{B}=2,u_{t}^{C}=1\\\\ 0&\\text{otherwise}\\end{cases}.$$\n\nIt can be verified that, under either the concept of Nash equilibrium or correlated equilibrium, $\\Upsilon_{t}(\\pi_{t})$ contains only one element: player $B$ plays action 1 with probability $\\frac{1}{2+p}$ and player $C$ plays their two actions with equal probability independent of player $B$'s action. Now suppose that\n\n$$q_{t}^{A}(\\pi_{t},u_{t})=\\begin{cases}p&u_{t}^{B}=1\\\\ 0&\\text{otherwise}\\end{cases}.$$\nThen we have \u02c6vA\nt (\u03c0t) =\np\n\n                      2+p for p \u2208 [0, 1]. Observe that\n\u02c6vA\nt is a strictly concave function. Hence the concave closure\nof \u02c6vA\n   t is just \u02c6vA\n           t itself, which is not piecewise linear.\n"
    },
    {
        "level": "##",
        "title": "Vii. Conclusion And Future Work",
        "content": "\nIn this work, we formulated a dynamic information disclosure game, called the signal picking game, where the principal sequentially commit to a signal/experiment to communicate with the receiver. We showed that there exist equilibria where both the principal and the receiver make decisions based on the canonical belief instead of their respective full information. We also provided a sequential decomposition procedure to find such equilibria.\n\nUnlike the CIB-belief-based sequential decomposition procedures of [34], [35], [36], [37], the sequential decomposition procedure of Theorem 1 always has a solution. The main reason is that the CIB belief in the signal picking game is strategy-independent, just like in [32]. The result illustrates the critical difference between strategy-dependent and strategy-independent compression of information in dynamic games.\n\nThere are a few future research problems arising from this work. The first problem is to extend our result to infinite horizon games. The second problem is to extend our result to settings with multiple senders.\n"
    },
    {
        "level": "##",
        "title": "References",
        "content": "\n[1] R. B. Myerson, \"Mechanism design,\" in Allocation, Information and\nMarkets.\nSpringer, 1989, pp. 191\u2013206.\n[2] E. Kamenica and M. Gentzkow, \"Bayesian persuasion,\" American\nEconomic Review, vol. 101, no. 6, pp. 2590\u20132615, 2011.\n[3] D. Bergemann and J. V\u00a8alim\u00a8aki, \"The dynamic pivot mechanism,\"\nEconometrica, vol. 78, no. 2, pp. 771\u2013789, 2010.\n[4] S. Athey and I. Segal, \"An efficient dynamic mechanism,\" Econometrica, vol. 81, no. 6, pp. 2463\u20132485, 2013.\n[5] A. Pavan, I. Segal, and J. Toikka, \"Dynamic mechanism design: A\nMyersonian approach,\" *Econometrica*, vol. 82, no. 2, pp. 601\u2013653, 2014.\n[6] D. Bergemann and J. V\u00a8alim\u00a8aki, \"Dynamic mechanism design: An\nintroduction,\" *Journal of Economic Literature*, vol. 57, no. 2, pp. 235\u2013 74, 2019.\n[7] D. Lingenbrink and K. Iyer, \"Optimal signaling mechanisms in unobservable queues with strategic customers,\" in Proceedings of the 2017\nACM Conference on Economics and Computation, 2017, pp. 347\u2013347.\n[8] J. C. Ely, \"Beeps,\" *American Economic Review*, vol. 107, no. 1, pp.\n31\u201353, 2017.\n[9] F. Farokhi, A. M. Teixeira, and C. Langbort, \"Estimation with strategic\nsensors,\" *IEEE Transactions on Automatic Control*, vol. 62, no. 2, pp. 724\u2013739, 2016.\n[10] M. O. Sayin, E. Akyol, and T. Bas\u00b8ar, \"Strategic control of a tracking\nsystem,\" in 2016 IEEE 55th Conference on Decision and Control (CDC).\nIEEE, 2016, pp. 6147\u20136153.\n[11] J. Renault, E. Solan, and N. Vieille, \"Optimal dynamic information\nprovision,\" *Games and Economic Behavior*, vol. 104, pp. 329\u2013349, 2017.\n[12] J. Best and D. Quigley, \"Honestly dishonest: A solution to the\ncommitment problem in Bayesian persuasion,\" Mimeo, Tech. Rep., 2016.\n[13] \u2014\u2014, \"Persuasion for the long-run,\" Economics Group, Nuffield\nCollege, University of Oxford, Tech. Rep., 2016.\n[14] P. B. Luh, S.-C. Chang, and T.-S. Chang, \"Solutions and properties\nof multi-stage Stackelberg games,\" *Automatica*, vol. 20, no. 2, pp. 251\u2013256, 1984. [Online]. Available: https://www.sciencedirect.com/ science/article/pii/0005109884900347\n[15] B. Tolwinski, \"A Stackelberg equilibrium for continuous-time differential games,\" in *The 22nd IEEE Conference on Decision and Control*,\n1983, pp. 675\u2013681.\n[16] \u2014\u2014, \"Closed-loop Stackelberg solution to a multistage linearquadratic game,\" *Journal of Optimization Theory and Applications*, vol. 34, no. 4, pp. 485\u2013501, 1981.\n[17] F. Farhadi, D. Teneketzis, and S. J. Golestani, \"Static and dynamic\ninformational incentive mechanisms for security enhancement,\" in 2018 European Control Conference (ECC).\nIEEE, 2018, pp. 1048\u2013\n1055.\n[18] L. Li, O. Massicot, and C. Langbort, \"Sequential public signaling in\nrouting games with feedback information,\" in 2018 IEEE Conference\non Decision and Control (CDC), 2018, pp. 2735\u20132740.\n[19] M. O. Sayin and T. Bas\u00b8ar, \"Dynamic information disclosure for\ndeception,\" in *2018 IEEE Conference on Decision and Control (CDC)*.\nIEEE, 2018, pp. 1110\u20131117.\n[20] M. O. Sayin, E. Akyol, and T. Bas\u00b8ar, \"Hierarchical multistage Gaussian signaling games in noncooperative communication and control systems,\" *Automatica*, vol. 107, pp. 9\u201320, 2019.\n[21] M. O. Sayin and T. Bas\u00b8ar, \"On the optimality of linear signaling to\ndeceive Kalman filters over finite/infinite horizons,\" in International Conference on Decision and Game Theory for Security.\nSpringer,\n2019, pp. 459\u2013478.\n[22] E. Meigs, F. Parise, A. Ozdaglar, and D. Acemoglu, \"Optimal\ndynamic information provision in traffic routing,\" arXiv preprint arXiv:2001.03232, 2020.\n[23] H. Tavafoghi and D. Teneketzis, \"Informational incentives for congestion games,\" in 2017 55th Annual Allerton Conference on Communication, Control, and Computing (Allerton).\nIEEE, 2017, pp.\n1285\u20131292.\n[24] F. Farhadi and D. Teneketzis, \"Dynamic information design: A simple problem on optimal sequential information disclosure,\" Dynamic\nGames and Applications, vol. 12, no. 2, pp. 443\u2013484, 2022.\n[25] S. Heinrich Von, \"Market structure and equilibrium,\" 2011. [26] R. B. Myerson and P. J. Reny, \"Perfect conditional \u03b5-equilibria of\nmulti-stage games with infinite sets of signals and actions,\" Econometrica, vol. 88, no. 2, pp. 495\u2013531, 2020.\n[27] M. Gentzkow and E. Kamenica, \"Bayesian persuasion with multiple\nsenders and rich signal spaces,\" *Games and Economic Behavior*, vol. 104, pp. 411\u2013429, 2017.\n[28] \u2014\u2014, \"Disclosure of endogenous information,\" Economic Theory\nBulletin, vol. 5, no. 1, pp. 47\u201356, 2017.\n[29] L. Doval and V. Skreta, \"Mechanism design with limited commitment,\" *arXiv preprint arXiv:1811.03579*, 2018.\n[30] J. De Loera, J. Rambau, and F. Santos, Triangulations: Structures for\nalgorithms and applications.\nSpringer Science & Business Media,\n2010, vol. 25.\n[31] P. R. Kumar and P. Varaiya, Stochastic systems: Estimation, identification and adaptive control.\nPrentice-Hall, Inc., 1986.\n[32] A. Nayyar, A. Gupta, C. Langbort, and T. Bas\u00b8ar, \"Common\ninformation based Markov perfect equilibria for stochastic games with asymmetric information: Finite games,\" IEEE Transactions on\nAutomatic Control, vol. 59, no. 3, pp. 555\u2013570, 2013. [Online].\nAvailable: https://doi.org/10.1109/tac.2013.2283743\n[33] D. Bergemann and S. Morris, \"Information design: A unified perspective,\" *Journal of Economic Literature*, vol. 57, no. 1, pp. 44\u201395, 2019.\n[34] Y. Ouyang, H. Tavafoghi, and D. Teneketzis, \"Dynamic games\nwith asymmetric information: Common information based perfect Bayesian equilibria and sequential decomposition,\" IEEE Transactions on Automatic Control, vol. 62, no. 1, pp. 222\u2013237, 2016. [Online].\nAvailable: https://doi.org/10.1109/tac.2016.2544936\n[35] H. Tavafoghi, Y. Ouyang, and D. Teneketzis, \"On stochastic dynamic\ngames with delayed sharing information structure,\" in 2016 IEEE 55th\nConference on Decision and Control (CDC).\nIEEE, 2016, pp. 7002\u2013\n7009. [Online]. Available: https://doi.org/10.1109/cdc.2016.7799348\n[36] D. Vasal, A. Sinha, and A. Anastasopoulos, \"A systematic process for\nevaluating structured perfect Bayesian equilibria in dynamic games with asymmetric information,\" IEEE Transactions on Automatic\nControl, vol. 64, no. 1, pp. 81\u201396, 2019. [Online]. Available: https://doi.org/10.1109/tac.2018.2809863\n[37] D. Tang, H. Tavafoghi, V. Subramanian, A. Nayyar, and D. Teneketzis,\n\"Dynamic games among teams with delayed intra-team information sharing,\" *Dynamic Games and Applications*, 2022. [Online]. Available:\nhttps://doi.org/10.1007/s13235-022-00424-4\n"
    },
    {
        "level": "##",
        "title": "Appendix A. Proofs Of Discrete Geometric Results",
        "content": "\nProof of Lemma 1. Let C1, \u00b7 \u00b7 \u00b7 *, C*k be polytopes such that (i) f is linear on each of C1, \u00b7 \u00b7 \u00b7 *, C*k (ii) C1 *\u222a \u00b7 \u00b7 \u00b7 \u222a* Ck = \u03a92.\n\nSince \u2113 is an affine function, we have the pre-images Dj = \u2113\u22121(Cj), j = 1, \u00b7 \u00b7 \u00b7 *, k* to be polytopes as well.\n\nf \u25e6 \u2113 is linear on each Dj (since it is the composition of two linear functions), and D1 *\u222a \u00b7 \u00b7 \u00b7 \u222a* Dk = \u03a91. We conclude that f \u25e6 \u2113 is a piecewise linear function.\n\nProof of Lemma 2. First, for any \u03c9, given a simplex C such that \u03c9 \u2208 C, there is a unique way to represent \u03c9 as a convex combination of vertices of C.\n\nSuppose that \u03c9 is in both simplices C and C\u2032. Then \u03c9 is in C \u2229 C\u2032, which is a face of both C and C\u2032. Since C \u2229 C\u2032 is a simplex, \u03c9 can be uniquely represented as a convex combination of vertices of C \u2229C\u2032. Since the set of vertices of C \u2229C\u2032 is a subset of the vertices of both C and C\u2032, we conclude that the above representation is also the unique way of representing\n\u03c9 as a convex combination of vertices of C (and of C\u2032). We conclude that for any \u03c9, there is a unique way to represent \u03c9\nas a convex combination of vertices of any simplex in \u03b3. Hence I(*f, \u03b3*) is well defined.\n\nFor any simplex C \u2208 \u03b3, I(*f, \u03b3*) is linear on C. Since the number of simplices in \u03b3 is finite and their union is \u03a9, we conclude that I(*f, \u03b3*) is a continuous piecewise linear function on \u03a9.\n\nProof of Lemma 3. For j = 1, \u00b7 \u00b7 \u00b7 *, k*, let Cj1, \u00b7 \u00b7 \u00b7 *, C*jmj be polytopes corresponding to the piecewise linear function fj under Definition 2, i.e. fj is linear on each of Cj1, \u00b7 \u00b7 \u00b7 *, C*jmj and Cj1 *\u222a \u00b7 \u00b7 \u00b7 \u222a* Cjmj = \u03a9. Define\n\n$$\\mathcal{S}=\\{C_{1i_{1}}\\cap C_{2i_{2}}\\cap\\cdots\\cap C_{ki_{k}}:1\\leq i_{1}\\leq m_{1},\\cdots,1\\leq i_{k}\\leq m_{k}\\}$$\n\n$\\mathcal{S}$ is a finite collection of polytopes. All of $f_{1},\\cdots,f_{k}$ are linear on each element of $\\mathcal{S}$. The union of $\\mathcal{S}$ equals $\\Omega$. Define\n\n$$A_{j}:=\\{\\omega\\in\\Omega:f_{j}(\\omega)\\geq f_{j^{\\prime}}(\\omega)\\quad\\forall j^{\\prime}=1,\\cdots,k\\},$$ $$\\mathcal{S}_{j}=\\{F\\cap A_{j}:F\\in\\mathcal{S}\\}.$$\nSj is the collection of subsets where fj is (one of) the maximum among f1, \u00b7 \u00b7 \u00b7 *, f*k. Sj is also a finite collection of polytopes, since each F \u2229 Aj is a subset of F that satisfy certain linear constraints.\n\nSimilarly, let Dj1, \u00b7 \u00b7 \u00b7 *, D*jnj be polytopes corresponding to the piecewise linear function \u03c1j. Define\n\n$${\\mathcal{R}}_{j}=\\{F\\cap D_{j i}:F\\in{\\mathcal{S}}_{j},1\\leq i\\leq n_{j}\\}$$\nFor each polytope F *\u2208 R*j, \u03c1j is linear on F, and fj is (one of) the maximum among f1, \u00b7 \u00b7 \u00b7 *, f*k for all points in F.\n\nThe union of Rj equals Aj.\n\nLet P be the set of vertices of polytopes in R1*\u222a\u00b7 \u00b7 \u00b7\u222aR*k. P is a finite set. Define *B \u2282* \u03a9\u00d7R by B = {(\u03c9, \u03a8(\u03c9)) : \u03c9 *\u2208 P}*.\n\nLet Z be the convex hull of B. We have Z to be a polytope with its vertices contained in B.\n\nLet \u02c6\u03a8 be the concave closure of \u03a8. We will show that the function \u02c6\u03a8 is represented by the upper face of Z. Then we obtain a triangulation of \u03a9 by projecting a triangulation of Z in a similar way to the construction of regular triangulations\n(see Section 2.2 of [30]).\n\nStep 1: Prove that \u02c6\u03a8(\u03c9) = max{y : (\u03c9, y) *\u2208 Z}*.\n\nDefine \u03a8(\u03c9) := max{y : (\u03c9, y) *\u2208 Z}*. \u03a8 is a concave function.\n\nIt is clear that *Z \u2282* cvxg(\u03a8), since B is a subset of the graph of \u03a8. Therefore \u03a8(\u03c9) \u2264 \u02c6\u03a8(\u03c9).\n\nConsider any \u03c9 \u2208 \u03a9. Let j\u2217 be such that j\u2217 \u2208 \u03a5(\u03c9) and \u03a8(\u03c9) = \u03c1j\u2217(\u03c9). Then \u03c9 \u2208 Aj\u2217. We have \u03c9 \u2208 F for some F *\u2208 R*j\u2217. Let \u03c91, \u00b7 \u00b7 \u00b7 , \u03c9m *\u2208 P* be the vertices of F. We can write \u03c9 = \u03b11\u03c91 + *\u00b7 \u00b7 \u00b7* + \u03b1k\u03c9m for some \u03b11, \u00b7 \u00b7 \u00b7 , \u03b1m \u2265\n0, \u03b11 + *\u00b7 \u00b7 \u00b7* + \u03b1m = 1. Since \u03c1j\u2217 is linear on F we have\n\n$$\\Psi(\\omega)=\\rho_{J^{*}}(\\omega)=\\alpha_{1}\\rho_{J^{*}}(\\omega_{1})+\\cdots+\\alpha_{k}\\rho_{J^{*}}(\\omega_{m})\\tag{2}$$\n\nSince $\\omega_{1},\\cdots,\\omega_{m}\\in F$ and $F\\subset A_{J^{*}}$. By definition, $j^{*}\\in\\mathrm{T}(\\omega_{i})$ for all $i=1,\\cdots,m$. Therefore $\\rho_{J^{*}}(\\omega_{i})\\leq\\Psi(\\omega_{i})$ for all $i=1,\\cdots,m$. Consequently, combining (2) we have\n\n$$\\Psi(\\omega)\\leq\\alpha_{1}\\Psi(\\omega_{1})+\\cdots+\\alpha_{k}\\Psi(\\omega_{m})$$\nGiven that (*\u03c9, \u03b1*1\u03a8(\u03c91) + \u00b7 \u00b7 \u00b7 + \u03b1k\u03a8(\u03c9k)) *\u2208 Z*, we have\n\n$$\\alpha_{1}\\Psi(\\omega_{1})+\\cdots+\\alpha_{k}\\Psi(\\omega_{k})\\leq\\overline{\\Psi}(\\omega)$$\n\nHence $\\Psi(\\omega)\\leq\\overline{\\Psi}(\\omega)$. Therefore, $\\overline{\\Psi}$ is a concave function above $h$. Since the concave closure $\\hat{\\Psi}(\\omega)$ is the smallest concave function above $h$, we conclude that $\\Psi(\\omega)\\leq\\overline{\\Psi}(\\omega)$ for all $\\omega\\in\\Omega$.\n\nTherefore $\\hat{\\Psi}=\\overline{\\Psi}$, completing the proof of Step 1.\n\n**Step 2:** Construct the triangulation $\\gamma$ and show that $\\hat{\\Psi}=\\mathbb{I}(h,\\gamma)$.\n\nLet *A \u2282* \u03a9 \u00d7 R be the graph of \u02c6\u03a8. A is also the union of upper faces of Z. Let \u03d1 be a point set triangulation (as defined in Def. 2.2.1 in [30]) of the finite point set B. (A point set triangulation of a finite set of points always exists. See Section\n2.2.1 of [30].) Let \u02c6\u03d1 be the restriction of \u03d1 to A, i.e. \u02c6\u03d1 := {F : F \u2282 A, F \u2208 \u03d1}. It can be shown that \u02c6\u03d1 is a simplicial complex (i.e. a polyhedral complex where all polytopes are simplices. See Def. 2.1.5 in [30].) with vertices contained in A \u2229 B.\n\nSince A is the upper convex hull of Z, the projection map proj\u03a9 : \u03a9 \u00d7 R \ufffd\u2192 \u03a9, (*\u03c9, y*) \ufffd\u2192 \u03c9 is a bijection between A\nand \u03a9. Let \u03b3 be the projection of \u02c6\u03d1 on to \u03a9, i.e. \u03b3 = {proj\u03a9(F) : F \u2208 \u02c6\u03d1}. We conclude that \u03b3 is a simplical complex that spans \u03a9, i.e. a triangulation of \u03a9.\n\nThe inverse map proj\u22121\n\u03a9\n: \u03a9 *\ufffd\u2192 A* is a piecewise linear map that is linear on each simplex in \u03b3. Therefore we have\n\u02c6\u03a8(\u03c9) = I(\u02c6\u03a8, \u03b3)(\u03c9) for all \u03c9 \u2208 \u03a9. Since the vertices of \u02c6\u03d1 are contained in both A and B, we have \u02c6\u03a8(\u03c9) = \u03a8(\u03c9) for each vertex \u03c9 of \u03b3. (Recall that B is a subset of the graph of h and A is the graph of \u02c6\u03a8.) Therefore we have \u02c6\u03a8(\u03c9) = I(\u03a8, \u03b3)(\u03c9)\nfor all \u03c9 \u2208 \u03a9.\n"
    },
    {
        "level": "##",
        "title": "B. Proof Of Main Results",
        "content": "\nLemma 4. *There exist a belief system* \u00b5\u2217 = (\u00b5\u2217A, \u00b5\u2217B) such that (i) \u00b5\u2217 is consistent with any strategy profile (gA, gB);\n(ii) the canonical belief system \u03ba is the marginals of \u00b5\u2217.\n\nProof of Lemma 4. Define \u00b5\u2217i t : Hi t \ufffd\u2192 \u2206(X1:t) recursively through the following:\n\n- \u00b5\u2217A\n1 (hA\n1 ) := \u02c6\u03c0.\n\u02dcx1:t \u00b5\u2217A\nt (\u02dcx1:t|hA\nt )\u03c3t(mt|\u02dcxt)\n- \u00b5\u2217B\nt (x1:t|hB\nt ) :=\n\u00b5\u2217A\nt (x1:t|hA\nt )\u03c3t(mt|xt)\n\ufffd\n- \u00b5\u2217A\nt+1(x1:t+1|hA\nt+1) := \u00b5\u2217B\nt (x1:t|hB\nt )P(xt+1|xt, ut)\nThrough induction on t it is clear that \u03ba is the marginal distribution derived from \u00b5\u2217.\nIt remains to show the consistency of \u00b5\u2217 w.r.t. any strategy profile g = (gA, gB). We will also show it via induction:\n- \u00b5\u2217A\n1\nis clearly consistent with any g since it is defined to be the prior distribution of X1.\n- Suppose that \u00b5\u2217A\nt\nis consistent with g. Then consider any hB\nt = (\u03c31:t, m1:t, u1:t\u22121) \u2208 HB\nt such that Pg(hB\nt ) > 0. Then\nwe have Pg(hA\nt ) > 0, and \u00b5\u2217A\nt (x1:t|hA\nt ) = Pg(x1:t|hA\nt ) follows by induction hypothesis. Therefore\nPg(x1:t|hB t ) = Pg(x1:t|\u03c3t, mt, hA t ) = Pg(x1:t, \u03c3t, mt, hA t ) Pg(\u03c3t, mt, hA t ) \u02dcx1:t Pg(\u02dcx1:t, hA t )Pg(\u03c3t|\u02dcx1:t, hA t )Pg(mt|\u03c3t, \u02dcx1:t, hA t ) = Pg(x1:t, hA t )Pg(\u03c3t|x1:t, hA t )Pg(mt|\u03c3t, x1:t, hA t ) \ufffd \u02dcx1:t Pg(\u02dcx1:t, hA t )gA t (\u03c3t|hA t )\u03c3t(mt|\u02dcxt) = Pg(x1:t, hA t )gA t (\u03c3t|hA t )\u03c3t(mt|xt) \ufffd \u02dcx1:t Pg(\u02dcx1:t|hA t )\u03c3t(mt|\u02dcxt) = Pg(x1:t, hA t )\u03c3t(mt|xt) \ufffd \u02dcx1:t Pg(\u02dcx1:t, hA t )\u03c3t(mt|\u02dcxt) = Pg(x1:t|hA t )\u03c3t(mt|xt) \ufffd \u02dcx1:t \u00b5\u2217A t (\u02dcx1:t|hA t )\u03c3t(mt|\u02dcxt) = \u00b5\u2217B t (x1:t|hB t ) = \u00b5\u2217A t (x1:t|hA t )\u03c3t(mt|xt) \ufffd\nwhich means that \u00b5\u2217B\nt is consistent with g.\n\n- Suppose that \u00b5\u2217B\nt\nis consistent with g. Then consider any hA\nt+1 = (\u03c31:t, m1:t, u1:t) \u2208 HA\nt+1 such that Pg(hA\nt+1) > 0.\nThen we have Pg(hB\nt ) > 0, and \u00b5\u2217B\nt (x1:t|hB\nt ) = Pg(x1:t|hB\nt ) follows by induction hypothesis. Then we have\nPg(x1:t+1|hA t+1) = Pg(x1:t+1, hA t+1) Pg(hA t+1) $$=\\frac{\\mathbb{P}^{g}(x_{1:t},h_{t}^{B})\\mathbb{P}^{g}(u_{t}|x_{1:t},h_{t}^{B})\\mathbb{P}^{g}(x_{t+1}|x_{1:t},u_{t},h_{t}^{B})}{\\sum_{\\tilde{x}_{1:t},\\ \\mathbb{P}^{g}(\\tilde{x}_{1:t},h_{t}^{B})\\mathbb{P}^{g}(u_{t}|\\tilde{x}_{1:t},h_{t}^{B})}$$ $$=\\frac{\\mathbb{P}^{g}(x_{1:t},h_{t}^{B})g_{t}^{B}(u_{t}|h_{t}^{B})\\mathbb{P}(x_{t+1}|x_{t},u_{t})}{\\sum_{\\tilde{x}_{1:t},\\ \\mathbb{P}^{g}(\\tilde{x}_{1:t},h_{t}^{B})g_{t}(u_{t}|h_{t}^{B})}$$ $$=\\frac{\\mathbb{P}^{g}(x_{1:t},h_{t}^{B})}{\\sum_{\\tilde{x}_{1:t},\\ \\mathbb{P}^{g}(\\tilde{x}_{1:t},h_{t}^{B})}}\\cdot\\mathbb{P}(x_{t+1}|x_{t},u_{t})=\\mu_{t}^{*B}(x_{1:t}|h_{t}^{B})\\mathbb{P}(x_{t+1}|x_{t},u_{t})$$ $$=\\mu_{t}^{*A}(x_{1:t+1}|h_{t+1}^{A})$$\nwhich means that \u00b5\u2217A\nt+1 is consistent with g.\n\nProof of Theorem 1. Let \u00b5\u2217 be a belief system that satisfies Lemma 4. It is shown by Lemma 4 that \u00b5\u2217 is consistent with\nany strategy profile g. Hence to show that \u03bb\u2217 forms a CBB-PBE we only need to show sequential rationality.\n  Step 1: Fixing the principal's strategy to be \u03bb\u2217A, show that \u03bb\u2217B\n                                                            \u03c4:T is sequentially rational at any hB\n                                                                                             \u03c4 \u2208 HB\n                                                                                                   \u03c4 at any time \u03c4,\ngiven the belief \u00b5\u2217B\n                \u03c4 (hB\n                     \u03c4 ).\n  To prove Step 1, we argue that at hB\n                                    \u03c4 , the receiver is facing an MDP problem with state process \u03a0B\n                                                                                                 t = \u03baB\n                                                                                                        t (HB\n                                                                                                            t ) and\naction Ut for t \u2265 \u03c4.\n  First, we can write\n\n\ufffd t=\u03c4 rB t (Xt, Ut) E\u00b5\u2217B \u03c4 (hB \u03c4 ),\u03bb\u2217A t ,gB \u03c4:T \ufffd T \ufffd \ufffd t=\u03c4 E\u00b5\u2217B \u03c4 (hB \u03c4 ),\u03bb\u2217A t ,gB \u03c4:T [rB t (Xt, Ut)|HB t , Ut] = E\u00b5\u2217B \u03c4 (hB \u03c4 ),\u03bb\u2217A t ,gB \u03c4:T \ufffd T \ufffd\n\nwhere for any hB\n               t such that P\u00b5\u2217B\n                             \u03c4\n                               (hB\n                                 \u03c4 ),\u03bb\u2217A\n                                    t\n                                      ,gB\n                                        \u03c4:T (hB\n                                             t ) > 0 we have\n\n\u02dcx1:t rB t (\u02dcxt, ut)P\u00b5\u2217B \u03c4 (hB \u03c4 ),\u03bb\u2217A t ,gB \u03c4:T (\u02dcx1:t|hB t ) E\u00b5\u2217B \u03c4 (hB \u03c4 ),\u03bb\u2217A t ,gB \u03c4:T [rB t (Xt, Ut)|hB t , ut] = \ufffd \u02dcxt rB t (\u02dcxt, ut)\u03c0B t (\u02dcxt) =: \u02dcrB t (\u03c0B t , ut) = \ufffd\n\nwhere \u03c0B\n      t := \u03baB\n            t (hB\n                t ). The second equality is true due to Lemma 4.\n  Therefore we can write\n\n. \ufffd \ufffd t=\u03c4 \u02dcrB t (\u03a0B t , Ut) t=\u03c4 rB t (Xt, Ut) = E\u00b5\u2217B \u03c4 (hB \u03c4 ),\u03bb\u2217A t ,gB \u03c4:T \ufffd T \ufffd E\u00b5\u2217B \u03c4 (hB \u03c4 ),\u03bb\u2217A t ,gB \u03c4:T \ufffd T \ufffd\n\n We now show that \u03a0B\n                    t\n                       is a controlled Markov Chain controlled by Ut. By Definition 6, we have \u03a0B\n                                                                                         t+1\n                                                                                             =\n\u03bet+1(\u03a0A\n     t+1, \u03a3t+1, Mt+1), where \u03a0A\n                          t+1 = \u2113t(\u03a0B\n                                   t , Ut), \u03a3t+1 = \u03bb\u2217B\n                                                t+1(\u03a0A\n                                                    t+1). Therefore we have\n\nP\u00b5\u2217B \u03c4 (hB \u03c4 ),\u03bb\u2217A t ,gB \u03c4:T (\u03c0B t+1|hB t , ut) \u02dcmt+1 1{\u03c0B t+1=\u03bet+1(\u03c0A t+1,\u03c3t+1, \u02dcmt+1)}P\u00b5\u2217B \u03c4 (hB \u03c4 ),\u03bb\u2217A t ,gB \u03c4:T ( \u02dcmt+1|hB t , ut) = \ufffd \u02dcxt+1 \u03c3t+1( \u02dcmt+1|\u02dcxt+1)P\u00b5\u2217B \u03c4 (hB \u03c4 ),\u03bb\u2217A t ,gB \u03c4:T (\u02dcxt+1|hA t+1) = \ufffd \u02dcmt+1 1{\u03c0B t+1=\u03bet+1(\u03c0A t+1,\u03c3t+1, \u02dcmt+1)} \ufffd \u02dcxt+1 \u03c3t+1( \u02dcmt+1|\u02dcxt+1)\u03c0A t+1(\u02dcxt+1) = \ufffd \u02dcmt+1 1{\u03c0B t+1=\u03bet+1(\u03c0A t+1,\u03c3t+1, \u02dcmt+1)} \ufffd \u02dcxt+1 \u03c3t+1( \u02dcmt+1|\u02dcxt+1)\u03c0A t+1(\u02dcxt+1)\n\nwhere \u03c0A\n      t+1 = \u2113t(\u03c0B\n               t , ut), \u03c3t+1 = \u03bb\u2217A\n                             t+1(\u03c0A\n                                  t+1). The last equality is true due to Lemma 4.\n  By construction, \u03c3t+1 = \u03bb\u2217A\n                        t+1(\u03c0A\n                             t+1) induces the distribution C(\u03c0A\n                                                         t+1, \u03b3t+1) from \u03c0A\n                                                                        t+1. This means that\n                         \ufffd\n\n\u02dcmt+1 1{\u03c0B t+1=\u03bet+1(\u03c0A t+1,\u03c3t+1, \u02dcmt+1)} \ufffd = C(\u03c0A t+1, \u03b3t+1)(\u03c0B t+1)\nWe conclude that\n\nP\u00b5\u2217B \u03c4 (hB \u03c4 ),\u03bb\u2217A t ,gB \u03c4:T (\u03c0B t+1|hB t , ut) = C(\u2113t(\u03c0B t , ut), \u03b3t+1)(\u03c0B t+1).\n\n  In particular, this means that the conditional distribution of \u03a0B\n                                                                   t+1 given all of (\u03a0B\n                                                                                       1:t, U B\n                                                                                            1:t) is dependent only on (\u03a0B\n                                                                                                                          t , Ut),\nproving that \u03a0B\n               t is a controlled Markov Chain controlled by Ut for t \u2265 \u03c4.\n  Therefore, at hB\n                  \u03c4 , the receiver faces an MDP problem with state \u03a0B\n                                                                          t , action Ut, instantaneous reward \u02dcrB\n                                                                                                                  t (\u03a0B\n                                                                                                                      t , Ut) and\ntransition kernel P(\u03c0B\n                     t+1|\u03c0B\n                           t , ut) = C(\u2113t(\u03c0B\n                                            t , ut), \u03b3t+1)(\u03c0B\n                                                            t+1).\n  Now, by construction, we know that\n\n\u02c6vB t (\u03c0B t ) = max ut \ufffd \u02dcxt rB t (\u02dcxt, ut)\u03c0t(\u02dcxt) + V B t+1(\u2113t(\u03c0t, ut)) \ufffd\ufffd = max ut \ufffd \u02dcrB t (\u03c0B t , ut) + \ufffd \u02c6vB t+1(\u00b7)dC(\u2113t(\u03c0B t , ut), \u03b3t+1) \ufffd (3)\n\nand \u03bb\u2217B\n     t (\u03c0B\n         t ) attains the maximum in (3). Therefore \u03bb\u2217B\n                                                \u03c4:T solves the Bellman equation for the MDP problem specified\nabove, and hence is an optimal strategy. Furthermore, V B\n                                                1 (\u02c6\u03c0) =\n                                                       \ufffd\n                                                         \u02c6vB\n                                                          1 (\u00b7)dC(\u02c6\u03c0, \u03b3t) is the optimal total expected payoff for\nthe receiver when the principal plays \u03bb\u2217A.\n  Step 2: Fixing the receiver's strategy to be \u03bb\u2217B, show that \u03bb\u2217A\n                                                      \u03c4:T is sequentially rational at any hA\n                                                                                     \u03c4 \u2208 HA\n                                                                                          \u03c4 at any time \u03c4,\ngiven the belief \u00b5\u2217A\n               \u03c4 (hA\n                   \u03c4 ).\n  Similar to Step 1, we argue that at hA\n                                  \u03c4 , the principal is facing an MDP problem with state process \u03a0A\n                                                                                         t = \u03baA\n                                                                                               t (HA\n                                                                                                  t ) and\naction \u03a3t for t \u2265 \u03c4.\n  First, we write\n\n\ufffd \ufffd t=\u03c4 rA t (Xt, Ut) t=\u03c4 E\u00b5\u2217A \u03c4 (hA \u03c4 ),gA \u03c4:T ,\u03bb\u2217B t [rA t (Xt, Ut)|HA t , \u03a3t] E\u00b5\u2217A \u03c4 (hA \u03c4 ),gA \u03c4:T ,\u03bb\u2217B t \ufffd T \ufffd = E\u00b5\u2217A \u03c4 (hA \u03c4 ),gA \u03c4:T ,\u03bb\u2217B t \ufffd T \ufffd\n\n  Given that the receiver uses the CBB strategy \u03bb\u2217B, we know that Ut = \u03bb\u2217B\n                                                                          t (\u03a0B\n                                                                                t ) where \u03a0B\n                                                                                            t = \u03bet(\u03a0A\n                                                                                                     t , \u03a3t, Mt). For any\nhA\n t such that P\u00b5\u2217A\n                \u03c4\n                  (hA\n                    \u03c4 ),gA\n                       \u03c4:T ,\u03bb\u2217B\n                            t (hA\n                                t ) > 0 we have\n\nE\u00b5\u2217A \u03c4 (hA \u03c4 ),gA \u03c4:T ,\u03bb\u2217B t [rA t (Xt, Ut)|hA t , \u03c3t] \u02dcxt, \u02dcmt rA t (\u02dcxt, \u03bb\u2217B t (\u03bet(\u03c0A t , \u03c3t, \u02dcmt)))P\u00b5\u2217A \u03c4 (hA \u03c4 ),gA \u03c4:T ,\u03bb\u2217B t ( \u02dcmt, \u02dcxt|hA t , \u03c3t) = \ufffd \u02dcxt, \u02dcmt rA t (\u02dcxt, \u03bb\u2217B t (\u03bet(\u03c0A t , \u03c3t, \u02dcmt)))\u03c3t( \u02dcmt|\u02dcxt)P\u00b5\u2217A \u03c4 (hA \u03c4 ),gA \u03c4:T ,\u03bb\u2217B t (\u02dcxt|hA t , \u03c3t) = \ufffd $$=\\sum_{\\delta_{+},\\eta_{t}}^{\\pi,\\pi}r_{t}^{A}(\\tilde{x}_{t},\\lambda_{t}^{\\pi}{}^{B}(\\xi_{t}(\\pi_{t}^{A},\\sigma_{t},\\tilde{m}_{t})))\\sigma_{t}(\\tilde{m}_{t}|\\tilde{x}_{t})\\pi_{t}^{A}(\\tilde{x}_{t})=:\\tilde{r}_{t}^{A}(\\pi_{t}^{A},\\sigma_{t})$$\n\nwhere $\\pi_{t}^{A}:=r_{t}^{A}(h_{t}^{A})$. The third equality is true due to Lemma 4.\n\nTherefore we can write\n\n$$\\mathbb{E}^{\\mu,\\tau^{A}(h_{t}^{A}),\\sigma_{t-\\tau}^{A},\\lambda_{t}^{\\pi}}\\left[\\sum_{t=\\tau}^{T}r_{t}^{A}(X_{t},U_{t})\\right]=\\mathbb{E}^{\\mu,\\tau^{A}(h_{t}^{A}),\\sigma_{t-\\tau}^{A},\\lambda_{t}^{\\pi}}\\left[\\sum_{t=\\tau}^{T}\\tilde{r}_{t}^{A}(\\Pi_{t}^{A},\\Sigma_{t})\\right].$$\n\nWe now show that $\\Pi_{t}^{A}$ is a controlled Markov process with action $\\Sigma_{t}$. We know that \n\n\u03a0A\n t+1 = \u2113t(\u03a0B\n       t , U B\n         t ),\n              U B\n               t = \u03bb\u2217B\n                  t (\u03a0B\n                     t ),\n                          \u03a0B\n                           t = \u03bet(\u03a0A\n                                t , \u03a3t, Mt).\n\nHence \u03a0A\n       t+1 is a function of \u03a0A\n                         t , \u03a3t, and Mt. Furthermore,\n\n\u02dcxt \u03c3t(mt|\u02dcxt)P\u00b5\u2217A \u03c4 (hA \u03c4 ),gA \u03c4:T ,\u03bb\u2217B t (\u02dcxt|hA t , \u03c3t) P\u00b5\u2217A \u03c4 (hA \u03c4 ),gA \u03c4:T ,\u03bb\u2217B t (mt|hA t , \u03c3t) = \ufffd \u02dcxt \u03c3t(mt|\u02dcxt)\u03c0A t (\u02dcxt). = \ufffd\n\nTherefore the conditional distribution of \u03a0A\n                                           t+1 given (HA\n                                                         t , \u03a3t) depends only on (\u03a0A\n                                                                                     t , \u03a3t), proving that \u03a0A\n                                                                                                            t is a controlled\nMarkov process. We conclude that at hA\n                                       \u03c4 , the principal faces an MDP problem with state \u03a0A\n                                                                                             t , action \u03a3t, and instantaneous\nreward \u02dcrA\n        t (\u03a0A\n             t , \u03a3t) for t \u2265 \u03c4.\n  Next we will show that \u03bb\u2217A\n                            \u03c4:T is a dynamic programming solution of this MDP.\n  Induction Variant: V A\n                        t , as defined in (1f), is the value function for this MDP.\n  Induction Step: Suppose that V A\n                                   t+1 is the value function for this MDP at time t + 1 and consider the stage optimization\nproblem at \u03c0A\n             t .\n  Note that the instantaneous cost can be written as\n\n\u02dcxt, \u02dcmt rA t (\u02dcxt, \u03bb\u2217B t (\u03bet(\u03c0A t , \u03c3t, \u02dcmt)))\u03c3t( \u02dcmt|\u02dcxt)\u03c0A t (\u02dcxt) \u02dcrA t (\u03c0A t , \u03c3t) = \ufffd \ufffd \u02dcmt \u02c6xt \u03c3t( \u02dcmt|\u02c6xt)\u03c0A t (\u02c6xt) \u02c6xt \u03c3t( \u02dcmt|\u02c6xt)\u03c0A t (\u02c6xt) = \ufffd \ufffd\ufffd \ufffd \ufffd\ufffd \u02dcxt rA t (\u02dcxt, \u03bb\u2217B t (\u03bet(\u03c0A t , \u03c3t, \u02dcmt))) \u03c3t( \u02dcmt|\u02dcxt)\u03c0A t (\u02dcxt) \ufffd \ufffd \u02dcmt \u02dcxt rA t (\u02dcxt, \u03bb\u2217B t (\u03bet(\u03c0A t , \u03c3t, \u02dcmt)))\u03bet(\u03c0A t , \u03c3t, \u02dcmt)(\u02dcxt) \u02c6xt \u03c3t( \u02dcmt|\u02c6xt)\u03c0A t (\u02c6xt) = \ufffd \ufffd\ufffd \ufffd \ufffd\ufffd = E \ufffd \ufffd\ufffd \u02dcxt rA t (\u02dcxt, \u03bb\u2217B t (\u03a0B t ))\u03a0B t (\u02dcxt) \ufffd\ufffd\ufffd\u03c0A t , \u03c3t\nwhere \u03a0B\nt is a random distribution that follows the distribution induced by \u03c3t from \u03c0A\nt .\n\nHence objective function for the stage optimization can be written as\n\n$$\\tilde{Q}_{t}^{A}(\\pi_{t}^{A},\\sigma_{t})=\\tilde{r}_{t}^{A}(\\pi_{t}^{A},\\sigma_{t})+\\mathbb{E}[V_{t+1}^{A}(\\Pi_{t+1}^{A})|\\pi_{t}^{A},\\sigma_{t}]$$ $$=\\tilde{r}_{t}^{A}(\\pi_{t}^{A},\\sigma_{t})+\\mathbb{E}[V_{t+1}^{A}(\\ell_{t}(\\Pi_{t}^{B},\\lambda_{t}^{*B}(\\Pi_{t}^{B})))|\\pi_{t}^{A},\\sigma_{t}]$$ $$=\\mathbb{E}\\left[\\tilde{v}_{t}^{A}(\\Pi_{t}^{B})|\\pi_{t}^{A},\\sigma_{t}\\right]$$\nwhere\n\n$$\\hat{v}_{i}^{A}(\\pi_{i}):=\\sum_{\\pi_{k}}r_{i}^{A}(\\hat{x}_{k},\\lambda_{i}^{B}(\\pi_{i}))\\pi_{i}(\\hat{x}_{k})+V_{i+1}^{A}(\\ell_{i}(\\pi_{i}^{B},\\lambda_{i}^{B}(\\pi_{i}^{B})))$$\n\nBy construction of $\\lambda_{i}^{B}$, we know that $\\hat{v}_{i}^{A}=\\hat{v}_{i}^{A}$ (defined in (1c)). Therefore, the stage optimization problem can be reformulated as:\n\n$$\\max_{r_{i}\\in\\mathcal{U}_{i}(\\mathcal{X}_{i}\\cup\\mathcal{X}_{i})}\\left\\{\\hat{v}_{i}^{A}(\\cdot)dv_{i}\\right.$$ (SP) subject to $$\\left.\\nu_{i}\\right.$$ is incomplete from $$\\pi_{i}^{A}$$\n\nand the optimal signal is any signal that induces an optimal distribution \u03bd\u2217\n                                                                           t of (SP) from \u03c0A\n                                                                                            t .\n  By the seminal result on one-shot information design in [2], we know that the optimal value of (SP) is given by the\nconcave closure of the function \u02c6vA\n                                 t evaluated at \u03c0A\n                                                  t .\n  By construction, we have V A\n                                t\n                                   to be the concave closure of \u02c6vA\n                                                                     t . Furthermore, \u03bb\u2217A\n                                                                                        t (\u03c0A\n                                                                                             t ) is assumed to induce the\ndistribution \u03bdt = C(\u03c0A\n                      t , \u03b3t), where we know that\n                                                   \ufffd\n                                                     \u02c6vA\n                                                       t (\u00b7)dC(\u03c0A\n                                                                t , \u03b3t) = V A\n                                                                            t (\u03c0A\n                                                                                t ). Hence \u03bb\u2217A\n                                                                                             t (\u03c0A\n                                                                                                  t ) is an optimal solution\nfor the stage optimization problem, and V A\n                                          t\n                                             is the value function at time t, proving the induction step.\n\n  We conclude that \u03bb\u2217A\n                    \u03c4:T is an optimal strategy for the principal at hA\n                                                                \u03c4 given the belief system \u00b5\u03c4(hA\n                                                                                             \u03c4 ) and the receiver's\nstrategy \u03bb\u2217B. Furthermore, V A\n                          1 (\u02c6\u03c0) is the optimal total expected payoff for the principal when the receiver plays \u03bb\u2217A. Hence\nwe have completed the proof of sequential rationality.\n\n"
    }
]