[
    {
        "level": "#",
        "title": "Early Period Of Training Impacts Out-Of-Distribution Generalization",
        "content": "\nChen Cecilia Liu, Iryna Gurevych Ubiquitous Knowledge Processing Lab, Department of Computer Science, Hessian Center for AI (hessian.AI), Technical University of Darmstadt\n"
    },
    {
        "level": "##",
        "title": "Abstract",
        "content": "\nPrior research has found that differences in the early period of neural network training significantly impact the performance of in-distribution (ID) tasks. However, neural networks are often sensitive to out-of-distribution (OOD) data, making them less reliable in downstream applications. Yet, the impact of the early training period on OOD generalization remains understudied due to its complexity and lack of effective analytical methodologies. In this work, we investigate the relationship between learning dynamics and OOD generalization during the early period of neural network training. We utilize the trace of Fisher Information and sharpness, with a focus on gradual unfreezing (i.e. progressively unfreezing parameters during training) as the methodology for investigation. Through a series of empirical experiments, we show that 1) selecting the number of trainable parameters at different times during training, i.e. realized by gradual unfreezing - has a minuscule impact on ID results, but greatly affects the generalization to OOD data; 2) the absolute values of sharpness and trace of Fisher Information at the initial period of training are not indicative for OOD generalization, but the relative values could be; 3) the trace of Fisher Information and sharpness may be used as indicators for the removal of interventions during early period of training for better OOD generalization.\n"
    },
    {
        "level": "##",
        "title": "1 Introduction",
        "content": "\nWhile deep neural networks have achieved impressive results in their training tasks, they are often sensitive to distribution shifts (i.e. out-of-distribution, OOD) during inference. As in many applications of deep neural networks, the training and testing data may come from different distributions, such as training with perfect images or texts but inference with noise-corrupted data [21, 46], data obtained from different time periods [37, 58], or across different languages and domains [55, 54, 40, 34, 19], to name a few. Failure to generalize to the OOD setting degrades the models' robustness and reliability.\n\nA plethora of research has found that differences in the early period of training have a significant impact on the in-distribution (ID) performance [17, 1, 47, 15, inter alia] for a wide range of settings.\n\nThese settings include classification, multimodal learning, training from scratch, parameter-efficient fine-tuning or even federated learning. The wide observation of such a period in machine learning applications suggests that the early period of learning is generally important for neural network training [33], which is sometimes analogous to biological processes (such as the critical learning period in animals [1, 32]). In particular, prior literature identifies that modifications (or interventions) to the optimization process are critical ways to shape the early period of training. Training techniques such as weight decay [17], learning rates [27, 47], data augmentations [17, 42] (such as with MixUp [61]) or adding noise to weights [16] impact learning dynamics early on, and can significantly improve or hamper the final task results depending on the time of application or removal. Yet, there is very limited work exploring how the early period of training impacts generalization to OOD data during testing, as well as \"when\"\nthe early period of training significantly affects the outcome of final (OOD) results. [41] found that using gradual unfreezing [24] (i.e., progressively releasing trainable parameters during training at fixed time intervals) can impact the trace of Fisher Information in the early period of training; however, the work was focused on which parameters to select for better cross-lingual transfer (where cross-lingual transfer is a form of natural OOD generalization). Besides the trace of Fisher Information, other sharpness metrics have also been used to study the generalization of network training, especially after the success of methods such as Sharpness-Aware Minimization\n(SAM) [14, 36, 62], however, sharpness is less used in prior work to study the early period of training.\n\nIn this work, we delve deeper and advance our understanding of how the early period of training impacts OOD generalization through a series of empirical investigations. We first employ gradual unfreezing [24] as a method to intervene in the dynamics of the early period of training from scratch for OOD generalization and investigate how the early period of training impacts OOD generalization.\n\nThis is done in a simple and controlled setting. Next, we investigate which metrics are effective for studying the early period of training for OOD generalization. Here, we utilize Fisher Information and sharpness and show how these metrics change in the early period of training with gradual unfreezing, and examine their impact on the final solutions. After this, we focus on whether there is an optimal time (i.e. \"when\") to remove intervention to achieve a good trade-off between ID and OOD generalization. Along the way, we also try to answer the questions: how early does the \"early period\" begin and can our observations extend to other settings without gradual unfreezing or with different model architectures?\n\nTo summarize, in this paper, we show that 1) when considering the number of trainable parameters at a time, i.e. realized by gradual unfreezing - has a minuscule impact on ID results, but greatly affects the generalization on OOD data; 2) we also show that the value of sharpness and trace of Fisher Information during the initial period of training are not indicative for OOD generalization, but their relative values could be; 3) the trace of Fisher Information and sharpness may signify the removal of interventions during the early period of training.\n"
    },
    {
        "level": "##",
        "title": "2 Related Work",
        "content": "\nEarly period of neural network training. Under the standard usage of the term generalization\n(in-distribution, where training and testing data are assumed to be from the same distribution), prior work [17] shows that the early period of training of neural networks exhibits a \"critical learning period\" when trained from scratch. Regularization and interventions applied in this critical period affect final task results. [27] indicates that when learning with a lower learning rate, Fisher Information exhibits an \"explosion\"\nwhich impedes generalization. Applying regularization to the trace of Fisher Information alleviates the negative impact of the high Fisher Information. [42] shows the termination of MixUp early in training and switching to standard empirical risk minimization helps with better ID generalization. [60, 16] shows that even winning \"lottery tickets\" emerge in the early period of training with large learning rates. The critical learning period is also found in many other settings, such as in multimodal models [32], in linear models [33], in transformers [47] and Federated learning [57]. However, prior work focuses on in-distribution generalization, ignoring the setting of OOD generalization.\n\n[41] shows the early period of training relates to cross-lingual transfer performance in the parameterefficient fine-tuning (PEFT) setting with transformer models, connecting the early period of training with progressive parameter unfreezing and OOD generalization (as cross-lingual data are naturallyoccurring OOD data). We utilize findings in [41] and base our work on gradual unfreezing [24]. In this paper, we focus on 1) a general setup (i.e. training from scratch), and 2) the characterization of the early period of training and its relationship to OOD generalization.\n\nFisher Information, sharpness and generalization. Fisher Information has been studied in many prior works such as [5, 45] to investigate and improve optimization behaviour. Similarly, sharpness is another popular metric used to study optimization behaviour and its relationship to generalization.\n\n[28] found a correlation between sharpness and the ratio of learning rate to batch size. [29, 11, 49] give theoretical guarantees on the generalization error using sharpness-related measures and conduct large empirical experiments showing that sharpness-based measures correlate with generalization. However, there have been debates on whether sharp minima (such as a high largest eigenvalue of the training Hessian, \u03bbmax) imply poor generalization [9] and demonstrate the limits of \u03bbmax in explaining in-distribution generalization [30]. Most of the current research efforts are towards analyzing the loss landscape at convergence, for in-distribution generalization. For OOD, [2] shows adaptive sharpness is not a stable measurement for OOD generalization of the final solution. However, the relationship between metrics such as Fisher Information, sharpness and OOD generalization in the early period of training is unclear.\n"
    },
    {
        "level": "##",
        "title": "3 Preliminaries 3.1 Fisher Information Matrix (Fim)",
        "content": "\nTo investigate the training process, we first look at the Fisher Information [13]. Fisher Information reflects the local curvature and measures the amount of information with respect to network parameters, i.e. how sensitive the network predictions are to the changes in parameters. A larger Fisher Information indicates that a small change in network parameters can change the output significantly, which can be interpreted as a \"sharper\" loss landscape.\n\nLet x be the inputs and y be the labels of a dataset D. Given a neural network that is parameterized by w. The Fisher Information is defined as:\n\n$$F(w)=\\mathbb{E}_{P_{x,y}}\\left[\\nabla_{w}\\log p_{w}(\\hat{y}|x)\\nabla_{w}\\log p_{w}(\\hat{y}|x)^{T}\\right].\\tag{1}$$\n\nEstimating the full Fisher Information is usually expensive, prior work shows that the trace of the Fisher Information ($\\texttt{tr}(\\text{F})$) correlates well with the full Fisher Information and can be used for real applications and capture signals during the learning process [1, 27, 53, inter alia]. Using the empirical data distribution $\\hat{Q}(x)$:\n\n$$\\texttt{tr}(\\text{F})=\\mathbb{E}_{x\\sim\\hat{Q}(x)}\\,\\mathbb{E}_{\\hat{y}\\sim p_{w}(\\hat{y}|x)}\\,||\\nabla_{w}\\log p_{w}(\\hat{y}|x)||^{2}.\\tag{2}$$\n"
    },
    {
        "level": "##",
        "title": "3.2 Sharpness",
        "content": "\nLet $\\mathcal{L}_{P}(w)=\\frac{1}{|D|}\\sum_{(x,y)\\in D}\\log p_{w}(y|x)$ be the loss over training datasets of a neural network parameterized by $w$, and $\\delta$ be a small perturbation drawn from a noise distribution, such as a Gaussian distribution $\\mathcal{N}(0,\\rho^{2}diag(c^{2}))$. The definition of worst-case and average-case sharpness are [14, 36, 2, 22]:\n\n$$S^{\\rho}_{avg}=\\mathbb{E}_{\\delta\\sim\\mathcal{N}(0,\\rho^{2}diag(c^{2}))}\\,\\mathcal{L}_{D}(w-\\delta)-\\mathcal{L}_{D}(w),\\tag{3}$$\n\n$$S^{\\rho}_{worst}=\\max_{\\|\\delta\\|c^{-1}\\|_{p}\\leq\\rho}\\,\\mathcal{L}_{D}(w-\\delta)-\\mathcal{L}_{D}(w),\\tag{4}$$\n\nwhere $\\odot c^{-1}$ is element-wise multiplication.\n\nThe sharpness here refers to how rapidly the loss changes with respect to the changes in the model parameters.1 While both the Fisher Information and sharpness are used for investigating loss landscapes and generalization, they offer different views of the training process. Both S\u03c1\navg and S\u03c1\nworst are studied in the prior literature for generalization. For instance, [29, 10, 36]\nshow that the worst-case sharpness correlates better with generalization, at convergence. While prior work believe that flatter (less sharp) minima in the loss landscape lead to better generalization in neural networks [22, 31, 26, 4], these metrics' attribution to the early period of training and how their early period trends are related to OOD generalization is understudied.\n"
    },
    {
        "level": "##",
        "title": "3.3 Gradual Unfreezing",
        "content": "\nGradual unfreezing [24] is a simple tuning method that progressively increases the number of trainable parameters (i.e. unfreeze, layer-by-layer) of the neural network from the top to the bottom of the network with a fixed interval of training steps, k (i.e. the unfreezing time). In this paper, we used a modified formulation of gradual unfreezing [41], where we progressively unfreeze the parameters during the early period in the training top-down and for \"blocks\" of parameters (a block of parameters can be a single layer or several consecutive layers, in our case, we use the namespace of the parameters used in standard implementations to determine blocks). See Appendix A for details and the algorithm.\n"
    },
    {
        "level": "##",
        "title": "4 Experimental Setup",
        "content": "\nTo investigate our research questions, we perform two sets of experiments. First in a controlled setup, with datasets such as CIFAR10 [35] and models such as ResNet [20] to observe patterns and validate our hypothesis. Then, we experiment with the complex setup with transformer models. We use \u03c1 = 0.01 to calculate the sharpness (both average-case and worst-case) with 15 samples, and L2\nnorm for the worst-case sharpness. We normalize the tr(F) by the number of trainable parameters.\n\nWe use the Auto-PGD algorithm [8] as implemented in [2] (we refer the readers to the original papers for details) for computing worst-case sharpness, as it is a hyperparameter-free estimation method.\n\nControlled Setup.\n\nIn this setup, following [21], we use ResNet-18 and VGG-11 as the neural networks for this study (training from scratch). We perform experiments on classic image classification datasets and use MNIST [38], CIFAR10 [35] and CIFAR100 [35] for training. For out-of-distribution evaluation, we use the corrupted corresponding evaluation datasets, named MNIST-C [48], CIFAR10- C [21] and CIFAR-100-C [21] (results averaged across corruptions and severities). The ID evaluation sets are the original test sets respectively.\n\nWe use the SGD optimizer and the CIFAR datasets are applied with standard augmentations (i.e.\n\nrandom crops and flips). We report results over 6 random seeds for MNIST (due to the high variance in OOD results), and we use 4 random seeds for other datasets. Other hyperparameters such as learning rate or weight decay are specified in Appendix B. In our experiments, the default learning rate specified in Appendix B is denoted as lrd and we also experimented with reduced learning rates which are 1/10th of the default, specified as 0.1*lrd.\n\nComplex Setup.\n\nWe also conduct experiments using transformers. As pre-train then fine-tuning becomes a popular way to adapt general foundational models to downstream tasks, we examine the cross-lingual transfer (train with English data, test with other languages) task using parameterefficient fine-tuning (PEFT) with the LoRA [25] adapters (HuggingFace PEFT [44] implementation).\n\nThis setting is parallel to our controlled setting because: 1) only English data (i.e. ID data) is used for training and validation (other language data are for testing), this is a natural setting of OOD generalization with parallel evaluation protocol to the image tasks; 2) using LoRA adapters allows us to inject randomly initialized parameters for learning, which is analogous to our controlled setting.\n\nWe train with SQuAD [51] (English, question and answering task) and MNLI [56] (English, natural language inference task), and evaluate on XQuAD [3], MLQA [39] and XNLI [7]. We use XLM- RoBERTa [6] as the pre-trained multilingual transformer backbones and AdamW [43] as the optimizer.\n\nWe report results across 4 seeds for all experiments in this setting. Please see Appendix B for hyperparameters.\n"
    },
    {
        "level": "##",
        "title": "5 Gradual Unfreezing In The Early Period Of Training Can Improve Out-Of-Distribution Generalization",
        "content": "\nRecall from \u00a7 2, where gradual unfreezing improved OOD performance for PEFT with transformers. Here, we first validate that gradual unfreezing applied to the early period of training in our controlled setting (training from scratch) could also help OOD generalization. By examining three different datasets and two model architectures in Table 1, progressive parameter unfreezing (i.e. gradual unfreezing) does not influence ID results by a large margin (mostly minor degradation, but can also positively impact the ID results). However, gradual unfreezing has a non-negligible positive impact on the OOD results. This observation is also applicable to different learning rates, although the default (larger) learning rate empirically is better for both ID and OOD for CIFAR datasets. Here, we provide evidence that gradual unfreezing can improve OOD performance when training from scratch even if it was proposed for transfer learning [24], and validate the usability of gradual unfreezing as an intervention for our study.\n\n|             | MNIST RN18   | CIFAR10 RN18   | CIFAR100 RN18   | CIFAR10 VGG11   |\n|-------------|--------------|----------------|-----------------|-----------------|\n| Method      | ID / OOD     | ID / OOD       | ID / OOD        | ID / OOD        |\n| lr          |              |                |                 |                 |\n| d           |              |                |                 |                 |\n| 99.06/33.36 | 93.32/72.36  | 71.07/45.10    | 88.62/71.63     |                 |\n| lr          |              |                |                 |                 |\n| d           |              |                |                 |                 |\n| + GU        | 98.98/       |                |                 |                 |\n| 63.99       |              |                |                 |                 |\n| 93.26/      |              |                |                 |                 |\n| 72.95       |              |                |                 |                 |\n| 71.03/      |              |                |                 |                 |\n| 46.34       |              |                |                 |                 |\n| 88.53/      |              |                |                 |                 |\n| 72.16       |              |                |                 |                 |\n| 0.1*        |              |                |                 |                 |\n| lr          |              |                |                 |                 |\n| d           |              |                |                 |                 |\n| 99.26/58.46 | 91.66/71.14  | 69.95/44.59    | 86.93/68.41     |                 |\n| 0.1*        |              |                |                 |                 |\n| lr          |              |                |                 |                 |\n| d           |              |                |                 |                 |\n| + GU        | 99.18/       |                |                 |                 |\n| 62.51       |              |                |                 |                 |\n| 91.51/      |              |                |                 |                 |\n| 71.26       |              |                |                 |                 |\n| 70.67/      |              |                |                 |                 |\n| 46.03       |              |                |                 |                 |\n| 87.01/      |              |                |                 |                 |\n| 69.45       |              |                |                 |                 |\n"
    },
    {
        "level": "##",
        "title": "6 Early Period Of Training Impacts Out-Of-Distribution Generalization 6.1 Evidence Of Impact On Out-Of-Distribution Generalization",
        "content": "\n[17, 1] among others show that for ID generalization, there is a \"critical learning period\" of the neural network. ID generalization degrades with the delaying application of regularization, as well as the removal of regularization. Using gradual unfreezing, we experimented with different unfreezing steps k (ranging from 1 to equally dividing the total training steps among the number of trainable parameter blocks) to measure its impact on both ID and OOD test results. Indeed (as in Figure 1), it is possible that withholding trainable parameters can influence the OOD generalization as early as after training on a single batch of data. The effect is especially prominent for simpler datasets like MNIST. Prolonging the unfreezing interval of parameters during training initially results in minimal change in the ID test performance with a larger learning rate, subsequently leading to quick deterioration of the ID results. The deterioration of ID results over unfreezing intervals aligns with trends observed in the early stages of training using other interventions, as reported in prior work [17, 1], while the effects on OOD results serve as evidence that the early period of training can impact OOD generalization. Gradual unfreezing casts interesting trends on the OOD generalization and shows the trade-off between ID and OOD generalizations. Before the quick deterioration of ID results, there is a short window where OOD results could be improved. As soon as the ID results start decreasing rapidly, the OOD results first increase, then rapidly decrease for CIFAR10/100 with ResNet, but not in MNIST.\n\nThere seems to be a range of k in the early period of training, such that the ID results decayed minimally, but with better OOD results, and we will examine this in more detail in \u00a7 6.4.\n"
    },
    {
        "level": "##",
        "title": "6.2 Learning Dynamics In The Early Period Of Training",
        "content": "\nObserving Figure 2, by freezing the number of trainable parameters at a time (and gradually unfreezing them), we can induce higher Fisher Information and larger S\u03c1\navg, S\u03c1\nworst at the beginning of training compared to the standard training procedure, although there is an anomaly in the tr(F) of CIFAR100\nwith ResNet. In general, the longer we withhold parameters, the higher the level of sharpness and tr(F) we can sustain, unfreezing parameters reduce these metrics.\n\nWhile there are variations between S\u03c1\navg, S\u03c1\nworst and tr(F) they are all sensitive to the early period of training and interventions. S\u03c1\navg shows more consistent trends across datasets and architectures compared to the other two metrics. Due to the randomness in estimating S\u03c1\navg, S\u03c1\nworst, and tr(F), it is also evident that a single, absolute largest value of these metrics during the early period of training may not be a consistent indication of OOD generalization (or ID generalization, in fact).\n\nThis indicates that the discussion for a high or low value of S\u03c1\n                                                               avg, S\u03c1\n                                                                     worst, or tr(F) during the early\nperiod of training should be relative rather than absolute.\n\nEmpirically, our findings differed from prior work on ID generalization (such as [27]) that demon-\nstrates the 'explosion' of tr(F) during the early period of training (due to using a small learning rate)\nis harmful. Here, a higher tr(F) induced by parameter freezing does not hurt generalization, in both\nID and OOD. When considering the trainable parameters as a variable, having initial higher sharpness\nor tr(F) can be advantageous up to a certain time frame during training. More interestingly, some\nsub-figures (such as Figure 2 (f) and (j)) show a rapid increase of sharpness after about 100 training\nsteps (see the k=750 curve). The tr(F) and sharpness transition from a transient phase to a relatively\nstabilized value when withholding parameters for long. Introducing new trainable parameters induces\na quick drop in the corresponding metrics. As a result, the optimization trajectory and learning\nmechanism change when manipulating the trainable parameters through unfreezing during the early\nperiod of training (such as the network resorting to higher rank features at the initial learning period,\nmore details in Appendix E.1).\n\nOverall, these results suggest that while a lower sharpness or tr(F) during the early period of\ntraining may be good for ID generalization, when factoring in the trainable parameters (as also shown\nempirically in Table 1), a lower initial sharpness or tr(F) could lead to worse OOD generalization.\nThis observation applies strictly to the very early period of training, and the eventual reduction of\nsharpness or tr(F) after the initial period is still desirable (evident in Figure 1, Figure 2, and work\nlike SAM [14, 36, 52]). Importantly, while sharpness and tr(F) are effective metrics to study the\nearly period of training, we need to look at their relative trends for OOD generalization (in fact, also\nin ID).\n"
    },
    {
        "level": "##",
        "title": "6.3 Final Solutions And Out-Of-Distribution Results",
        "content": "\nChanging the learning dynamics in the early period of training inevitably results in different final solutions. For example with CIFAR10 on ResNet, the largest eigenvalue of the Hessian (\u03bbmax, calculated on a subset of training data) of final solutions shows a negative correlation with OOD results\n(see Appendix C). However, such a negative correlation is not consistent nor always statistically significant across different setups. Our results complement the findings in [2], which serve as additional evidence of the need for developing new robust metrics and further thorough investigation for OOD generalization.\n"
    },
    {
        "level": "##",
        "title": "6.4 Learning Dynamics Could Signify The Time Period To Remove Interventions",
        "content": "\nWhile the value of tr(F) or sharpness during the initial period of learning (or at the end of learning)\nmay not be indicative of good OOD generalization in general, they could signify the time to release parameters for minimal ID decay and better OOD results. When unfreezing parameters (Figure 2), we observed that in many cases, those metrics experience a \"transient\" period (the first 50-100 steps, characterized by rapid growth or drop of the sharpness or tr(F)), followed by a 'stabilization' phase where the rate of change in metric values slows down (unless parameters are released).\n\nNotice that the best range of k for overall best ID and OOD results is after the stabilization of sharpness and tr(F) (in Figure 1 and Figure 2), but not for too long. For instance in Figure 1 (b), although the OOD results improved significantly, the ID results deteriorated drastically after 800-1000 training steps (at least 1 point drop for CIFAR10/100). This observation leads to the conjecture that the best time to remove the intervention (i.e. unfreezing parameters in our case) while keeping reasonable ID results (less than 0.5 points decrease in accuracy) and achieving better OOD results must satisfy two constraints: 1) after the initial rapid change of sharpness or tr(F) (the transient phase), and 2) not too far into the stabilization phase.\n\nThe second constraint is self-evident in Figure 1, as a larger k hurts both ID and OOD results. To quickly validate the first constraint, we use MNIST to pick the earliest ending step of the transient period among three metrics (S\u03c1\nworst, S\u03c1\navg and tr(F)). Then we experiment with 10 different k values each consecutively (10 steps apart) that are smaller or greater than that k value. For the smaller ks, we get 98.93/52.72 as the median ID/OOD results, for the larger ks, we get 98.91/53.54 as the median ID/OOD results, which validates the constraints. The stabilization of examined metrics indicates when to introduce new trainable parameters. Next, we use a heuristic algorithm that satisfies the above-mentioned constraints to determine the stabilization time of the three metrics (we first detect a significant change in metrics, then detect the stabilization point of the metrics, the algorithm is in Appendix D). The OOD results are then compared with with ten random k values per dataset (k \u2264 800) to determine the winning rate (i.e., the percentage of times where the value picked by the algorithm is better than a sampled value), and the results are in Table 2. Empirically, using such an algorithm is better than doing a random hyperparameter search the majority of the time. In most cases, the degradation of ID accuracy is within 0.5 points, except when using the VGG network. Nonetheless, this further validates that the stabilization of S\u03c1\nworst, S\u03c1\navg and tr(F) could signify the removal of interventions (in our case gradual unfreezing) to trade some ID performance for OOD. While tr(F) shows better results, there isn't a clear winning metric for intervention removal due to: 1) metrics exhibiting high noise during training; and 2) the determined stabilization points from different metrics collide or are very close to each other. We will defer the exploration of more sophisticated algorithms for future work. However, it's worth noting the existence of an optimal time that effectively balances good ID and ODD results.\n\n| MNIST RN18   | CIFAR10 RN18   | CIFAR100 RN18   | WR          | CIFAR10 VGG11   | WR          |\n|--------------|----------------|-----------------|-------------|-----------------|-------------|\n| Method       | ID / OOD       | ID / OOD        | ID / OOD    | -               | ID / OOD    |\n| Standard     | 99.06/33.36    | 93.32/72.36     | 71.07/45.10 | -               | 88.62/71.63 |\n| GU           |                |                 |             |                 |             |\n| S            |                |                 |             |                 |             |\n| \u03c1            |                |                 |             |                 |             |\n| worst        |                |                 |             |                 |             |\n| 98.78/52.48  | 93.06/72.75    | 70.68/45.19     | 60%         | 87.69/71.47     | 40%         |\n| GU           |                |                 |             |                 |             |\n| S            |                |                 |             |                 |             |\n| \u03c1            |                |                 |             |                 |             |\n| avg          |                |                 |             |                 |             |\n| 98.78/52.48  | 93.02/72.58    | 70.67/45.35     | 60%         | 87.71/          |             |\n| 72.37        |                |                 |             |                 |             |\n| 100%         |                |                 |             |                 |             |\n| GU           |                |                 |             |                 |             |\n| tr           |                |                 |             |                 |             |\n| (F)          |                |                 |             |                 |             |\n| 98.91/       |                |                 |             |                 |             |\n| 54.12        |                |                 |             |                 |             |\n| 93.02/       |                |                 |             |                 |             |\n| 73.56        |                |                 |             |                 |             |\n| 70.78/       |                |                 |             |                 |             |\n| 45.82        |                |                 |             |                 |             |\n| 83%          | 88.40/71.86    | 60%             |             |                 |             |\n"
    },
    {
        "level": "##",
        "title": "7 Generality Of Findings On The Early Period Of Training 7.1 Higher Initial Sharpness Via Learning Rate",
        "content": "\nK=200\nUsing gradual unfreezing is a specific case where high sharpness at the initial learning period could benefit OOD generalizations. A critical question is: is there another way to intervene in the early period of training with higher sharpness, that also positively impacts OOD generalization? Recall that a higher learning rate typically results in lower sharpness\n(and a lower learning rate results in higher sharpness, as indicated in\n[27]). Based on our findings in the previous sections, we hypothesize that using a lower learning rate at the initial period of learning, then switching to a higher learning rate later (high sharpness to low sharpness), may help OOD generalization (surprisingly, this is exactly a simple form of learning rate warm-up!).\n\nTo validate, we use the CIFAR10 dataset on ResNet18 with two learning rates: we initially use 1/10th of the default learning rate, then increase the learning rate to the default value after k steps (i.e.\n\nlow-to-high, denoted as lrl2h, and the reverse is lrh2l. k is determined using random hyperparameter search). The results are in Table 3 (an example sharpness profile is in Figure 3, with more details in Appendix E.2); indeed, lrl2h can provide better OOD results (and without degrading ID results). Further, with the learning rate switching step k determined using\n\nMethod\nID/OOD\nMethod\nID/OOD\nlrd\n93.32/72.36\nlrl2h\n93.35/72.89\n0.1*lrd\n91.66/71.14\nlrh2l\n91.58/71.18\n\ntr(F), S\u03c1\navg and S\u03c1\nworst, the OOD results are 72.57 and 72.94 (same k for both sharpness metrics)\nrespectively.\n"
    },
    {
        "level": "##",
        "title": "7.2 Empirical Validations In Transformers",
        "content": "\nFor our complex experimental setup (described in \u00a74), Figure 4 shows the learning dynamics of XLM-R with MNLI in the early period, withholding trainable parameters increases the sharpness and tr(F), note that in Figure 4 (c) the S\u03c1\nworst value is negative, withholding trainable parameters still increase the S\u03c1\nworst during training based on Eqn. 4 (the learning dynamics for the SQuAD dataset is in Appendix E.3).\n\nSimilarly, the results using tr(F) to determine the k for unfreezing is in Table 4 (k values are in Appendix D, results with S\u03c1\nworst / S\u03c1\navg are in Table 7 in the Appendix) and the winning rate is 80%.\n\nThe ID results are not sacrificed in this experimental setting, hence further pointing towards that the stabilization of sharpness and tr(F) could signify 'when' to remove intervention in the early period of training for better OOD generalization.\n\n|             |               | XQuAD         | MLQA       | XNLI        | WR             |\n|-------------|---------------|---------------|------------|-------------|----------------|\n| Method      | F1- En/X-ling | EM- En/X-ling | F1- X-ling | EM- X-ling  | Acc- En/X-ling |\n| Standard    | 82.96/68.72   | 71.39/52.64   | 56.27      | 40.93       | 83.17/71.84    |\n| GU          |               |               |            |             |                |\n| tr          |               |               |            |             |                |\n| (F)         |               |               |            |             |                |\n| 83.77/70.70 | 72.33/54.40   | 58.47         | 42.31      | 83.36/72.49 | 80%            |\n"
    },
    {
        "level": "##",
        "title": "8 Conclusions",
        "content": "\nIn this work, we investigate the early period of training and its impact on out-of-distribution generalization. We demonstrate that using gradual unfreezing to modulate the number of trainable parameters during the early period of training can affect out-of-distribution generalization in various settings, including transformers, and reveal that the number of trainable parameters at a time is an important factor that was missing in the previous literature. We observe different patterns to previous work in in-distribution generalization, where higher sharpness and tr(F) during the early period of training may be beneficial for out-of-distribution generalization. We reveal that metric values during the early period of training may not be indicative of the out-of-distribution generalization, however, they could signify \"when\" to remove interventions such as gradual unfreezing (or to increase the learning rate,\n\u00a77.1) for better results.\n\nThe significance of effective training and fine-tuning, along with the growing emphasis on research into techniques that involve modifying only partial parameters of the final model, such as freezing parameters (e.g. Adapters [23, 50, 25] or pruning) or network expansions [59, 12, 18] cannot be overstated. Our empirical investigations highlight the need to develop a more theoretical understanding of the early period of training and OOD generalization, as well as the creation of new theoretical metrics for better indication of OOD generalization.\n"
    },
    {
        "level": "##",
        "title": "9 Acknowledgement",
        "content": "\nThis work was funded by the German Federal Ministry of Education and Research (BMBF) under the promotional reference 13N15897 (MISRIK).\n"
    },
    {
        "level": "##",
        "title": "References",
        "content": "\n[1] Alessandro Achille, Matteo Rovere, and Stefano Soatto. Critical learning periods in deep\nnetworks. In 7th International Conference on Learning Representations, ICLR 2019, New\nOrleans, LA, USA, May 6-9, 2019, 2019.\n[2] Maksym Andriushchenko, Francesco Croce, Maximilian M\u00fcller, Matthias Hein, and Nicolas\nFlammarion. A modern look at the relationship between sharpness and generalization. In\nInternational Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu,\nHawaii, USA, volume 202 of *Proceedings of Machine Learning Research*, pages 840\u2013902.\nPMLR, 2023.\n[3] Mikel Artetxe, Sebastian Ruder, and Dani Yogatama. On the cross-lingual transferability of\nmonolingual representations. In Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 4623\u20134637, Online, July 2020. Association for Computational\nLinguistics.\n[4] Junbum Cha, Sanghyuk Chun, Kyungjae Lee, Han-Cheol Cho, Seunghyun Park, Yunsung Lee,\nand Sungrae Park. SWAD: domain generalization by seeking flat minima. In Advances in Neural\nInformation Processing Systems 34: Annual Conference on Neural Information Processing\nSystems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 22405\u201322418, 2021.\n[5] Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian\nBorgs, Jennifer T. Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-SGD: Biasing gradient descent into wide valleys. In 5th International Conference on Learning Representations,\nICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net,\n2017.\n[6] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek,\nFrancisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020,\npages 8440\u20138451. Association for Computational Linguistics, 2020.\n[7] Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger\nSchwenk, and Veselin Stoyanov. XNLI: Evaluating cross-lingual sentence representations. In\nProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,\npages 2475\u20132485, Brussels, Belgium, October-November 2018. Association for Computational Linguistics.\n[8] Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an\nensemble of diverse parameter-free attacks. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 2206\u20132216. PMLR, 2020.\n[9] Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize\nfor deep nets. In Proceedings of the 34th International Conference on Machine Learning, ICML\n2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine\nLearning Research, pages 1019\u20131028. PMLR, 2017.\n[10] Gintare Karolina Dziugaite, Alexandre Drouin, Brady Neal, Nitarshan Rajkumar, Ethan Caballero, Linbo Wang, Ioannis Mitliagkas, and Daniel M. Roy. In search of robust measures of generalization. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.\n[11] Gintare Karolina Dziugaite and Daniel M. Roy. Computing nonvacuous generalization bounds\nfor deep (stochastic) neural networks with many more parameters than training data.\nIn\nProceedings of the Thirty-Third Conference on Uncertainty in Artificial Intelligence, UAI 2017,\nSydney, Australia, August 11-15, 2017. AUAI Press, 2017.\n[12] Utku Evci, Bart van Merrienboer, Thomas Unterthiner, Fabian Pedregosa, and Max Vladymyrov.\nGradMax: Growing neural networks using gradient information. In The Tenth International\nConference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. Open-\nReview.net, 2022.\n[13] Rory A. Fisher. Theory of statistical estimation. Mathematical Proceedings of the Cambridge\nPhilosophical Society, 22:700 - 725, 1925.\n[14] Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for efficiently improving generalization. In 9th International Conference on Learning\nRepresentations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.\n[15] Stanislav Fort, Gintare Karolina Dziugaite, Mansheej Paul, Sepideh Kharaghani, Daniel M.\nRoy, and Surya Ganguli. Deep learning versus kernel learning: an empirical study of loss landscape geometry and the time evolution of the neural tangent kernel. In Advances in Neural\nInformation Processing Systems 33: Annual Conference on Neural Information Processing\nSystems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.\n[16] Jonathan Frankle, David J. Schwab, and Ari S. Morcos. The early phase of neural network\ntraining. In 8th International Conference on Learning Representations, ICLR 2020, Addis\nAbaba, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.\n[17] Aditya Golatkar, Alessandro Achille, and Stefano Soatto. Time matters in regularizing deep\nnetworks: Weight decay and data augmentation affect early learning dynamics, matter little near convergence. In Advances in Neural Information Processing Systems 32: Annual Conference on\nNeural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver,\nBC, Canada, pages 10677\u201310687. Curran Associates, Inc., 2019.\n[18] Xiaotao Gu, Liyuan Liu, Hongkun Yu, Jing Li, Chen Chen, and Jiawei Han. On the transformer growth for progressive BERT training. In Proceedings of the 2021 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, pages 5174\u20135180, Online, June 2021. Association for Computational Linguistics.\n[19] Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. In 9th\nInternational Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May\n3-7, 2021. OpenReview.net, 2021.\n[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR\n2016, Las Vegas, NV, USA, June 27-30, 2016, pages 770\u2013778. IEEE Computer Society, 2016.\n[21] Dan Hendrycks and Thomas G. Dietterich. Benchmarking neural network robustness to common\ncorruptions and perturbations. In 7th International Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.\n[22] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Flat minima. *Neural computation*, 9(1):1\u201342, 1997.\n[23] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe,\nAndrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning\nfor NLP. In *Proceedings of the 36th International Conference on Machine Learning*, volume 97 of *Proceedings of Machine Learning Research*, pages 2790\u20132799. PMLR, 09\u201315 Jun 2019.\n[24] Jeremy Howard and Sebastian Ruder. Universal language model fine-tuning for text classification. In Proceedings of the 56th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 328\u2013339, Melbourne, Australia, July 2018. Association for Computational Linguistics.\n[25] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,\nLu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In\nInternational Conference on Learning Representations, 2022.\n[26] Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry P. Vetrov, and Andrew Gordon\nWilson. Averaging weights leads to wider optima and better generalization. In Proceedings\nof the Thirty-Fourth Conference on Uncertainty in Artificial Intelligence, UAI 2018, Monterey,\nCalifornia, USA, August 6-10, 2018, pages 876\u2013885. AUAI Press, 2018.\n[27] Stanislaw Jastrzebski, Devansh Arpit, Oliver Astrand, Giancarlo B Kerg, Huan Wang, Caiming\nXiong, Richard Socher, Kyunghyun Cho, and Krzysztof J Geras. Catastrophic fisher explosion:\nEarly phase fisher matrix impacts generalization. In Proceedings of the 38th International\nConference on Machine Learning, volume 139 of *Proceedings of Machine Learning Research*,\npages 4772\u20134784. PMLR, 18\u201324 Jul 2021.\n[28] Stanislaw Jastrzebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua\nBengio, and Amos J. Storkey. Three factors influencing minima in SGD. *ArXiv*, abs/1711.04623, 2017.\n[29] Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantastic\ngeneralization measures and where to find them. In 8th International Conference on Learning\nRepresentations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.\n[30] Simran Kaur, Jeremy Cohen, and Zachary Chase Lipton. On the maximum hessian eigenvalue\nand generalization. In Proceedings on \"I Can't Believe It's Not Better! - Understanding\nDeep Learning Through Empirical Falsification\" at NeurIPS 2022 Workshops, volume 187 of\nProceedings of Machine Learning Research, pages 51\u201365. PMLR, 03 Dec 2023.\n[31] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping\nTak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima.\nIn 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April\n24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017.\n[32] Michael Kleinman, Alessandro Achille, and Stefano Soatto. Critical learning periods for\nmultisensory integration in deep networks. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pages 24296\u201324305. IEEE, 2023.\n[33] Michael Kleinman, Alessandro Achille, and Stefano Soatto. Critical learning periods emerge\neven in deep linear networks. In The Twelfth International Conference on Learning Representations, 2024.\n[34] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay\nBalsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, Etienne David, Ian Stavness, Wei Guo, Berton Earnshaw, Imran S. Haque, Sara M. Beery, Jure Leskovec, Anshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang.\nWILDS: A benchmark of in-the-wild distribution shifts. In Proceedings of the 38th International\nConference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of\nProceedings of Machine Learning Research, pages 5637\u20135664. PMLR, 2021.\n[35] Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009. [36] Jungmin Kwon, Jeongseop Kim, Hyunseo Park, and In Kwon Choi. ASAM: adaptive sharpnessaware minimization for scale-invariant learning of deep neural networks. In Proceedings of\nthe 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual\nEvent, volume 139 of *Proceedings of Machine Learning Research*, pages 5905\u20135914. PMLR,\n2021.\n[37] Angeliki Lazaridou, Adhiguna Kuncoro, Elena Gribovskaya, Devang Agrawal, Adam Liska,\nTayfun Terzi, Mai Gimenez, Cyprien de Masson d'Autume, Tom\u00e1s Kocisk\u00fd, Sebastian Ruder,\nDani Yogatama, Kris Cao, Susannah Young, and Phil Blunsom. Mind the gap: Assessing temporal generalization in neural language models. In Advances in Neural Information Processing\nSystems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS\n2021, December 6-14, 2021, virtual, pages 29348\u201329363, 2021.\n\n[38] Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning\napplied to document recognition. *Proc. IEEE*, 86(11):2278\u20132324, 1998.\n[39] Patrick Lewis, Barlas Oguz, Ruty Rinott, Sebastian Riedel, and Holger Schwenk. MLQA:\nEvaluating cross-lingual extractive question answering. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics, pages 7315\u20137330, Online, July 2020.\nAssociation for Computational Linguistics.\n[40] Chen Liu, Gregor Geigle, Robin Krebs, and Iryna Gurevych. FigMemes: A dataset for figurative\nlanguage identification in politically-opinionated memes. In Proceedings of the 2022 Conference\non Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab\nEmirates, December 7-11, 2022, pages 7069\u20137086. Association for Computational Linguistics,\n2022.\n[41] Chen Cecilia Liu, Jonas Pfeiffer, Ivan Vulic, and Iryna Gurevych. Improving generalization of\nadapter-based cross-lingual transfer with scheduled unfreezing. *CoRR*, abs/2301.05487, 2023.\n[42] Zixuan Liu, Ziqiao Wang, Hongyu Guo, and Yongyi Mao. Over-training with mixup may hurt\ngeneralization. In The Eleventh International Conference on Learning Representations, ICLR\n2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023.\n[43] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In 7th International\nConference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\nOpenReview.net, 2019.\n[44] Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul, and\nBenjamin Bossan. PEFT: state-of-the-art parameter-efficient fine-tuning methods. https:\n//github.com/huggingface/peft, 2022.\n[45] James Martens and Roger B. Grosse. Optimizing neural networks with kronecker-factored\napproximate curvature. In Proceedings of the 32nd International Conference on Machine\nLearning, ICML 2015, Lille, France, 6-11 July 2015, volume 37 of JMLR Workshop and\nConference Proceedings, pages 2408\u20132417. JMLR.org, 2015.\n[46] Paul Michel and Graham Neubig. MTNT: A testbed for machine translation of noisy text. In\nProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,\npages 543\u2013553, Brussels, Belgium, October-November 2018. Association for Computational Linguistics.\n[47] Marius Mosbach, Maksym Andriushchenko, and Dietrich Klakow. On the stability of fine-tuning\nBERT: Misconceptions, explanations, and strong baselines. In 9th International Conference on\nLearning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net,\n2021.\n[48] Norman Mu and Justin Gilmer. MNIST-C: A robustness benchmark for computer vision. *CoRR*,\nabs/1906.02337, 2019.\n[49] Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring\ngeneralization in deep learning. In Advances in Neural Information Processing Systems 30:\nAnnual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long\nBeach, CA, USA, pages 5947\u20135956, 2017.\n[50] Jonas Pfeiffer, Andreas R\u00fcckl\u00e9, Clifton Poth, Aishwarya Kamath, Ivan Vulic, Sebastian Ruder,\nKyunghyun Cho, and Iryna Gurevych. AdapterHub: A framework for adapting transformers. In\nProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:\nSystem Demonstrations, EMNLP 2020 - Demos, Online, November 16-20, 2020, pages 46\u201354.\nAssociation for Computational Linguistics, 2020.\n[51] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical\nMethods in Natural Language Processing, pages 2383\u20132392, Austin, Texas, November 2016.\nAssociation for Computational Linguistics.\n[52] Tom Sherborne, Naomi Saphra, Pradeep Dasigi, and Hao Peng. TRAM: Bridging trust regions\nand sharpness aware minimization. In The Twelfth International Conference on Learning\nRepresentations, 2024.\n[53] Yi-Lin Sung, Varun Nair, and Colin Raffel. Training neural networks with fixed sparse masks.\nIn Advances in Neural Information Processing Systems 34: Annual Conference on Neural\nInformation Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages\n24193\u201324205, 2021.\n[54] Aarne Talman and Stergios Chatzikyriakidis. Testing the generalization power of neural network\nmodels across NLI benchmarks. In Proceedings of the 2019 ACL Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for NLP, pages 85\u201394, Florence, Italy, August\n2019. Association for Computational Linguistics.\n[55] Kexin Wang, Nils Reimers, and Iryna Gurevych. TSDAE: Using transformer-based sequential\ndenoising auto-encoderfor unsupervised sentence embedding learning. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 671\u2013688, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.\n[56] Adina Williams, Nikita Nangia, and Samuel R. Bowman. A broad-coverage challenge corpus\nfor sentence understanding through inference. In Proceedings of the 2018 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1\n(Long Papers), pages 1112\u20131122. Association for Computational Linguistics, 2018.\n[57] Gang Yan, Hao Wang, and Jian Li. Seizing critical learning periods in federated learning. In\nThirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference\non Innovative Applications of Artificial Intelligence, IAAI 2022, The Twelveth Symposium on\nEducational Advances in Artificial Intelligence, EAAI 2022 Virtual Event, February 22 - March\n1, 2022, pages 8788\u20138796. AAAI Press, 2022.\n[58] Huaxiu Yao, Caroline Choi, Bochuan Cao, Yoonho Lee, Pang Wei Koh, and Chelsea Finn.\nWild-Time: A benchmark of in-the-wild distribution shift over time. In Advances in Neural\nInformation Processing Systems 35: Annual Conference on Neural Information Processing\nSystems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022.\n[59] Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju Hwang. Lifelong learning with\ndynamically expandable networks. In 6th International Conference on Learning Representations,\nICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings.\nOpenReview.net, 2018.\n[60] Haoran You, Chaojian Li, Pengfei Xu, Yonggan Fu, Yue Wang, Xiaohan Chen, Yingyan Lin,\nZhangyang Wang, and Richard G. Baraniuk. Drawing early-bird tickets: Toward more efficient training of deep networks. In *International Conference on Learning Representations*, 2020.\n[61] Hongyi Zhang, Moustapha Ciss\u00e9, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond\nempirical risk minimization. In 6th International Conference on Learning Representations,\nICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings.\nOpenReview.net, 2018.\n[62] Yaowei Zheng, Richong Zhang, and Yongyi Mao. Regularizing neural networks via adversarial\nmodel perturbation. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR\n2021, virtual, June 19-25, 2021, pages 8156\u20138165. Computer Vision Foundation / IEEE, 2021.\n"
    },
    {
        "level": "##",
        "title": "A Gradual Unfreezing",
        "content": "\nFollowing the notations and algorithm in [41], let FORWARD(\u2217) be the standard forward pass, and BACKWARD(\u2217) calculates gradients and performs updates for trainable parameters. The modified gradual unfreezing algorithm is in Algorithm 1. In our experiments, we partition the blocks by their natural namespaces, as the following:\nResNet18: The definition block follows the standard implementation of ResNet, with an input convolution layer and a batch norm group together as the additional block. The model parameters are partitioned into 5 blocks, and a classification head.\n\nVGG11: The definition block follows the standard implementation of VGG, with 8 blocks in total.\n\nThe classification head consists of 3 linear layers with a ReLU function in between.\n"
    },
    {
        "level": "##",
        "title": "Algorithm 1 Gradual Unfreezing",
        "content": "\nRequire: A model's eventual trainable parameters are partitioned into blocks j \u2208 {0*, . . . , L* \u2212 1} parameterized by \u03b8j, with a task-specific classification head C, and an unfreezing interval k. A set S of the indices of parameter blocks to unfreeze.\n\n1: Initialize C, \u03b8j for all j\n2: S \u2190 {C} 3: j \u2190 L \u2212 1\n4: **for** i = 1 *. . .* N do 5:\nSample a data batch b \u223c D\n6:\nif i mod k == 0 and i \u2264 kL then\n7:\nS \u2190 S \u222a {\u03b8j}\n8:\nj \u2190 j \u2212 1\n9:\nend if\n10:\nFORWARD(\u2217)\n11:\nBACKWARD(S)\n12: end for\nXLM-RoBERTa + LoRA: The experiment follows [41]. Each parameter block consists of 2 sets of LoRA adapters added to the query and value of the backbone transformer from the same layer. The LoRA parameters are partitioned into 12 blocks, and a classification head, where the classification head and the last layer of LoRA adapters are trainable initially.\n"
    },
    {
        "level": "##",
        "title": "B Hyperparameters",
        "content": "\nFor all our experiments, the hyperparameters are listed in Table 5. We use the default hyperparameters for the AdamW optimizer, except for the learning rate. All other hyperparameters for the transformer experiments follow [41], and we use the HuggingFace PEFT [44] implementations of LoRA.\n\nFor calculating S\u03c1\nworst and S\u03c1\navg, we use L2 norm and \u03c1 = 0.01 with 15 examples. We follow the setup in [2] and use the implementation with 2048 data points from the training data (un-augmented when calculating sharpness metrics) for all experiments. We use a batch size of 256, except for SQuAD (the batch size is 32) for calculating all the metrics. The sharpness and tr(F) are recorded every 10 batches (steps) for all datasets.\n\n| MNIST           | CIFAR10   | CIFAR10   | CIFAR100   | SQuAD   | MNLI   |\n|-----------------|-----------|-----------|------------|---------|--------|\n| RN18            | RN18      | VGG11     | RN18       | XLM-R   | XLM-R  |\n| optimizer       | SGD       | SGD       | SGD        | SGD     | AdamW  |\n| lr scheduler    | const.    | const.    | const.     | const.  | linear |\n| lr              |           |           |            |         |        |\n| d               |           |           |            |         |        |\n| 0.01            | 0.1       | 0.15      | 0.01       | 0.0005  | 0.0005 |\n| batch size      | 128       | 128       | 128        | 128     | 32     |\n| training epochs | 10        | 200       | 200        | 200     | 15     |\n| weight decay    | 0.01      | 0         | 0          | 0.0005  | 0.01   |\n| momentum        | 0.9       | 0         | 0          | 0.9     | -      |\n| LoRA r          | -         | -         | -          | -       | 8      |\n| LoRA alpha      | -         | -         | -          | -       | 8      |\n| LoRA dropout    | -         | -         | -          | -       | 0.2    |\n"
    },
    {
        "level": "##",
        "title": "C Properties Of The Final Solutions And Ood Results",
        "content": "\nWe plot the final solution's \u03bbmax (largest eigenvalue of training data feature), S\u03c1\nworst and S\u03c1\navg against the OOD test results in Figure 5 respectively.\n\nWhile in general the sharpness measures and OOD have negative correlations (i.e. the smaller sharpness values the better, especially S\u03c1\nworst has a consistent negative correlation), they are not always statistically significant (e.g., for MNIST). The learning rate has a big impact on the final solutions' sharpness. Furthermore, such as in Figure 5 (c), we can even attain slightly positive correlations. Our results complement the findings in [2], which serve as evidence pointing towards the need for developing robust new metrics and thorough investigation for OOD generalization.\n"
    },
    {
        "level": "##",
        "title": "D Algorithm To Determine The Unfreeze Time",
        "content": "\nTo verify our hypothesis, we use a simple algorithm with heuristic to determine the unfreezing time k, Algorithm 2 presents the flow, \u03c4 is 3 or 8 and \u03f5 is 0.02 (i.e. the percentage of change in the signal is within 2%). The algorithm takes t\u2206 \u02c6\nS as the input, which is the index marking the end of the rapid increase of the signal using a similar logic.\n"
    },
    {
        "level": "##",
        "title": "Algorithm 2 Find Stabilization",
        "content": "\n1: **procedure** FIND_STABILIZATION_BY_MEAN( \u02c6*S, t*\u2206 \u02c6\nS *, \u03c4, \u03f5*) \u25b7 \u02c6S is an array of normalized signal when only the head is trainable, t\u2206 \u02c6\nS is\nthe index marking the end of the rapid increasing of the signal, \u03c4 is the window for smoothing the signals, \u03f5 is the threshold in changes of the signal for stabilization.\n2:\nif t\u2206 \u02c6\nS > 0 then\n3:\n\u02c6S = \u02c6S[t\u2206 \u02c6\nS :]\n4:\nend if\n5:\n\u00b5 \u02c6\nS = moving_average( \u02c6*S, \u03c4*)\n6:\n\u2206\u00b5 \u02c6\nS = np.abs(np.diff(\u00b5 \u02c6\nS))\n7:\nfor i, \u03b4 in enumerate(\u00b5 \u02c6\nS) do\n8:\nif \u03b4 \u2264 \u03f5 then\n9:\nindex = i\n\u25b7 The first time where the change is smaller than \u03c4.\n10:\nbreak\n11:\nend if\n12:\nend for\n13:\nif t\u2206 \u02c6\nS > 0 then\n14:\nindex = index + t\u2206 \u02c6\nS\n15:\nend if\n16:\nreturn index\n17: end procedure\nUsing the heuristic algorithm, we determine the value k for experiments in Table 6, where we observe the determined k are very close to each other except for VGG with CIFAR10 and XLM-R with SQuAD. All the k value are shown visually in Figure 6, overlaying on top of the learning dynamics.\n\nMetric\nMNIST RN18\nCIFAR10 RN18\nCIFAR100 RN18\nCIFAR10 VGG11\nSQuAD XLM-R\nMNLI XLM-R\nS\u03c1\nworst\n270\n230\n260\n960\n810\n780\nS\u03c1\navg\n270\n270\n250\n1010\n1090\n720\ntr(F)\n210\n260\n230\n250\n1310\n790\n\nTable 7 shows the complete results for our complex experimental setup (described in \u00a74). While all results are better than the standard training, empirically, tr(F) is a metric that gives a better winning rate compared to a random hyperparameter search.\n\n|             |               | XQuAD         | MLQA       | XNLI        | WR             |\n|-------------|---------------|---------------|------------|-------------|----------------|\n| Method      | F1- En/X-ling | EM- En/X-ling | F1- X-ling | EM- X-ling  | Avg- En/X-ling |\n| Standard    | 82.96/68.72   | 71.39/52.64   | 56.27      | 40.93       | 83.17/71.84    |\n| GU          |               |               |            |             |                |\n| S           |               |               |            |             |                |\n| \u03c1           |               |               |            |             |                |\n| worst       |               |               |            |             |                |\n| 83.78/70.09 | 72.10/54.17   | 57.86         | 42.02      | 82.83/72.13 | 45%            |\n| GU          |               |               |            |             |                |\n| S           |               |               |            |             |                |\n| \u03c1           |               |               |            |             |                |\n| avg         |               |               |            |             |                |\n| 83.84/70.00 | 72.12/53.69   | 58.10         | 42.03      | 83.03/72.27 | 40%            |\n| GU          |               |               |            |             |                |\n| tr          |               |               |            |             |                |\n| (F)         |               |               |            |             |                |\n| 83.77/70.70 | 72.33/54.40   | 58.47         | 42.31      | 83.36/72.49 | 80%            |\n"
    },
    {
        "level": "##",
        "title": "E Additional Learning Dynamics E.1 Ranks",
        "content": "\nFigure 7 shows the evolution of feature ranks before the classification head for the first 2000 training steps. We observe standard training typically starts with a lower feature rank, as the training progresses the feature rank gradually increases. When withholding parameters from training, feature ranks are high at the initial period of learning, and as we gradually release parameters, the feature ranks reduce compared to the initial value.\n"
    },
    {
        "level": "##",
        "title": "E.2 Change Learning Rate From Low To High",
        "content": "\nWe provide an example of S\u03c1\navg during training when switching from a low learning rate to a high learning rate at k = 200 in Figure 8. From Figure 8, we can see the S\u03c1\navg is high initially, then quickly drops when the learning rate increases at step k. More interestingly, switching the learning rate induces similar effects as release parameters from frozen.\n"
    },
    {
        "level": "##",
        "title": "E.3 Squad",
        "content": "\nIn Figure 9, we present the learning dynamics for XLM-R with SQuAD in the early period of learning.\n\nThe learning dynamics show a similar trend as the SQuAD dataset, the S\u03c1\nworst value is also negative, and withholding trainable parameters increases the S\u03c1\nworst during training based on Eqn. 4 in our main paper.\n\nK=200\nK=200\nK=200\n"
    },
    {
        "level": "##",
        "title": "E.4 1/10Th Of The Default Learning Rate",
        "content": "\nFigure 10 shows the learning dynamics in the early period of training using 1/10th of the default learning rate (i.e. 0.1*lrd). The trends are similar to using the default learning rate."
    }
]