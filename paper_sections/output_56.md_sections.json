[
    {
        "level": "#",
        "title": "Contrastive Learning On Multimodal Analysis Of Electronic Health Records",
        "content": "\nTianxi Cai1,2\u22c6, Feiqing Huang1\u22c6, Ryumei Nakada3\u22c6, Linjun Zhang3\u22c6, Doudou Zhou1\u22c6\n1Department of Biostatistics, Harvard T.H. Chan School of Public Health, Boston, MA\n2Department of Biomedical Informatics, Harvard Medical School, Boston, MA\n3Department of Statistics, Rutgers University, Piscataway, NJ\n\u22c6alphabetical order\n"
    },
    {
        "level": "##",
        "title": "Abstract",
        "content": "\nElectronic health record (EHR) systems contain a wealth of multimodal clinical data including structured data like clinical codes and unstructured data such as clinical notes. However, many existing EHR-focused studies has traditionally either concentrated on an individual modality or merged different modalities in a rather rudimentary fashion.\n\nThis approach often results in the perception of structured and unstructured data as separate entities, neglecting the inherent synergy between them. Specifically, the two important modalities contain clinically relevant, inextricably linked and complementary health information. A more complete picture of a patient's medical history is captured by the joint analysis of the two modalities of data. Despite the great success of multimodal contrastive learning on vision-language, its potential remains under-explored in the realm of multimodal EHR, particularly in terms of its theoretical understanding. To accommodate the statistical analysis of multimodal EHR data, in this paper, we propose a novel multimodal feature embedding generative model and design a multimodal contrastive loss to obtain the multimodal EHR feature representation. Our theoretical analysis demonstrates the effectiveness of multimodal learning compared to single-modality learning and connects the solution of the loss function to the singular value decomposition of a pointwise mutual information matrix. This connection paves the way for a privacy-preserving algorithm tailored for multimodal EHR feature representation learning. Simulation studies show that the proposed algorithm performs well under a variety of configurations. We further validate the clinical utility of the proposed algorithm in real-world EHR data.\n\nKeywords: Natural language processing, textual data, structured data, representation learning, singular value decomposition.\n"
    },
    {
        "level": "##",
        "title": "1 Introduction",
        "content": "\nThe growing accessibility of Electronic Health Record (EHR) data presents numerous opportunities for clinical research, ranging from patient profiling (Halpern et al., 2016) to predicting medical events (Choi et al., 2017). However, the complexity increases with the multimodal nature of EHR data, which encompasses diverse clinical data from patient demographics and genetic information to unstructured textual data like clinical notes, and structured data such as diagnostic and procedure codes, medication orders, and lab results.\n\nA key challenge in EHR-focused research lies in effectively merging these different data types and ensuring that their clinical aspects are meaningfully represented. Research has shown the benefits of integrating structured and unstructured data for tasks like automated clinical code assignment (Scheurwegs et al., 2016), managing chronic diseases (Sheikhalishahi et al., 2019), and pharmacovigilance (Stang et al., 2010).\n\nWhile these different modalities serve as complementary data sources, there is significant overlap and correlation among these data (Qiao et al., 2019). Joint representation of both structured and narrative data into a more manageable low-dimensional space where similar features are grouped closely can significantly improve the utility of both data types. This representation learning technique has gained popularity for its ability to capture and represent the intricate relationships among various EHR features.\n\nDespite the extensive research on EHR feature representation, most existing studies have primarily focused on either structured (Choi et al., 2016a; Kartchner et al., 2017; Hong et al., 2021; Zhou et al., 2022) or unstructured data modalities (De Vine et al., 2014;\nChoi et al., 2016b; Beam et al., 2019; Alsentzer et al., 2019; Huang et al., 2020; Lehman and Johnson, 2023). For instance, Alsentzer et al. (2019) adapted the BERT model (Devlin et al., 2019) to the clinical domain by training on the MIMIC-III clinical notes (Johnson et al., 2016). It was extremely time/resource-consuming to train such a black-box model. De Vine et al. (2014) aligned free-text features with the Unified Medical Language System (UMLS)'s common concept unique identifier (CUI) space (McInnes et al., 2007). They then utilized the skip-gram algorithm (Mikolov et al., 2013) on concatenated concept documents to learn the CUI embeddings. Similarly, Choi et al. (2016b) and Beam et al. (2019) employed singular value decomposition (SVD) on a pointwise mutual information (PMI) matrix derived from CUI co-occurrences in unstructured text to generate CUI embeddings.\n\nThis approach was also adopted by Hong et al. (2021) for code embeddings. This SVD- PMI algorithm not only ensures scalability but also upholds data privacy through the use of aggregate co-occurrence data, offering a scalable variant of the skip-gram algorithm (Levy and Goldberg, 2014). Its interpretability is further highlighted in the dynamic log-linear topic model developed by Arora et al. (2016).\n\nRecent studies have emerged on leveraging multimodal EHR features for enhanced predictive modeling, as demonstrated by the work of Khadanga et al. (2019); Zhang et al. (2020); Bardak and Tan (2021); Gan et al. (2023). Specifically, Gan et al. (2023) enhanced code and CUI embeddings by employing the SVD-PMI algorithm, facilitating the integration of structured and unstructured data. Despite these advancements, methods relying on straightforward data merging may not fully account for the intricate interactions within multimodal data, potentially leading to biases. This issue will be further examined in our theoretical (Section 3) and numerical analyses (Sections 4 and 5).\n\nAddressing these limitations, Liu et al. (2022) introduced a multimodal pre-trained language model incorporating a cross-attention mechanism to enrich EHR representations across structured and unstructured data domains.\n\nAdditionally, the domain has seen progress in multimodal contrastive learning strategies, as evidenced by research from Li and Gao (2022); Yin et al. (2023); Wang et al. (2023). These strategies, drawing inspiration from successful vision-language models like the Contrastive Language-Image Pre-Training (CLIP) by Radford et al. (2021), aim to train unified representations of diverse data modalities. However, these approaches, grounded in deep neural networks, face challenges due to their \"black-box\" nature, including a lack of theoretical underpinning, computational complexity, and concerns over privacy since they necessitate access to individual patient data. These factors contribute to their limited applicability in the context of EHR data.\n\nWhile some theoretical analyses of multimodal learning exist, their applicability to EHR\ndata has been limited. Groundbreaking studies like Huang et al. (2021) have illustrated the benefits of multimodal learning, showing that learning across multiple modalities can reduce population risk compared to single-modality methods. Recently, Deng et al. (2023) theoretically proved the zero-shot transfer ability of CLIP. Furthermore, Nakada et al. (2023) explored multimodal contrastive learning's performance under a spiked covariance model. However, these studies do not directly apply to the unique discrete feature structure of EHR data, leaving an unaddressed theoretical gap in understanding multimodal contrastive learning's application in healthcare.\n\nBridging this gap is vital, as it lays a solid foundation for multimodal contrastive learning's development and implementation in healthcare, maximizing its potential to improve patient care and drive medical research forward.\n\nTo overcome these limitations, we introduce the Contrastive Learning Algorithm for Integrated Multimodal Electronic health records (CLAIME). Our findings confirm that CLAIME is not only an effective tool for deriving multimodal EHR feature representations but also a method that respects privacy by requiring only aggregated data. Additionally, we propose a novel multimodal feature embedding generative model (2.1) in Section 2.1, designed to enhance statistical analysis of multimodal EHR data. This model is notable for its interpretability and accurate portrayal of EHR data generation. It distinguishes itself from earlier word vector generative models (Arora et al., 2016, 2018; Lu et al., 2023; Xu et al., 2022) by (1) enabling the integration of multimodal EHR features, and (2) allowing patient heterogeneity by incorporating error terms specific to patients, thus increasing the model's robustness. Within this generative framework, we validate the consistency of the CLAIME algorithm and clarify the relationship between multimodal feature embeddings and a multimodal PMI matrix. The proposed algorithm is also privacy-preserving since it only requires summary-level data, opening doors for collaboration across multiple institutions. Our research also fills a theoretical void in the analysis of multimodal contrastive learning for EHR data.\n\nThe rest of the paper is structured as follows. Section 2 elaborates on the proposed method.\n\nSection 3 presents the theoretical properties of our algorithm.\n\nIn Section 4, simulation studies demonstrate the algorithm's effectiveness under various configurations. Section 5 further validates our algorithm's clinical applicability in EHR studies. Finally, Section 6 includes discussions.\n"
    },
    {
        "level": "##",
        "title": "2 Method 2.1 Notation",
        "content": "\nFor any matrix A, let \u2225A\u2225, \u2225A\u2225max and \u2225A\u2225F be its operator norm, entrywise maximum norm and Frobenius norm, respectively. We define Pp(A) as the top-p right singular vectors of A. When the right singular vectors are not unique, we choose arbitrary singular vectors.\n\nDenote sj(A) as the j-th largest singular value of A. Let Od,p (d \u2265 p) denote a set of d \u00d7 p orthonormal matrices. For two sequences of positive numbers {ak} and {bk}, we write ak \u2272 bk or ak = O(bk) or bk \u2273 ak or bk = \u03a9(ak) if there exists some constant *C >* 0 such that ak \u2264 Cbk for all k. We denote ak \u226a bk or ak = o(bk) if limk\u2192\u221e ak/bk = 0. For any positive integer I, let [I] = {1, 2, \u00b7 \u00b7 \u00b7 *, I*}. We write a \u2228 b and a \u2227 b to denote max(*a, b*) and min(*a, b*), respectively. We denote by ej the j-th unit vector in Rd where its j-th element is one, and all other elements are zero.\n"
    },
    {
        "level": "##",
        "title": "2.2 Model Assumptions",
        "content": "\nAssume that the collections of codes from the structured modality and CUIs from the unstructured modality are denoted by W(1) := [d1] and W(2) := [d] \\ [d1] respectively, where d = d1 + d2, with d representing the total number of unique features across both modalities. Suppose that we have n independent patients. For each patient i, the observed\n\ncodes and CUIs are denoted as {w(1)\n                                i,t }t\u2208[T (1)\n                                      i\n                                        ] and {w(2)\n                                                i,t }t\u2208[T (2)\n                                                      i\n                                                        ], where w(1)\n                                                                 i,t\n                                                                     \u2208 W(1) and\n\nw(2)\n i,t \u2208 W(2). The sizes of these sets are given by T (1)\n                               i\n                                 \u2265 2 and T (2)\n                                       i\n                                         \u2265 2, respectively. We\n\nmodel the probability of observing specific codes and CUIs for the i-th patient based on their embeddings v\u22c6\nw \u2208 Rp as follows:\n\n$$\\mathbb{P}(w^{(1)}_{i,t}=w|\\mathbf{c}_{i},\\mathbf{\\epsilon}^{(1)}_{i})=\\frac{\\exp(\\langle\\mathbf{v}^{*}_{w},\\mathbf{c}_{i}\\rangle+\\epsilon_{i,w})}{\\sum_{w^{\\prime}\\in W^{(1)}}\\exp(\\langle\\mathbf{v}^{*}_{w^{\\prime}},\\mathbf{c}_{i}\\rangle+\\epsilon_{i,w^{\\prime}})},\\ \\ w\\in\\mathcal{W}^{(1)},t\\in[T^{(1)}_{i}],\\ \\text{and}\\tag{2.1}$$ $$\\mathbb{P}(w^{(2)}_{i,t}=w|\\mathbf{c}_{i},\\mathbf{\\epsilon}^{(2)}_{i})=\\frac{\\exp(\\langle\\mathbf{v}^{*}_{w},\\mathbf{c}_{i}\\rangle+\\epsilon_{i,w})}{\\sum_{w^{\\prime}\\in W^{(2)}}\\exp(\\langle\\mathbf{v}^{*}_{w^{\\prime}},\\mathbf{c}_{i}\\rangle+\\epsilon_{i,w^{\\prime}})},\\ \\ w\\in\\mathcal{W}^{(2)},t\\in[T^{(2)}_{i}].$$\n\nHere $\\mathbf{c}_{i}\\sim N(\\mathbf{0},\\mathbf{I}_{p})$ represents a latent vector specific to patient $i$, reflecting their clinical \n\nstate. The error terms, \u03f5(1)\n                        i\n                           = (\u03f5i,1, . . . , \u03f5i,d1)\u22a4 \u223c N(0, \u03a31) and \u03f5(2)\n                                                               i\n                                                                  = (\u03f5i,d1+1, . . . , \u03f5i,d)\u22a4 \u223c\n\nN(0, \u03a32), account for patient-specific variations and address the variability not captured\n\nby the term \u27e8v\u22c6\n             w, ci\u27e9, where \u03a31 and \u03a32 are some unknown positive semi-definite matrices.\n"
    },
    {
        "level": "##",
        "title": "2.3 Claime Algorithm",
        "content": "\nWe define code and CUI embedding matrices as V\u22c6\n                                              1 = (v\u22c6\n                                                    1, . . . , v\u22c6\n                                                           d1)\u22a4 \u2208 Rd1\u00d7p and V\u22c6\n                                                                             2 =\n\n(v\u22c6\n  d1+1, . . . , v\u22c6\n              d)\u22a4 \u2208 Rd2\u00d7p, respectively, and aim to infer V\u22c6 =\n                                                                       \ufffd\n                                                                        V\u22c6\u22a4\n                                                                          1 , V\u22c6\u22a4\n                                                                                2\n                                                                                  \ufffd\u22a4 \u2208 Rd\u00d7p. The\n\nembeddings should reflect clinical semantics, meaning that highly similar (e.g. rheumatoid arthritis and juvenile rheumatoid arthritis) or related (e.g. fasting glucose and type II diabetes) EHR entities should have close embeddings. Before introducing our algorithm,\n\nwe first define aggregate co-occurrence matrices C(M,M) and D(M,M\u2032) for M, M \u2032 \u2208 {1, 2}\n\nacross different modalities as:\n\n$$\\mathbf{C}^{(M,M)}(w,w^{\\prime})=\\sum_{i=1}^{n}\\mathbf{C}_{i,i}^{(M,M)}(w,w^{\\prime}),\\ \\ \\mathbf{D}^{(M,M^{\\prime})}(w,w^{\\prime})=\\sum_{i=1}^{n}\\mathbf{D}_{i,i}^{(M,M^{\\prime})}(w,w^{\\prime})$$\nwhere C(M,M)\ni,j\n(*w, w*\u2032) =\n\ufffd\ufffd{(*t, s*) \u2208 [T (M)\ni\n] \u00d7 [T (M)\nj\n] : t \u0338= *s, w*(M)\ni,t\n= *w, w*(M\u2032)\nj,s\n= w\u2032}\n\ufffd\ufffd and D(M,M\u2032)\ni,j\n(*w, w*\u2032) =\n\ufffd\ufffd{(*t, s*) \u2208 [T (M)\ni\n] \u00d7 [T (M\u2032)\nj\n] : w(M)\ni,t\n= *w, w*(M\u2032)\nj,s\n= w\u2032}\n\ufffd\ufffd for *i, j* \u2208 [n] and M *\u2208 {*1, 2}. Further, we define the marginal co-occurrence of w *\u2208 W*(M) as:\n\n$$\\gamma^{(M)}_{w}={\\bf C}^{(M,M)}(w,\\cdot)=\\sum_{w^{\\prime}\\in{\\cal W}^{(M)}}{\\bf C}^{(M,M)}(w,w^{\\prime}).\\tag{2.2}$$\nscaling factors. CLAIME utilizes the multimodal contrastive learning loss defined as:\n\nWe introduce S(M)\n              q\n                 = (n\u22121 \ufffdn\n                          i=1(T (M)\n                               i\n                                  )q)1/q for q \u2265 1 and S(1,2)\n                                                      1\n                                                           = n\u22121 \ufffdn\n                                                                   i=1 T (1)\n                                                                       i\n                                                                         T (2)\n                                                                          i\n                                                                             as\n\n$$\\begin{split}\\mathcal{L}_{\\text{CLIAME}}(\\mathbf{V}_{1},\\mathbf{V}_{2})&=\\frac{1}{n(nS_{1}^{(1)}S_{1}^{(2)}-S_{1}^{(1,2)})}\\sum_{i\\neq j}\\sum_{\\in[T_{i}^{(1)}]}\\sum_{s\\in[T_{j}^{(2)}]}\\frac{\\langle\\mathbf{v}_{w_{i}^{(1)}},\\mathbf{v}_{w_{j}^{(2)}}\\rangle}{\\gamma_{w_{i}}^{(1)},\\gamma_{w_{j}}^{(2)},}\\\\ &\\quad-\\frac{1}{nS_{1}^{(1,2)}}\\sum_{i=1}^{n}\\sum_{t\\in[T_{i}^{(1)}]}\\sum_{s\\in[T_{i}^{(2)}]}\\frac{\\langle\\mathbf{v}_{w_{i}^{(1)}},\\mathbf{v}_{w_{j}^{(2)}}\\rangle}{\\gamma_{w_{i}}^{(1)},\\gamma_{w_{i}}^{(2)},}+\\frac{\\lambda}{2}\\|\\mathbf{V}_{1}\\mathbf{V}_{2}^{T}\\|_{\\text{F}}^{2}.\\end{split}\\tag{2.3}$$\nHere V1 = (v1*, . . . ,* vd1)\u22a4 \u2208 Rd1\u00d7p and V2 = (vd1+1, . . . , vd)\u22a4 \u2208 Rd2\u00d7p, *\u03bb >* 0 serves as a regularization coefficient, \u03b3(M)\nw\n, M = 1, 2 are weights chosen based on the frequency of w, as defined in (2.2). Our theoretical analysis in Section 3 motivates the choice of \u03b3(M)\nw to guide the minimizer towards V\u22c6. The essence of the multimodal contrastive learning loss in CLAIME is to enhance the representation of similar features across different modalities by bringing them closer together while distancing those that are dissimilar. In the context of EHR data, this translates to aligning the embeddings of codes and CUIs that have clinical correlations and separating those that do not. The aim is to maximize the inner product of embeddings for features that co-occur within the same patient's data. Remark 2.1. Our CLAIME framework can be naturally extended to non-lienar loss functions. Note that our CLAIME loss function (2.3) can be written as\n\n$$\\mathcal{L}_{\\text{CLIME}}(\\mathbf{V}_{1},\\mathbf{V}_{2})=-\\frac{1}{\\sum_{i=1}^{n}T_{i}^{(1)}T_{i}^{(2)}}\\sum_{i=1}^{n}\\left\\{T_{i}^{(1)}T_{i}^{(2)}s_{ii}-\\alpha\\tau\\sum_{j\\neq i}^{n}T_{i}^{(1)}T_{j}^{(2)}s_{ij}\\right\\}+R(\\mathbf{V}_{1},\\mathbf{V}_{2}),$$\n\nwhere $R(\\mathbf{V}_{1},\\mathbf{V}_{2})$ is a smooth regularizer, and\n\n$$\\alpha_{T}=\\frac{\\sum_{i=1}^{n}T_{i}^{(1)}T_{i}^{(2)}}{\\sum_{i\\neq j}^{n}T_{i}^{(1)}T_{j}^{(2)}},\\ \\ s_{ij}=\\frac{1}{T_{i}^{(1)}T_{j}^{(2)}}\\sum_{l\\in[T_{i}^{(1)}]}\\sum_{s\\in[T_{j}^{(2)}]}\\frac{\\langle\\mathbf{V}_{w_{ij}^{(1)},\\mathbf{V}_{w_{ji}^{(2)}}\\rangle}}{\\gamma_{lw_{ij}^{(1)}}\\gamma_{lw_{ij}^{(2)}}},\\ \\ \\ \\ \\ i,j\\in[n].$$\n\nOne can consider the following non-linear loss function analogous to the CLIP loss function.\n\n(Radford et al., 2021):\n\n$$\\mathcal{L}^{\\prime}_{\\text{CLAME}}(\\mathbf{V}_{1},\\mathbf{V}_{2})=-\\frac{1}{\\sum_{i=1}^{n}T_{i}^{(1)}T_{i}^{(2)}}\\sum_{i=1}^{n}T_{i}^{(1)}T_{i}^{(2)}\\log\\frac{\\exp(s_{ii}/\\tau)}{\\sum_{j\\neq i}^{n}T_{i}^{(1)}T_{j}^{(2)}\\exp(s_{ij}/\\tau)}+R(\\mathbf{V}_{1},\\mathbf{V}_{2}),$$\n\nwhich becomes equivalent to the loss (2.3) when $\\tau\\to\\infty$.\n\nTo obtain \ufffdV = ( \ufffdV\u22a4\n1 , \ufffdV\u22a4\n2 )\u22a4 = arg min LCLAIME(V1, V2) efficiently in practice, we note that LCLAIME(V1, V2) can be expressed in terms of pair-wise co-occurrences of concepts as:\n\n$$n(nS_{1}^{(1)}S_{1}^{(2)}-S_{1}^{(1,2)})\\sum_{w=d_{1}+1}^{d_{1}}\\sum_{\\gamma^{(1)}_{w}\\gamma^{w}_{w^{\\prime}}}^{d_{2}}\\sum_{i=1}^{n}\\sum_{j\\neq i}^{n}\\sum_{l\\in\\mathbb{T}^{(l)}_{i}\\setminus e\\in\\mathbb{T}^{(l)}_{j})}\\mathbb{I}(w_{i,t}^{(1)}=w)\\mathbb{I}(w_{i,s}^{(2)}=w^{\\prime})$$ $$-\\frac{1}{nS_{1}^{(1,2)}}\\sum_{w=1}^{d_{1}}\\sum_{w=d_{1}+1}^{d_{2}}\\frac{\\langle\\mathbf{v}_{w},\\mathbf{v}_{w^{\\prime}}^{*}\\rangle}{\\gamma^{(1)}_{w}\\gamma^{(2)}_{w}}\\sum_{i=1}^{n}\\sum_{l\\in\\mathbb{T}^{(l)}_{i}\\setminus e\\in\\mathbb{T}^{(l)}_{i})}\\mathbb{I}(w_{i,t}^{(1)}=w)\\mathbb{I}(w_{i,s}^{(2)}=w^{\\prime})+\\frac{\\lambda}{2}\\|\\mathbf{V}_{1}\\mathbf{V}_{2}^{\\mathsf{T}}\\|_{\\mathrm{F}}^{2}\\,.\\tag{2.4}$$\nSubsequently, via arguments given in Supplementary S2, we have the following proposition.\n\n2 F + (constant), LCLAIME(V1, V2) = \u03bb \ufffd\ufffd\ufffdV1V\u22a4 2 \u2212 1 \u03bb \ufffd PMICLAIME \ufffd\ufffd\ufffd 2\nwhere \ufffd\nPMICLAIME = { \ufffd\nPMICLAIME(*w, w*\u2032)}w\u2208W(1),w\u2032\u2208W(2) with\n\n, \ufffd \ufffd D(1,2)(*w, w*\u2032) C(1,1)(w, \u00b7)C(2,2)(w\u2032, \u00b7) D(1,2)(\u00b7, \u00b7) \u2212 C(c)(*w, w*\u2032) n(nS(1) 1 S(2) 1 \u2212 S(1,2) 1 ) \ufffd PMICLAIME(*w, w*\u2032) := C(1,1)(\u00b7, \u00b7)C(2,2)(\u00b7, \u00b7)\n\nand C(c)(w, w\u2032) = \ufffdn\n                 i=1\n                   \ufffdn\n                     j\u0338=i D(1,2)\n                          i,j (w, w\u2032).\n\nProposition 2.1 shows that LCLAIME(V1, V2) can be related to the SVD of an empirical\n\nassociation matrix, where its element \ufffd\n                                     PMICLAIME(w, w\u2032) estimates the association between\n\nthe features w and w\u2032. We will later demonstrate the convergence of \ufffd\n                                                                       PMICLAIME to the\n\npopulation PMI matrix in Section 3. As a result, \ufffd\n                                                PMICLAIME(w, w\u2032) can be viewed as a\n\nmodified estimator of the population PMI matrix. The final embeddings \ufffdV1 \ufffdV\u22a4\n                                                                          2 are inferred\n\nthrough a rank-p SVD of \ufffd\n                        PMICLAIME, preserving data privacy and offering a scalable estimation. To be more specific, let \ufffdU1 \ufffd\u039b \ufffdU2 denote the rank-p SVD of \ufffd\n                                                                      PMICLAIME, where\n\n\ufffdU1 \u2208 Rd1\u00d7p and \ufffdU2 \u2208 Rd2\u00d7p are the matrices of left and right singular vectors, respectively, and \ufffd\u039b \u2208 Rp\u00d7p is a diagonal matrix with its diagonal elements being the top p singular\n\nof the regularization parameter \u03bb does not play a crucial role, and for the sake of simplicity,\n\nvalues. Then, we set \ufffdV1 = \ufffdU1 \ufffd\u039b1/2 and \ufffdV2 = \ufffdU2 \ufffd\u039b1/2. It is worth noting that the selection we will assign it a value of 1 in our upcoming numerical analyses.\n"
    },
    {
        "level": "##",
        "title": "2.4 Comparison Between Claime And Simple Concatenation",
        "content": "\nWe next contrast CLAIME with the simple approach of ignoring between-modality differences between different modalities. Dealing with multimodal data often presents difficulties, leading to conventional methods that overlook the differences between various modalities. A basic strategy commonly adopted is to simply merge the two modalities through direct concatenation, after which algorithms initially intended for unimodal data are applied. However, this rudimentary treatment of multimodal data may lead to substantial bias due to the inherent heterogeneity between different modalities. To illustrate, consider the concatenated data for the i-th patient represented as\n\n$\\{w_{i,t}\\}_{t\\in[T_{i}]}=\\left(w_{i,1}^{(1)},\\ldots,w_{i,T_{i}^{(1)}}^{(1)},w_{i,1}^{(2)},\\ldots,w_{i,T_{i}^{(2)}}^{(2)}\\right)$, where $T_{i}=T_{i}^{(1)}+T_{i}^{(2)}$.\n\nA popular method to handle such data is the SVD-PMI algorithm, as referenced in (Levy and Goldberg, 2014; Gan et al., 2023). In this context, we establish the co-occurrence matrices for the concatenated dataset as follows:\n\n$$\\mathbf{C}=\\begin{bmatrix}\\mathbf{C}^{(1,1)}&\\mathbf{D}^{(1,2)}\\\\ \\\\ \\mathbf{D}^{(2,1)}&\\mathbf{C}^{(2,2)}\\end{bmatrix},$$\n\nwhere $\\mathbf{C}^{(M,M)}$ and $\\mathbf{D}^{(M,M^{\\prime})}$ are defined in Section 2.3. Subsequently, the empirical concatenated PMI matrix, denoted as $\\widehat{\\mathsf{PMI}}=\\{\\widehat{\\mathsf{PMI}}(w,w^{\\prime})\\}_{w,w^{\\prime}\\in[4]}$, is formulated as\n\n$$\\widehat{\\mathsf{PMI}}(w,w^{\\prime})=\\log\\frac{\\mathbf{C}(w,w^{\\prime})\\mathbf{C}(\\cdot,\\cdot)}{\\mathbf{C}(w,\\cdot)\\mathbf{C}(w^{\\prime},\\cdot)},\\tag{2.5}$$\n\nwhere $\\mathbf{C}(w,\\cdot)=\\sum_{w^{\\prime}=1}^{d}\\mathbf{C}(w,w^{\\prime})$ and $\\mathbf{C}(\\cdot,\\cdot)=\\sum_{w=1}^{d}\\mathbf{C}(w,\\cdot)$. Following this, we conduct a rank-$p$ eigen-decomposition of $\\widehat{\\mathsf{PMI}}$, represented as $\\widehat{\\mathbf{U}}_{\\mathrm{Cou}}\\widehat{\\mathbf{A}}_{\\mathrm{Cou}}\\widehat{\\mathbf{U}}_{\\mathrm{Cou}}$. The estimator of \nrank-p eigen-decomposition of \ufffd\nPMI, represented as \ufffdUCon \ufffd\u039bCon \ufffdUCon. The estimator of V\u22c6\nThe second prevalent technique is contrastive learning (CL), applied directly to the is then achieved by setting \ufffdVCon = \ufffdUCon \ufffd\u039b1/2\nCon. We refer to this method as \"Concate\".\n\nconcatenated dataset. Specifically, we define the contrastive loss for V \u2208 Rd\u00d7p as follows:\n\n$$\\begin{split}\\mathcal{L}_{\\text{CL}}(\\mathbf{V})&=-\\frac{1}{\\sum_{i=1}^{n}T_{i}(T_{i}-1)}\\sum_{i=1}^{n}\\sum_{t\\in[T_{i}]}\\sum_{s\\in[T_{i}](t)}\\frac{\\langle\\mathbf{v}_{w_{i,t}},\\mathbf{v}_{w_{i,s}}\\rangle}{\\gamma_{w_{i,t}}\\gamma_{w_{i,s}}}\\\\ &\\quad+\\frac{1}{\\sum_{i=1}^{n}\\sum_{j\\neq i}T_{i}T_{j}}\\sum_{i\\neq j}\\sum_{t\\in[T_{i}]}\\sum_{s\\in[T_{j}]}\\frac{\\langle\\mathbf{v}_{w_{i,t}},\\mathbf{v}_{w_{j,s}}\\rangle}{\\gamma_{w_{i,t}}\\gamma_{w_{i,s}}}+\\frac{\\lambda}{2}\\|\\mathbf{V}\\mathbf{V}^{\\top}\\|_{F}^{2},\\end{split}\\tag{2.6}$$\n\nwhere $\\gamma_{w}=\\mathbf{C}(w,\\cdot)$. Let $\\widehat{\\mathbf{V}}_{\\text{CL}}=\\arg\\min\\mathcal{L}_{\\text{CL}}(\\mathbf{V})$. Similar to Proposition 2.1, we have the \nfollowing proposition. Here, we define\n\nD = \uf8ee \uf8f9 i=1 C(c)\u22a4 C(2) j\u0338=i C(M,M) i,j (*w, w*\u2032) for w, w\u2032 \u2208 W(M). n \ufffd n \ufffd \uf8ef\uf8f0 C(1) C(c) \uf8fa\uf8fb with C(M)(*w, w*\u2032) =\nProposition 2.2. We have\n\n2 F + (constant), LCL(V) = \u03bb \ufffd\ufffd\ufffdVV\u22a4 \u2212 1 \u03bb \ufffd PMICL \ufffd\ufffd\ufffd 2 where \ufffd PMICL = { \ufffd PMICL(w, w\u2032)}w,w\u2032\u2208[d] with . \ufffd C(*w, w*\u2032) \ufffd C(w, \u00b7)C(w\u2032, \u00b7) C(\u00b7, \u00b7) \u2212 D(*w, w*\u2032) n(nS(1) 1 S(2) 1 \u2212 S(1,2) 1 ) \ufffd PMICL(*w, w*\u2032) := C(\u00b7, \u00b7)C(\u00b7, \u00b7)\nProposition 2.2 reveals that obtaining \ufffdVCL by minimizing the loss LCL is essentially found in Supplementary S4.4. This method is referred to as CL. Nevertheless, as we will equivalent to applying PCA on the matrix \ufffd\nPMICL. The proof for Proposition 2.2 can be demonstrate in Section 3, both \ufffdVCon and \ufffdVCL are not optimal solutions.\n"
    },
    {
        "level": "##",
        "title": "3 Theoretical Analysis",
        "content": "\nIn this section, we investigate the theoretical properties of CLAIME and draw comparisons with the Concate and CL methods. First, let us establish some fundamental concepts. For\n\nw \u2208 W(M), and t \u2208 [T (M)\n             i\n               ], denote X(M)\n                      i,w (t) = I{w(M)\n                              i,t\n                                = w}. According to the model (2.1),\n\nconditioned on ci and \u03f5(M)\ni\n, the variable X(M)\ni,w (t) follows a Bernoulli distribution, with the probability of occurrence given by\n\n$$p_{i,w}^{(M)}=\\mathbb{E}[X_{i,w}^{(M)}(t)|\\mathbf{c}_{i},\\mathbf{c}_{i}^{(M)}]=\\frac{\\exp(\\langle\\mathbf{v}_{w}^{*},\\mathbf{c}_{i}\\rangle+\\epsilon_{i,w})}{\\sum_{w^{\\prime}\\in W^{(M)}}\\exp(\\langle\\mathbf{v}_{w^{\\prime}}^{*},\\mathbf{c}_{i}\\rangle+\\epsilon_{i,w^{\\prime}})}\\,.\\tag{3.1}$$\n\nHere, the probability $p_{i,w}^{(M)}$ is a function of the discourse vector $\\mathbf{c}_{i}$ and the noise term \n\u03f5(M)\ni\n, and is independent of t.\n\nNext, we denote the expected value of p(M)\ni,w as p(M)\nw\n=\nE[p(M)\ni,w ]. Further, we define the co-occurrence of a feature w *\u2208 W*(M) at time t for the i-th\n\npatient and feature w\u2032 \u2208 W(M\u2032) at time s for the j-th patient as X(M,M\u2032)\n                                                                  i,j,w,w\u2032 (t, s) = I{w(M)\n                                                                                   i,t\n                                                                                       =\n\nw, w(M\u2032)\n    j,s\n        = w\u2032} for t \u0338= s. This variable follows a Bernoulli distribution, with the probability\n\nof occurrence denoted as p(M,M\u2032)\n                         i,j,w,w\u2032. Furthermore, we define the expected value for the case of\n\ni = j as p(M,M\u2032)\n            w,w\u2032\n                    = E[p(M,M\u2032)\n                            i,i,w,w\u2032 ]. It is important to note that when i \u0338= j, the expected value\n\nE[p(M,M\u2032)\n  i,j,w,w\u2032] equals the product p(M)\n                    w\n                      p(M\u2032)\n                       w\u2032\n                         . Now, we introduce the population PMI matrix\n\nPMI defined as\n\n$$\\mathbb{PMI}:=\\begin{pmatrix}\\mathbb{PMI}^{(1,1)}&\\mathbb{PMI}^{(1,2)}\\\\ \\\\ \\mathbb{PMI}^{(2,1)}&\\mathbb{PMI}^{(2,2)}\\end{pmatrix}\\text{with}\\mathbb{PMI}^{(M,M^{\\prime})}(w,w^{\\prime})=\\log\\frac{p_{w,w^{\\prime}}^{(M,M^{\\prime})}}{p_{w^{\\prime}}^{(M^{\\prime})}p_{w^{\\prime}}^{(M^{\\prime})}}\\tag{3.2}$$\n\nfor $w\\in\\mathcal{W}^{(M^{\\prime})}$, $w^{\\prime}\\in\\mathcal{W}^{(M^{\\prime})}$ with $M,M^{\\prime}\\in\\{1,2\\}$. Before proceeding with our theoretical \nanalysis, let us establish some general assumptions. Based on our data generation model, we can only identify the left singular space and the singular values of V\u22c6\n1 and V\u22c6\n2. Thus, without loss of generality, we can assume that the singular value decomposition of V\u22c6\nM is\n\nV\u22c6\n M = U\u22c6\n    M\u039b\u22c6\n      M for M \u2208 {1, 2}, where U\u22c6\n                     M \u2208 OdM,p represents the left singular vectors,\n\nand \u039b\u22c6\n     M \u2208 Rp\u00d7p is a diagonal matrix containing the corresponding singular values. The\n\nembedding can only be identified up to a mean shift. For simplicity, we assume V\u22c6\u22a4\nM 1dM =\n0.\n\np/dM, \u2225\u039b\u22c6\nM\u2225/sp(\u039b\u22c6\nM) \u2272 1, Assumption 3.1. For M *\u2208 {*1, 2}, assume that \u2225U\u22c6\nM\u22252,\u221e \u2272\n\ufffd\nand \u2225\u039b\u22c6\nM\u2225 \u226a 1.\n\nAssumption 3.1 is widely known as the incoherence constant condition in the literature appearing in matrix completion (Cand`es and Recht, 2009), PCA (Zhang et al., 2022), and the analysis of representation learning algorithm (Ji et al., 2021).\n\nWe introduce the notation d = min(d1, d2), and \u03a3 = diag(\u03a31, \u03a32).\n\nAssumption 3.2. Assume that log d \u226a d1 \u2227 d2, p log12 d \u226a d1 \u2227 d2, p \u226a (log log d)2, n \u226b p2d5 log2 d and p3 log6 d \u226a d3/2/d1/2.\n\nAssumption 3.2 imposes a condition on the dimension of the low-rank representation.\n\nThis assumption is technically necessary in bounding the normalizing constant of the loglinear word production model (Lemma S5.2). Although, we can relax the assumption of p \u226a (log log d)2 into p \u226a log2 d, we stick to the current assumption for the sake of clarity.\n\nAssumption 3.3. Assume that \u2225diag(\u03a3M)\u2225max \u2272 *p/d*M for M *\u2208 {*1, 2} and 1/p \u226a sp(\u03a3) \u2264 \u2225\u03a31*\u2225 \u2228 \u2225*\u03a32\u2225 \u2272 1.\n\nAssumption 3.3 implies that the signal-to-noise ratio is bounded above and below.\n\nNote that the signal, measured by the smallest positive singular value of (1/p)V\u22c6\nMV\u22c6\u22a4\nM\u2032 for M, M \u2032 \u2208 {1, 2} is of order 1/p from Assumption 3.1.\n\nTheorem 3.1. Under Assumptions 3.1, 3.2 and 3.3, we have\n\n$$\\left\\|\\mathbb{P}\\mathbb{M}\\mathbb{I}^{(M,M)}-\\left(\\frac{1}{p}\\mathbf{V}_{M}^{\\star}\\mathbf{V}_{M}^{\\star\\top}+\\mathbf{\\Sigma}_{M}\\right)\\right\\|\\lesssim\\frac{p^{3/2}}{d_{M}}\\log^{6}d,\\ \\ M\\in\\{1,2\\}$$ $$\\left\\|\\mathbb{P}\\mathbb{M}\\mathbb{I}^{(1,2)}-\\frac{1}{p}\\mathbf{V}_{1}^{\\star}\\mathbf{V}_{2}^{\\star\\top}\\right\\|\\lesssim\\frac{p^{3/2}}{d}\\log^{6}d,$$\nand hence\n\n$$\\left\\|\\mathbb{PMI}-\\left(\\frac{1}{p}\\mathbf{V^{*}V^{*^{\\top}}}+\\mathbf{\\Sigma}\\right)\\right\\|\\lesssim\\frac{p^{2}}{d}\\log^{6}d.$$\n\nThe proof of Theorem 3.1 is presented in Supplementary S4.1. This theorem demon\nstrates that the PMI matrix for each modality can be closely approximated by the embedding space plus the noise covariance matrix \u03a3M. When the noise level \u03a3M is substantial, the PMI value PMI(M,M)(*w, w*\u2032) for a single modality may significantly deviate from \u27e8v\u22c6\nw, v\u22c6\nw\u2032\u27e9/p. On the other hand, the cross-modal PMI matrix PMI(1,2) can be directly estimator for PMI(1,2).\n\napproximated by V\u22c6\n                 1V\u22c6\u22a4\n                    2 /p. The theorem below affirms that \ufffd\n                                                       PMICLAIME is an effective\n\nTheorem 3.5. *Suppose that* n \u226b d2 log2 d *and* \u2225BCon\u2225 \u2228 \u2225BCL\u2225 \u226a 1/p. Under Assumptions 3.1-3.5,\n\npV\u22c6V\u22c6\u22a4 + \u03a3 + BCon d log6 d, \u221an log d + p3 \ufffd1 \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd F \u2272 p3/2d5/2 \ufffd\ufffd\ufffd\ufffdsin \u0398 \ufffd Pp( \ufffdV\u22a4 Con), Pp pV\u22c6V\u22c6\u22a4 + \u03a3 + BCL d d log6 d \ufffd1/2p9/2 \ufffd1 \u221an log d + \ufffdd \ufffd\ufffd\ufffd\ufffdsin \u0398 \ufffd Pp( \ufffdV\u22a4 CL), Pp \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd F \u2272 p3d5/2\nholds with probability 1 \u2212 exp\n\ufffd\n\u2212\u03a9(log2 d)\n\ufffd\n.\n\nThe proof Theorem 3.5 is given in Supplementary S4.4. It shows that both Concate and CL will yield biased estimators for the embeddings V\u22c6 due to the inherent bias terms and covariance matrix of the noise. While the bias terms BCon and BCL can be removed, the influence of \u03a3 remains irreducible due to its unobserved nature. Comparing Theorems\n3.3 and 3.5, we find that CLAIME achieves a sharper rate than Concate and CL even when BCL, BCon and \u03a3 go to zero. As a result, CLAIME is a better estimator than Concate and CL.\n\nTo quantify the effect of BCon and BCL, we define the following empirical moments i\u2208[n] T (M)q i\n)1/q. We also define the empirical cross-moments as of T (M)\ni\n: S(M)\nq\n:= ((1/n) \ufffd\ni\u2208[n](T (1)\ni\n)q/2(T (2)\ni\n)q/2)1/q for q = 2, 4. Note that when the length of patient S(1,2)\nq\n:= ((1/n) \ufffd\ndata is the same, i.e., T (M)\ni\n= T (M) for all i \u2208 [n] for M = 1, 2, S(M)\nq\n= T (M) for all q \u2265 1.\n\nAssumption 3.5. For M = 1, 2, assume that 1 \u2273 S(1)\n                                          1 S(2)\n                                             1 /S(1,2)2\n                                                 2\n                                                     \u226b 1/n, S(M)2\n                                                             1\n                                                                /S(M)2\n                                                                  2\n                                                                      \u226b\n\n1/n and S(M)2\n        2\n             \u226b S(M)\n                1\n                   . Also assume that S(M)\n                                      1\n                                         \u2273 S(M)\n                                             2\n                                                \u2273 S(M)\n                                                   4\n                                                       and S(1,2)\n                                                            2\n                                                                \u2273 S(1,2)\n                                                                   4\n                                                                      .\n\nAssumption 3.5 requires that the empirical variance of T (M)\n                                                        i\n                                                            dominates the mean of\n\nT (M)\ni\n, and their moments are comparable to each other.\n\nFor the same length setting T (M)\ni\n\u2261 T (M) for all i \u2208 [n], the assumption is satisfied if T (M) \u2192 \u221e and n *\u2192 \u221e*.\n\nFor the bias term BCon and BCL, we have the following lemma. The proof of Lemma 3.1\nis given in Supplementary S4.7.\n\nLemma 3.1. Suppose that Assumption 3.5 holds. If T (1)\n                                        i\n                                           = T (2)\n                                             i\n                                               , then,\n\n$$\\|\\mathbf{B}_{\\mathrm{Con}}\\|\\;\\forall\\;\\|\\mathbf{B}_{\\mathrm{CL}}\\|\\lesssim d{\\frac{S_{1}^{(1)}}{S_{2}^{(1)2}}}.$$\n\nFurthermore, if T (1)\n                 i\n                    = T (2)\n                       i\n                           \u2261 T for all i \u2208 [n] for some T > 0, then\n\nT .\n\n\u2225BCon\u2225 \u2228 \u2225BCL\u2225 \u2272 d\n\nWe then characterize the effect of \u03a3.\n                                       We will show that when \u03a3 satisfies suitable\n\nconditions, the two existing methods Concate and CL will not be an ideal estimator of V\u22c6.\n\n(2/3)p.\n\nAssumption 3.6. Assume \u2225 sin \u0398(U\u22c6\n                              1, Pp(\u03a31))\u2225F \u2227 \u2225 sin \u0398(U\u22c6\n                                                 2, Pp(\u03a32))\u2225F \u2265\n                                                              \ufffd\n\nAssumption 3.6 holds if the column space of Pp(\u03a31) is nearly orthogonal to the column\n\nspace of U\u22c6\n           1 and that of U\u22c6\n                           2 nearly orthogonal to that of Pp(\u03a32).\n                                                                    Roughly speaking,\n\nthis assumption requires that we can well separate the features and noises based on their directions in each modality.\n\nTheorem 3.6. Suppose that Assumptions 3.1-3.6 hold. If \u2225BCon\u2225 \u2228 \u2225BCL\u2225 \u226a 1, then,\n\n\u2225 sin \u0398( \ufffdUCon, U\u22c6)\u2225F \u2273 \u221ap\n                        and\n                             \u2225 sin \u0398( \ufffdUCL, U\u22c6)\u2225F \u2273 \u221ap\n\nwith probability 1 \u2212 exp\n                        \ufffd\n                         \u2212\u03a9(log2 d)\n                                     \ufffd\n                                      .\n\nThe formal statement is available in Theorem S4.8.\n                                                   Theorem 3.6 shows that there\n\nis a huge deviation from the estimators of Concate and CL to the true parameters as\n\n\u2225 sin \u0398( \ufffdUCon, U\u22c6)\u2225F and \u2225 sin \u0398( \ufffdUCL, U\u22c6)\u2225F are both lower bounded by the order of \u221ap.\n\nSince for any \ufffdU, U\u22c6 \u2208 Od,p, \u2225 sin \u0398( \ufffdU, U\u22c6)\u2225F can be trivially upper bounded by \u221ap, this\n\nWe present an example construction of the error covariance matrix where we expect to\n\nresult shows that \ufffdUCon and \ufffdUCL can hardly capture any signal in U\u22c6.\n\nsee the performance difference among CLAIME, Concate, and CL.\n\nCorollary 3.1. Choose P(1) \u2208 Od1,p satisfying [U\u22c6\n                                            1; P(1)] \u2208 Od1,2p and \u2225[U\u22c6\n                                                                  1; P(1)]\u22252\n                                                                         2,\u221e \u2272\n\np/d1. Similarly choose P(2) \u2208 Od2,p such that [U\u22c6\n                                        2; P(2)] \u2208 Od2,2p and \u2225[U\u22c6\n                                                             2; P(2)]\u22252\n                                                                   2,\u221e \u2272 p/d2.\n\nLet \u03a31 := P(1)P(1)\u22a4, \u03a32 := (1/2)P(2)P(2)\u22a4, V\u22c6\n                                              1 = U\u22c6\n                                                     1 and V\u22c6\n                                                             2 = U\u22c6\n                                                                    2. For these noise\n\ncovariance matrices, Assumptions 3.1, 3.3 and 3.4 hold. In addition, Assumption 3.6 holds\n\nsince P(1)\u22a4V\u22c6\n             1 = P(2)\u22a4V\u22c6\n                        2 = Op\u00d7p. Under additional assumptions on (T (M)\n                                                                        i\n                                                                           )i\u2208[n], d1 and d2\n\n(Assumptions 3.2 and 3.5), we obtain\n\n$$\\|\\sin\\Theta(\\widehat{\\mathbf{U}}_{\\mathrm{Con}},\\mathbf{U}^{*})\\|_{\\mathrm{F}}\\gtrsim\\sqrt{p},\\ \\ \\|\\sin\\Theta(\\widehat{\\mathbf{U}}_{\\mathrm{CL}},\\mathbf{U}^{*})\\|_{\\mathrm{F}}\\gtrsim\\sqrt{p}\\quad\\text{and}$$\n\n$$\\min_{\\mathbf{H}\\in\\mathcal{D}_{\\mathrm{Pr}}}\\|\\sin\\Theta(\\widehat{\\mathbf{U}}_{\\mathbf{H}},\\mathbf{U}^{*})\\|_{\\mathrm{F}}\\lesssim\\frac{p^{3/2}d^{5/2}d}{\\sqrt{n}}\\log d+\\frac{p^{3}}{d}\\log^{6}d.$$\n\nCorollary 3.1 directly follows from Theorems 3.3 and 3.6. Note that the factor $1/2$ in \nthe definition of \u03a32 guarantees the eigengap condition at the p-th largest singular value of the joint noise covariance matrix \u03a3. The noise covariance in Corollary 3.1 implies that the subspace spanned by the noise is orthogonal to the subspace spanned by the signal, so that the noisy fluctuations in feature frequency distribution occurs simultaneously to many words, in a separable way from the signal. Under this condition, Concate and CL learn the eigenspace corresponding to noise covariance matrix, since those methods are vulnerable to large noise strength. On the other hand, CLAIME can effectively escape from the noise, and learns core representations when n and \u00afd are sufficiently large.\n"
    },
    {
        "level": "##",
        "title": "4 Simulation Studies",
        "content": "\nIn this section, we assess CLAIME's performance and compare it to CL and Concate via simulation studies. When patient-level data is not available, these three methods can be deployed by performing SVD on different aggregated PMI matrices introduced in Section 2. Since patient-level data is also available in the synthesized datasets, we can additionally apply the gradient descent-based multimodal contrastive learning method (CLAIME-GD) with target function at (2.3), and the gradient descent-based contrastive learning method (CL-GD) with target function at (2.6), respectively. The suffix \"-GD\" is added to distinguish them from their SVD-based counterparts. To enhance training efficiency, only 10 negative samples per patient are stochastically sampled. The penalty parameter \u03bb is set to 1, while the learning rate starts at 10\u22124 and decays by a factor of 10 every 10 epochs to ensure convergence. Both gradient-based algorithms converge when their loss functions change by less than 10\u22126.\n\nWe generate patient embedding vector ci \u2208 Rp for each patient i from a multivariate normal distribution N(0, Ip) independently. We construct the code and CUI embedding matrices V\u22c6\n1 \u2208 Rd1\u00d7p and V\u22c6\n2 \u2208 Rd2\u00d7p, respectively, by generating each entry from a standard normal distribution. The row means of V\u22c6\n1 and V\u22c6\n2 are standardized to zero, and their maximum singular values are standardized to 1. Then, we follow the data generation process in (2.1) to generate T (1)\ni number of codes and T (2)\ni number of CUIs for the i-th patient, where T (1)\ni\n, T (2)\ni follow a Poisson distribution with the mean of 50 and only nonzero values are retained. Two different cases of error covariance structures are considered.\n\nFor Case 1, we set \u03a3(M) = diag(Ip/2, 0)/c \u2208 RdM\u00d7dM for M = 1 or 2, where *c >* 0 is a constant controlling the signal-to-noise ratio (SNR) of the data generating process. For\n\nCase 2, we generated \u03c3w\n                        i.i.d.\n                        \u223c Unif(0, 1) and set \u03a3(M)\n                                               w,w\u2032 = \u03c1|w\u2212w\u2032|\u03c3w\u03c3w\u2032/2 for w, w\u2032 \u2208 [dM]\n\nand M *\u2208 {*1, 2}, where \u03c1 \u2208 (0, 1) controls both the structure and the SNR of the data generating process. Specifically, a larger \u03c1 leads to a greater signal-to-noise ratio in the synthesized data.\n\nFor both cases, three experiments are separately conducted to compare the performance of CLAIME, CLAIME-GD, CL, CL-GD, and Concate under different sample sizes n, feature sizes d, and SNR. Their performance is evaluated using the metric Err( \ufffdV, V\u22c6) =\ntrue embedding subspaces. We first fix the SNR by setting c = 0.2 for Case 1 and \u03c1 = 0.8\n\nmaxM=1,2 \u2225 \ufffdVM \ufffdV\u22a4\n           M \u2212 V\u22c6\n                MV\u22c6\u22a4\n                   M \u2225F, which measures the distance between the estimated and\n\nfor Case 2. In the first experiment, we vary the sample sizes n in {2 \u00d7 104, 4 \u00d7 104, 6 \u00d7\n\n104, 8 \u00d7 104, 105}, while fixing d = 2d1 = 2d2 = 1000 and p = 4. In the second experiment, we vary d in {200, 400, 600, 800, 1000} and set d1 = d2 = d/2, while fixing n = 105 and p = 4. For the third experiment, we first fix n = 105, d = 2d1 = 2d2 = 1000 and p = 4.\n\nThen, we vary c equally spaced between 0.2 and 1 for Case 1, and we vary \u03c1 equally spaced between 0 and 0.9 for Case 2.\n\nThe results for CL and CL-GD, CLAIME and CLAIME-GD are very close to each other, The error metrics Err( \ufffdV, V\u22c6), averaged over 500 repetitions, are presented in Figure 1.\n\nas indicated by the proximity of their lines in Figures 1 (a)\u2013(c). This suggests that employing gradient-based methods on patient-level data (i.e., CL-GD and CLAIME-GD) is comparable to utilizing SVD on summary-level PMI matrices (i.e., CL and CLAIME). This observation aligns with the theoretical findings in Propositions 2.1 and 2.2. On one hand, in Figures 1(a) and (b), the errors for CLAIME and CLAIME-GD decrease with an increase in n or a decrease in d. This behavior is anticipated and can be theoretically inferred from the upper bounds provided in Theorems 3.1 - 3.3. In contrast, the errors for Concate, CL, and CL-GD remain unchanged. This can be theoretically justified by their lower bounds being irrelevant to both n and d, as shown in Theorem 3.6. From the top panel of Figure 1(c), it can be observed that all methods exhibit larger errors when c decreases, or in other words, the SNR decreases. Comparing to the other methods, CLAIME and CLAIME-GD\ndemonstrate superior performance in the small SNR regions where c \u2264 0.5. Similarly, the bottom panel of Figure 1(c) shows a consistent increase in errors across all methods with higher values of \u03c1. CLAIME and CLAIME-GD again significantly outperform the other methods when \u03c1 is larger than 0.5. This supports our initial motivation that the proposed method will be more robust in cases when the data is noisier.\n"
    },
    {
        "level": "##",
        "title": "5 Application To Electronic Health Records Studies",
        "content": "\nWe consider training a joint representation of codified and narrative CUI features using summary EHR data of patients with at least 1 diagnostic code of rhematoid arthritis (RA) at Mass General Brigham (MGB). The MGB RA EHR cohort consist of 53,716 patients whose longitudinal EHR data have been summarized as co-occurrence counts of EHR concepts within a 30-day window. We include all EHR diagnosis, medication, and procedure codes which have been rolled up to higher concept levels: diagnostic codes to PheCodes1, procedure codes to clinical classification system (CCS)2, medication codes to ingredient level RxNorm codes (Liu et al., 2005). We include all EHR codified features along with CUIs that have occurred more than 15 times in the cohort. This results in d = 4, 668\nfeatures, including d1 = 3, 477 codified features with 1, 776 PheCodes for diseases, 238 CCS codes for procedures, and 1, 463 RxNorm codes for medications, and d2 = 1, 048 CUI\nfeatures.\n\nBased on the summary-level co-occurrence matrix, we can derive estimators for CLAIME, CL, and Concate, respectively. To assess the quality of the obtained embeddings, we utilize a benchmark previously introduced in Gan et al. (2023). Specifically, we conduct an evaluation focusing on the embeddings' capability to identify established relationships among EHR concept pairs. Various categories of known relations, including similarity and relatedness, have been curated from online knowledge sources. Similar pairs of codified concepts were mainly generated based on code hierarchies such as the PheCode hierarchy. Similar pairs of CUIs were extracted from relationships within the online clinical database UMLS. Additionally, we assessed the similarity between mapped code-CUI pairs, leveraging UMLS to map codified concepts to CUIs. For relatedness among CUI-CUI pairs, we extracted information from the UMLS, considering major classes like \"may treat or may prevent\", \"classifies\", \"differential diagnosis\", \"method of\", and \"causative\". Furthermore, related code-CUI pairs and code-code pairs were curated by mapping disease CUIs to PheCodes, drugs to RxNorm, and procedures to CCS categories. These mapped code pairs were then utilized to evaluate the system's ability to detect relatedness among codified features or the relatedness between codified and NLP features.\n\nvectors of related pairs and randomly selected pairs. This approach enables the calculation of the Area Under the Curve (AUC) of the cosine similarities, providing a metric for distinguishing known pairs from random pairs. The random pairs selected have the same semantic type as the known pairs. For example, in investigating the relationship \"may treat or may prevent,\" we only consider disease-drug pairs. This evaluation strategy allows us to quantify and compare the performance of the EHR embeddings across various relationship types.\n\nThe results are presented in Table 1. It can be observed that CLAIME excels in embedding relationships across two distinct modalities: code and CUIs. In particular, for the similar CUI-RxNorm group, CLAIME achieves about 6% gain in AUC compared to CL and Concate. It is also much better at capturing causative relationships between codes and CUIs, achieving a 4.4% gain in AUC. Moreover, CLAIME is slightly better in representing the parent-child hierarchical relationships between NLP concepts, with a 2% gain in AUC compared to the other two methods.\n\nSince the data comes from the RA cohort, it is of interest for us to further perform case studies for RA-related codified and CUI concepts. We select 7 key concepts for detailed analysis: rheumatoid arthritis, C-reactive proteins, stiffness of joint, Reiter's disease, swollen joint, pneumococcal vaccine, and leflunomide.\n\nFor each of the 7 concepts, we calculate its cosine similarity with all remaining concepts using the embeddings generated by CLAIME, Concate, and CL, respectively. Concepts are ranked by similarity, and a subset comprising the top-100 rankings from any one of the three methods is chosen as positive control. Then, we randomly select an equal number of other concepts as negative controls. This process produces an evaluation set for each of the 7 concepts. Concordance between similarity scores (from CLAIME, Concate, CL) and\n\n| Pairs               | Type                |   Group |   CLAIME |        CL |   Concate |\n|---------------------|---------------------|---------|----------|-----------|-----------|\n| CUI-PheCode         | 0.920               |   0.92  |    0.92  |   345     |           |\n| Similar             | CUI-RXNORM          |   0.972 |    0.919 |     0.918 |        49 |\n| summary             | 0.915               |   0.91  |    0.909 |   394     |           |\n| May Treat (Prevent) | 0.775               |   0.753 |    0.753 |  1302     |           |\n| CUI-Code            | Classifies          |   0.905 |    0.895 |     0.895 |       744 |\n| Related             | ddx                 |   0.773 |    0.765 |     0.766 |      1169 |\n| Causative           | 0.775               |   0.741 |    0.742 |   524     |           |\n| summary             | 0.800               |   0.783 |    0.784 |  3739     |           |\n| Parent              | 0.848               |   0.855 |    0.856 |   422     |           |\n| Similar             | Sibling             |   0.796 |    0.763 |     0.766 |       865 |\n| summary             | 0.813               |   0.794 |    0.796 |  1287     |           |\n| May Treat (Prevent) | 0.767               |   0.765 |    0.766 |   257     |           |\n| CUI-CUI             |                     |         |          |           |           |\n| Classifies          | 0.936               |   0.924 |    0.925 |    45     |           |\n| Related             | ddx                 |   0.741 |    0.771 |     0.773 |        95 |\n| Method of           | 0.809               |   0.796 |    0.8   |    15     |           |\n| summary             | 0.756               |   0.752 |    0.752 |   421     |           |\n| Similar             | PheCode Hierachy    |   0.914 |    0.909 |     0.909 |      3868 |\n| ddx                 | 0.790               |   0.796 |    0.795 |  5819     |           |\n| Classifies          | 0.896               |   0.9   |    0.9   |  4525     |           |\n| Code-Code           |                     |         |          |           |           |\n| Related             | May Treat (Prevent) |   0.734 |    0.727 |     0.725 |      4598 |\n| Causative           | 0.754               |   0.756 |    0.754 |  2468     |           |\n| summary             | 0.798               |   0.799 |    0.798 | 17410     |           |\n\nrelevance scores (from GPT3.5 (OpenAI, 2023), GPT4 (Achiam et al., 2023) ) is assessed using Kendall's tau rank correlation. Relevance scores are obtained by prompting GPT3.5\nand GPT4 to rate concept relevance on a scale of 0 to 1 between the given concept and its respective evaluation set.\n\nThe results, depicted in Figure 2, reveal that Concate and CL exhibit remarkably similar behavior, with the \"CL-GPTx\" lines obscured beneath \"Concate-GPTx\". CLAIME can effectively encode and preserve relations between these medical concepts and their associated features. Notably, negative rank correlations can be observed for CL-GPT3.5/4\nand Concate-GPT3.5/4 regarding \"leflunomide\" and \"pneumococcal vaccine\" related codes and CUIs. To gain more insights, some detailed examples are provided in Table 2.\n\n| ID                                                  | Desc                                       |   Concate |   CL |   CLAIME |   GPT3.5 |   GPT4 |\n|-----------------------------------------------------|--------------------------------------------|-----------|------|----------|----------|--------|\n| Relatedness or similarity with Leflunomide          |                                            |           |      |          |          |        |\n| PheCode:714.1                                       | Rheumatoid arthritis                       |      0.35 | 0.35 |     0.8  |      0.9 |    0.9 |\n| PheCode:714.2                                       | Juvenile rheumatoid arthritis              |      0.36 | 0.37 |     0.78 |      0.9 |    0.8 |\n| C0242708                                            | DMARDs                                     |           |      |          |          |        |\n| 0.36                                                | 0.36                                       |      0.81 | 0.9  |     1    |          |        |\n| RXNORM:5487                                         | Hydrochlorothiazide                        |      0.8  | 0.81 |     0.01 |      0.1 |    0.2 |\n| RXNORM:6809                                         | Metformin                                  |      0.79 | 0.79 |     0.01 |      0.2 |    0.2 |\n| RXNORM:2556                                         | Citalopram                                 |           |      |          |          |        |\n| 0.78                                                | 0.78                                       |      0.05 | 0.1  |     0.1  |          |        |\n| Relatedness or similarity with Pneumococcal vaccine |                                            |           |      |          |          |        |\n| C0151735                                            | Injection site reaction NOS                |      0.29 | 0.29 |     0.74 |      0.8 |    0.8 |\n| CCS:228                                             | Prophylactic vaccinations and inoculations |           |      |          |          |        |\n| 0.29                                                | 0.29                                       |      0.64 | 0.9  |     1    |          |        |\n| C1445860                                            | Protein antibody                           |      0.04 | 0.04 |     0.61 |      0.6 |    0.7 |\n| RXNORM:10689                                        | Tramadol                                   |      0.8  | 0.8  |     0.12 |      0.1 |    0.1 |\n| RXNORM:2556                                         | Citalopram                                 |           |      |          |          |        |\n| 0.75                                                | 0.75                                       |      0.15 | 0.1  |     0.1  |          |        |\n| RXNORM:11248                                        | Cyanocobalamin                             |      0.81 | 0.81 |     0.18 |      0.2 |    0.1 |\n\nMore specifically, leflunomide is a type of disease-modifying antirheumatic drugs (DMARDs)\nused to treat rheumatoid arthritis by reducing inflammation and permanent damage. Table 2 shows that both Concate and CL fail to capture the close connection between leflunomide, RA, and DMARDs. Additionally, they overestimate similarity with unrelated drugs like hydrochlorothiazide, metformin, and citalopram. In the case of pneumococcal vaccine, it is a vaccine targeting streptococcus pneumoniae infections and can potentially cause injection site reactions. It is related to protein antibodies as they all target the immune system (von Elten et al., 2014). Both Concate and CL struggle to capture this similarity and instead overestimate similarity with unrelated drugs like tramadol, cyanocobalamin, and citalopram, used for pain relief, vitamin B12 deficiency, and depressive disorders respectively.\n"
    },
    {
        "level": "##",
        "title": "6 Discussion And Conclusion",
        "content": "\nIn this paper, we propose the noisy multimodal log-linear production model for analyzing the multimodal EHR data and present the privacy-preserving algorithm CLAIME to estimate the multimodal feature embeddings with theoretical justification. Our algorithm is readily available for federated learning when multiple healthcare systems want to co-train their model but can not share patient-level data. It is also of great interest to consider a setting where different systems have overlapping but non-identical feature sets, in which case a block-wise matrix completion algorithm may be useful (Zhou et al., 2021).\n\nAlthough our current analysis focuses on two modalities, it can be extended to more than two modalities. The rich EHR data contains many different modalities such as genetic data and image data. It will be interesting to extend our current framework to accommodate these data. Besides, it will also be interesting to incorporate the patient demographic in the generative model to consider the effects of variables like sex and age.\n\nThis paper focuses on a linear loss function, while non-linear loss functions such as mentioned in Remark 2.1 also deserve to be explored.\n\nAnother interesting problem is the estimation of the patient embedding ci. The patient embedding is useful for many downstream tasks such as identifying \"patients like me\".\n"
    },
    {
        "level": "##",
        "title": "References",
        "content": "\nAchiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D.,\nAltenschmidt, J., Altman, S., Anadkat, S., et al. (2023). GPT-4 technical report. arXiv preprint arXiv:2303.08774.\nAlsentzer, E., Murphy, J. R., Boag, W., Weng, W.-H., Jin, D., Naumann, T., and\nMcDermott, M. (2019).\nPublicly available clinical bert embeddings.\narXiv preprint\narXiv:1904.03323.\nArora, S., Li, Y., Liang, Y., Ma, T., and Risteski, A. (2016). A latent variable model\napproach to pmi-based word embeddings. Transactions of the Association for Computational Linguistics, 4:385\u2013399.\nArora, S., Li, Y., Liang, Y., Ma, T., and Risteski, A. (2018).\nLinear algebraic structure of word senses, with applications to polysemy. Transactions of the Association for\nComputational Linguistics, 6:483\u2013495.\nBardak, B. and Tan, M. (2021). Improving clinical outcome predictions using convolution over medical entities with multimodal learning. *Artificial Intelligence in Medicine*,\n117:102112.\nBeam, A. L., Kompa, B., Schmaltz, A., Fried, I., Weber, G., Palmer, N., Shi, X., Cai,\nT., and Kohane, I. S. (2019). Clinical concept embeddings learned from massive sources of multimodal medical data. In *PACIFIC SYMPOSIUM ON BIOCOMPUTING 2020*,\npages 295\u2013306. World Scientific.\nCand`es, E. J. and Recht, B. (2009). Exact matrix completion via convex optimization.\nFoundations of Computational mathematics, 9(6):717\u2013772.\nChoi, E., Bahadori, M. T., Searles, E., Coffey, C., Thompson, M., Bost, J., Tejedor-Sojo,\nJ., and Sun, J. (2016a). Multi-layer representation learning for medical concepts. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery\nand Data Mining, pages 1495\u20131504.\nChoi, E., Schuetz, A., Stewart, W. F., and Sun, J. (2017). Using recurrent neural network models for early detection of heart failure onset. Journal of the American Medical\nInformatics Association, 24(2):361\u2013370.\nChoi, Y., Chiu, C. Y.-I., and Sontag, D. (2016b). Learning low-dimensional representations\nof medical concepts. *AMIA Summits on Translational Science Proceedings*, 2016:41\u201350.\nDe Vine, L., Zuccon, G., Koopman, B., Sitbon, L., and Bruza, P. (2014). Medical semantic\nsimilarity with a neural language model. In Proceedings of the 23rd ACM International\nConference on Information and Knowledge Management, pages 1819\u20131822.\nDeng, Y., Prasad, K., Fernandez, R., Smolensky, P., Chaudhary, V., and Shieber, S.\n(2023). Implicit chain of thought reasoning via knowledge distillation. arXiv preprint\narXiv:2311.01460.\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. (2019). BERT: pre-training of deep\nbidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, NAACL-HLT, pages 4171\u20134186.\nGan, Z., Zhou, D., Rush, E., Panickan, V. A., Ho, Y.-L., Ostrouchov, G., Xu, Z., Shen,\nS., Xiong, X., Greco, K. F., Hong, C., Bonzel, C.-L., Wen, J., Costa, L., Cai, T., Begoli, E., Xia, Z., Gaziano, J. M., Liao, K. P., Cho, K., Cai, T., and Lu, J. (2023). Arch:\nLarge-scale knowledge graph via aggregated narrative codified health records analysis. medRxiv.\n\nHalpern, Y., Horng, S., Choi, Y., and Sontag, D. (2016). Electronic medical record phenotyping using the anchor and learn framework. Journal of the American Medical Informatics Association, 23(4):731\u2013740.\nHong, C., Rush, E., Liu, M., Zhou, D., Sun, J., Sonabend, A., Castro, V. M., Schubert,\nP., Panickan, V. A., Cai, T., et al. (2021). Clinical knowledge extraction via sparse\nembedding regression (KESER) with multi-center large scale electronic health record data. *NPJ digital medicine*, 4(1):1\u201311.\nHuang, K., Singh, A., Chen, S., Moseley, E., Deng, C.-Y., George, N., and Lindvall, C.\n(2020). Clinical XLNet: Modeling sequential clinical notes and predicting prolonged mechanical ventilation. In Proceedings of the 3rd Clinical Natural Language Processing\nWorkshop, pages 94\u2013100, Online. Association for Computational Linguistics.\nHuang, Y., Du, C., Xue, Z., Chen, X., Zhao, H., and Huang, L. (2021). What makes multimodal learning better than single (provably). Advances in Neural Information Processing\nSystems, 34:10944\u201310956.\nJi, W., Deng, Z., Nakada, R., Zou, J., and Zhang, L. (2021). The power of contrast for\nfeature learning: A theoretical analysis. *arXiv preprint arXiv:2110.02473*.\nJohnson, A. E., Pollard, T. J., Shen, L., Lehman, L.-w. H., Feng, M., Ghassemi, M.,\nMoody, B., Szolovits, P., Anthony Celi, L., and Mark, R. G. (2016). MIMIC-III, a freely accessible critical care database. *Scientific Data*, 3(1):1\u20139.\nKartchner, D., Christensen, T., Humpherys, J., and Wade, S. (2017). Code2vec: Embed-\nding and clustering medical diagnosis data. In 2017 IEEE International Conference on Healthcare Informatics (ICHI), pages 386\u2013390.\n\nKhadanga, S., Aggarwal, K., Joty, S., and Srivastava, J. (2019). Using clinical notes with\ntime series data for ICU management. In Inui, K., Jiang, J., Ng, V., and Wan, X., editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 6432\u20136437, Hong Kong, China. Association for Computational\nLinguistics.\nLehman, E. and Johnson, A. (2023). Clinical-t5: Large language models built using mimic\nclinical text. *PhysioNet*.\nLevy, O. and Goldberg, Y. (2014). Neural word embedding as implicit matrix factorization.\nIn *Advances in Neural Information Processing Systems*, volume 27.\nLi, R. and Gao, J. (2022). Multi-modal contrastive learning for healthcare data analytics.\nIn *2022 IEEE 10th International Conference on Healthcare Informatics (ICHI)*, pages\n120\u2013127. IEEE.\nLiu, S., Ma, W., Moore, R., Ganesan, V., and Nelson, S. (2005). RxNorm: prescription for\nelectronic drug information exchange. *IT professional*, 7(5):17\u201323.\nLiu, S., Wang, X., Hou, Y., Li, G., Wang, H., Xu, H., Xiang, Y., and Tang, B. (2022).\nMultimodal data matters: language model pre-training over structured and unstructured electronic health records. *IEEE Journal of Biomedical and Health Informatics*, 27(1):504\u2013\n514.\nLu, J., Yin, J., and Cai, T. (2023). Knowledge graph embedding with electronic health\nrecords data via latent graphical block model. *arXiv preprint arXiv:2305.19997*.\nMcInnes, B. T., Pedersen, T., and Carlis, J. (2007). Using UMLS Concept Unique Identifiers (CUIs) for word sense disambiguation in the biomedical domain. In AMIA Annual\nSymposium Proceedings, volume 2007, pages 533\u2013537. American Medical Informatics Association.\nMikolov, T., Chen, K., Corrado, G., and Dean, J. (2013). Efficient estimation of word\nrepresentations in vector space. *Proceedings of Workshop at ICLR*, 2013.\nNakada, R., Gulluk, H. I., Deng, Z., Ji, W., Zou, J., and Zhang, L. (2023). Understanding multimodal contrastive learning and incorporating unpaired data. In International\nConference on Artificial Intelligence and Statistics, pages 4348\u20134380. PMLR.\nOpenAI (2023). ChatGPT: Optimizing language models for dialogue. URL: https://openai.\ncom/blog/chatgpt.\nQiao, Z., Wu, X., Ge, S., and Fan, W. (2019). Mnn: multimodal attentional neural networks\nfor diagnosis prediction. *Extraction*, 1(2019):A1.\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell,\nA., Mishkin, P., Clark, J., et al. (2021). Learning transferable visual models from natural language supervision. In *International conference on machine learning*, pages 8748\u20138763.\nPMLR.\nScheurwegs, E., Luyckx, K., Luyten, L., Daelemans, W., and Van den Bulcke, T. (2016).\nData integration of structured and unstructured sources for assigning clinical codes to patient stays. *Journal of the American Medical Informatics Association*, 23(e1):e11\u2013e19.\nSheikhalishahi, S., Miotto, R., Dudley, J. T., Lavelli, A., Rinaldi, F., Osmani, V., et al.\n(2019). Natural language processing of clinical notes on chronic diseases: systematic review. *JMIR medical informatics*, 7(2):e12239.\nStang, P. E., Ryan, P. B., Racoosin, J. A., Overhage, J. M., Hartzema, A. G., Reich, C.,\nWelebob, E., Scarnecchia, T., and Woodcock, J. (2010). Advancing the science for active surveillance: rationale and design for the observational medical outcomes partnership. Annals of Internal Medicine, 153(9):600\u2013606.\nvon Elten, K. A., Duran, L. L., Banks, T. A., Banks, T. A., Collins, L. C., and Collins,\nL. C. (2014). Systemic inflammatory reaction after pneumococcal vaccine: a case series. Human Vaccines & Immunotherapeutics, 10(6):1767\u20131770.\nWang, X., Luo, J., Wang, J., Yin, Z., Cui, S., Zhong, Y., Wang, Y., and Ma, F.\n(2023). Hierarchical pretraining on multimodal electronic health records. arXiv preprint\narXiv:2310.07871.\nXu, Z., Shen, S., Gan, Z., Doudou, Z., Cai, T., and Lu, J. (2022). Codes clinical correlation\ntest with inference on pmi matrix. Preprint.\nYin, Q., Zhong, L., Song, Y., Bai, L., Wang, Z., Li, C., Xu, Y., and Yang, X. (2023).\nA decision support system in precision medicine: contrastive multimodal learning for patient stratification. *Annals of Operations Research*, pages 1\u201329.\nZhang, A. R., Cai, T. T., and Wu, Y. (2022). Heteroskedastic pca: Algorithm, optimality,\nand applications. *The Annals of Statistics*, 50(1):53\u201380.\nZhang, Z., Liu, J., and Razavian, N. (2020). BERT-XML: Large scale automated ICD coding using BERT pretraining. In Rumshisky, A., Roberts, K., Bethard, S., and Naumann,\nT., editors, *Proceedings of the 3rd Clinical Natural Language Processing Workshop*, pages 24\u201334, Online. Association for Computational Linguistics.\n\nZhou, D., Cai, T., and Lu, J. (2021). Multi-source learning via completion of block-wise\noverlapping noisy matrices. *arXiv preprint arXiv:2105.10360*.\nZhou, D., Gan, Z., Shi, X., Patwari, A., Rush, E., Bonzel, C.-L., Panickan, V. A., Hong,\nC., Ho, Y.-L., Cai, T., et al. (2022). Multiview incomplete knowledge graph integration with application to cross-institutional ehr data harmonization. Journal of Biomedical\nInformatics, 133:104147."
    }
]